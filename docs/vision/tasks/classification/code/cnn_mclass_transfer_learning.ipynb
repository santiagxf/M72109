{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de clasificación de imágenes multiple con transferencia de aprendizaje\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intalamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/vision/tasks/classification/cnn_mclass_transfer_learning.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r cnn_mclass_transfer_learning.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://santiagxf.blob.core.windows.net/public/imdb-movie-genre.zip \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/\n",
    "!unzip -o -qq ./Datasets/imdb-movie-genre.zip -d Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que quiera descargar todo el conjunto de datos de imágenes puede utilizar el siguiente codigo para descargar las imagenes dadas sus URLs (aproximadamente 40K imagenes). Su uso es:\n",
    "\n",
    "```\n",
    "donwload_all(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def download_file(download_url, filename):\n",
    "    try:\n",
    "        response = urllib.request.urlopen(download_url)    \n",
    "    except urllib.error.HTTPError:\n",
    "        return False\n",
    "    \n",
    "    file = open(filename, 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    "    \n",
    "    return True\n",
    "\n",
    "def donwload_all(data):\n",
    "    data['Downloaded'] = False\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        data['Downloaded'][index] = download_file(row['Poster'], row['Image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre el conjunto de datos de este ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos proviene de [Kaggle](https://www.kaggle.com/neha1703/movie-genre-from-its-poster) y contiene carteles (posters) de películas del [sitio web de IMDB](https://www.imdb.com /). Este conjunto de datos contiene la siguiente información para cada película: \n",
    "\n",
    " - ID de IMDB\n",
    " - enlace de IMDB\n",
    " - título\n",
    " - puntuación de IMDB\n",
    " - género\n",
    " - un enlace para descargar el póster de la película\n",
    " \n",
    "En este conjunto de datos, cada póster de película puede pertenecer al menos a un género y puede tener como máximo 3 etiquetas asignadas. El número total de carteles ronda los 40K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Datasets/MovieGenre.csv\", encoding = \"ISO-8859-1\").dropna()\n",
    "data['Image'] = \"Datasets/SampleMoviePosters/\" + data[\"imdbId\"].astype(str) + \".jpg\"\n",
    "data['Genre'] = data['Genre'].apply(lambda x: x.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>Downloaded</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.imdb.com/title/tt114709</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Animation, Adventure, Comedy]</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>True</td>\n",
       "      <td>Datasets/SampleMoviePosters/114709.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>113497</td>\n",
       "      <td>http://www.imdb.com/title/tt113497</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>[Action, Adventure, Family]</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>True</td>\n",
       "      <td>Datasets/SampleMoviePosters/113497.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>113228</td>\n",
       "      <td>http://www.imdb.com/title/tt113228</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>True</td>\n",
       "      <td>Datasets/SampleMoviePosters/113228.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>114885</td>\n",
       "      <td>http://www.imdb.com/title/tt114885</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>5.7</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>True</td>\n",
       "      <td>Datasets/SampleMoviePosters/114885.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>113041</td>\n",
       "      <td>http://www.imdb.com/title/tt113041</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>5.9</td>\n",
       "      <td>[Comedy, Family, Romance]</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>True</td>\n",
       "      <td>Datasets/SampleMoviePosters/113041.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  imdbId                           Imdb Link  \\\n",
       "0           0  114709  http://www.imdb.com/title/tt114709   \n",
       "1           1  113497  http://www.imdb.com/title/tt113497   \n",
       "2           2  113228  http://www.imdb.com/title/tt113228   \n",
       "3           3  114885  http://www.imdb.com/title/tt114885   \n",
       "4           4  113041  http://www.imdb.com/title/tt113041   \n",
       "\n",
       "                                Title  IMDB Score  \\\n",
       "0                    Toy Story (1995)         8.3   \n",
       "1                      Jumanji (1995)         6.9   \n",
       "2             Grumpier Old Men (1995)         6.6   \n",
       "3            Waiting to Exhale (1995)         5.7   \n",
       "4  Father of the Bride Part II (1995)         5.9   \n",
       "\n",
       "                            Genre  \\\n",
       "0  [Animation, Adventure, Comedy]   \n",
       "1     [Action, Adventure, Family]   \n",
       "2               [Comedy, Romance]   \n",
       "3        [Comedy, Drama, Romance]   \n",
       "4       [Comedy, Family, Romance]   \n",
       "\n",
       "                                              Poster  Downloaded  \\\n",
       "0  https://images-na.ssl-images-amazon.com/images...        True   \n",
       "1  https://images-na.ssl-images-amazon.com/images...        True   \n",
       "2  https://images-na.ssl-images-amazon.com/images...        True   \n",
       "3  https://images-na.ssl-images-amazon.com/images...        True   \n",
       "4  https://images-na.ssl-images-amazon.com/images...        True   \n",
       "\n",
       "                                    Image  \n",
       "0  Datasets/SampleMoviePosters/114709.jpg  \n",
       "1  Datasets/SampleMoviePosters/113497.jpg  \n",
       "2  Datasets/SampleMoviePosters/113228.jpg  \n",
       "3  Datasets/SampleMoviePosters/114885.jpg  \n",
       "4  Datasets/SampleMoviePosters/113041.jpg  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtraremos el conjunto de datos solo por aquellas imágenes que están disponible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = data[data['Downloaded'] == True][\"Image\"].to_numpy()\n",
    "labels = data[data['Downloaded'] == True][\"Genre\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero debemos generar las etiquetas que se utilizarán para el modelo de clasificación. Dado que este es un problema de clasificación multiple, utilizaremos `MultiLabelBinarizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto de datos en entrenamiento y testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuramos algunos parámetros de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Tamaño en pixeles de las imágenes\n",
    "CHANNELS = 3 # Numero de canales. RGB = 3\n",
    "BATCH_SIZE = 256 # Tamaño del lote para entrenamiento\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Permite adaptar el preprocesamiento automaticamente para mejorar la performance\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Mezclaremos el conjunto de datos en lotes de 1024 imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para leer las imágenes de los directorios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "def parse_image(filename, labels, CHANNELS:int=3, IMG_SIZE:int=224):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    image_normalized = image_resized / 255.0\n",
    "\n",
    "    return image_normalized, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para generar el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos nuestros conjuntos de datos de entrenamiento y testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(X_train, y_train, is_training=True)\n",
    "test_ds = create_dataset(X_test, y_test, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando transferencia de aprendizaje en nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es TensorFlow Hub?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un concepto que es esencial en el desarrollo de software es la idea de reutilizar el código que está disponible a través de librerías. Las librerías hacen que el desarrollo sea más rápido y generan más eficiencia. Para los cientificos de datos que trabajan en visión artificial, entrenar arquitecturas de redes neuronales complejas desde cero resulta altamente complejo. TensorFlow Hub es una biblioteca que permite publicar y reutilizar componentes de aprendizaje automático prediseñados. Con TF.Hub, resulta sencillo volver a entrenar la capa superior de un modelo previamente entrenado para reconocer las clases en un nuevo conjunto de datos. TensorFlow Hub también distribuye modelos sin la capa de clasificación. Estos se pueden utilizar para realizar fácilmente el aprendizaje por transferencia. Puede descargar cualquier [modelo para imágenes compatible con Tensorflow 2 de tfhub.dev](https://tfhub.dev/s?module-type=image-feature-vector&q=tf2). La única condición es asegurarse de que la forma de las características de la imagen en nuestro conjunto de datos preparado coincida con la forma de entrada esperada del modelo que desea reutilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando un modelo para transferencia de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos una instancia previamente entrenada de MobileNetV2 y un tamaño de entrada de 224x224. MobileNetV2 es en realidad una gran familia de arquitecturas de redes neuronales que se diseñaron principalmente para acelerar la inferencia en el dispositivo celular. Vienen en diferentes tamaños según el multiplicador de profundidad (número de características en capas convolucionales ocultas) y el tamaño de las imágenes de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El extractor que estamos usando aquí acepta imágenes de forma (224, 224, 3) y devuelve un vector de 1280 de longitud para cada imagen. Para evitar que TensorFlow intente ajustar los pesos de esta parte del modelo, debemos congelar los pesos en la capa del extractor, de modo que el entrenamiento solo modifique las capas de clasificación. Por lo general, esto es una buena práctica cuando se trabaja con conjuntos de datos que son muy pequeños en comparación con el conjunto de datos original en el que se entrenó el extractor de predictor. Solo se recomienda entrenarla si el conjunto de datos de entrenamiento es grande y muy similar al conjunto de datos de ImageNet original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos el modelo agregando un clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    keras.layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    keras.layers.Dense(N_LABELS, activation='sigmoid', name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds,\n",
    "  epochs=EPOCHS,\n",
    "  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo en caso de que lo necesitemos más tarde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "t = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_path = \"./Models/imbd-mclass_{}\".format(t)\n",
    "tf.keras.experimental.export_saved_model(model, export_path)\n",
    "print(\"Modelo almacenado en: '{}'\".format(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.keras.experimental.load_from_saved_model(export_path,\n",
    "                                                       custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN - Transfer Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
