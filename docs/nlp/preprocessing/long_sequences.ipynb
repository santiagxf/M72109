{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Fine-tunning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "nlp-py38"
    },
    "kernelspec": {
      "display_name": "NLP (Python 3.8)",
      "language": "python",
      "name": "nlp-py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKENeWEEwKIb"
      },
      "source": [
        "Dividiendo documentos largos en subsecuencias\n",
        "============================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT_t9OxYwKIc"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Las secuencias de texto con las que trabajamos podrían ser de una cantidad de palabras infinita, sin embargo, en computación sabemos que no existe poder de computo infinito y por lo tanto es necesario imponer restricciones en las dimensiones de nuestros datos de entrada. Dependiendo de los requisitos computacionales del modelo con el que estamos trabajando y sus supociones, será entonces la longitud máxima de texto sobre la que podemos trabajar. Modelos complejos pondrán mayor presión de recursos de hardware y por lo tanto podrían ser más restrictivos con la cantidad de palabras que podemos procesar.\n",
        "\n",
        "Tenemos varias opciones para resolver esta limitación:\n",
        "\n",
        "- Truncar las secuencias a la máxima longitud disponible, con la esperanza de que toda la información relevante esté en la secuencia resultante. Claramente aquí perderá información y dependerá de la cantidad de información que piede si es una alternativa viable o no.\n",
        "- Utilizar un modelo que opere sobre secuencias más largas\n",
        "- Ejecutar el modelo sobre subsecuencias más pequeñas y luego entrenar un metamodelo que tome las predicciones de cada secuencia y las combine.\n",
        "- Dividir la secuencia en subsecuencias de un tamaño menos pero manteniendo algo del contexto de la subsecuencia anterior a la que estamos procesando. Luego ejecutar nuestro modelo tratando a cada subsecuencia como un documento distinto. Las predicciones de todas las subsecuencias luego son agregadas utilizando alguna función. Para ver un ejemplo de esto ultimo vea Dividiendo documentos largos en subsecuencias.\n",
        "\n",
        "En este ejemplo exploraremos como realizar la última opción:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b_oCydHKHEJ"
      },
      "source": [
        "### Preparación del ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFNu3tUYAKe0"
      },
      "source": [
        "Instalamos las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcb2-bO_tVS"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/preprocessing/long_sequences.txt \\\n",
        "  --no-clobber --quiet\n",
        "!pip install -r long_sequences.txt -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZrG1xI-61j0"
      },
      "source": [
        "### Sobre el conjunto de datos a utilizar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gBXNzwYwKIu"
      },
      "source": [
        "En este caso, no podremos trabajar con el conjnto de datos que veniamos trabajando anteriormente ya que los tweets en general son secuencias de texto cortas de hasta 140 caracateres. Es decir que nuestros modelos probablemente nunca se encuentren con el problema de longitudes largas en el texto.\n",
        "\n",
        "Para demostrar este ejemplo, utilizaremos el conjunto de datos \"20 grupos de notificas\". El conjunto de datos comprende alrededor de 18000 publicaciones de grupos de noticias sobre 20 temas distintos, desde deportes hasta noticias relacioandas con la aeronáutica.\n",
        "\n",
        "Para realizar el trabajo más sencillo, solo traeremos noticias de los grupos 'alt.atheism' y 'sci.space'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW6_ZXyGAOfs"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewFjFRl9KSwQ"
      },
      "source": [
        "Podemos ver que este conjunto de datos dispone de la siguiente cantidad de muestras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFEABYV3CNmi",
        "outputId": "9da0419e-889a-4590-ed9c-7bd309883c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Cantidad de textos:\", len(newsgroups_train.data), \"\\nCantidad de anotaciones:\", newsgroups_train.target.shape[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de textos: 1073 \n",
            "Cantidad de anotaciones: 1073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBdAoarcKvVn"
      },
      "source": [
        "Creemos un `pd.DataFrame` de `pandas` para que sea más fácil manipular los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCtqp7xw6LY6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({ 'text': newsgroups_train.data, 'category': newsgroups_train.target })"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlploSNM6LY6"
      },
      "source": [
        "## Resolviendo las limitaciones de longitud de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRbtuoJLK7Sf"
      },
      "source": [
        "Primero, revisemos las longitudes de los documentos que tenemos para tener una idea de que tan largos pueden llegar a ser. Primero dividiremos el mismo en palabras y luego generaremos un histograma para revisar que tan frecuentemente hay documentos con cada longitud:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MjFAxw1EorM"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "\n",
        "tokenized_text = tokenizer.texts_to_sequences(df['text'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TAJSnfAEQbP",
        "outputId": "bc3ec15f-1388-4819-b6f4-b966b8539804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "text_lens = [len(n) for n in tokenized_text]\n",
        "\n",
        "import seaborn as sns\n",
        "sns.displot(text_lens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f860990b610>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYQUlEQVR4nO3dfbBkdX3n8fcn3GGeSHiQCUUYLMYV3SXWZqVGFmM2pZJSNJbDbrEslEFU2Fmj8SGkYjBWrbV/WNGspVHXYM0KEbZchBAM6PqwCBMtqyLs+MyjTGCVoUAucW6TBQe58N0/+tyhucwwd+7c7l/P7fer6laf8zun+3z7cObDr399zulUFZKk0ful1gVI0qQygCWpEQNYkhoxgCWpEQNYkhqZal3AgTj99NPrK1/5SusyJGlfsqfGg7oH/NBDD7UuQZIW7aAOYEk6mBnAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjUxsAFcVMzMz+KOkklqZ2ADu9Xqc/ZEv0Ov1WpciaUJNbAADrFi1tnUJkibYRAewJLVkAEtSIwawJDViAEtSIwawJDViAEtSIwawJDUytABOcmmSB5PcMq/9HUnuSHJrkj8faH9vku1J7kzy6mHVJUnjYmqIr/0Z4L8Bl881JHkFsAn4jap6LMmvdu0nAWcDvw78GvC1JC+oqieGWJ8kNTW0HnBVfQP42bzm3wc+WFWPdes82LVvAj5XVY9V1T3AduCUYdUmSeNg1GPALwD+TZKbknw9yUu69uOAewfW29G1SdKyNcwhiL1t7yjgVOAlwFVJnrc/L5BkM7AZ4LnPfe6SFyhJozLqHvAO4Jrquxl4EjgauA84fmC99V3bM1TVlqraWFUb161bN/SCJWlYRh3Afwu8AiDJC4BDgYeA64Czk6xMsgE4Ebh5xLVJ0kgNbQgiyRXAy4Gjk+wA3g9cClzanZr2C+C86t8R/dYkVwG3AbPA2z0DQtJyN7QArqpz9rLo9/ay/geADwyrHkkaN14JJ0mNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1MioL0UeK1VFr9cD4PDDDydJ44okTZKJ7gHP7nqUt15+M+defOPuIJakUZnoHjDAitVrmZqa+N0gqYGJ7gFLUksGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiNDC+AklyZ5MMkte1j2R0kqydHdfJJ8PMn2JD9IcvKw6pKkcTHMHvBngNPnNyY5HngV8JOB5tcAJ3Z/m4GLh1iXJI2FoQVwVX0D+NkeFn0UeA9QA22bgMur71vAEUmOHVZtkjQORjoGnGQTcF9VfX/eouOAewfmd3Rte3qNzUm2Jdk2PT09pEolafhGFsBJ1gB/CvznA3mdqtpSVRurauO6deuWpjhJamBqhNv6Z8AG4PtJANYD30lyCnAfcPzAuuu7NklatkbWA66qH1bVr1bVCVV1Av1hhpOr6gHgOuCN3dkQpwK9qrp/VLVJUgvDPA3tCuDvgRcm2ZHk/GdZ/UvA3cB24L8DbxtWXZI0LoY2BFFV5+xj+QkD0wW8fVi1SNI48ko4SWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgSWrEAJakRgxgoKro9Xr07wkkSaNhAAOP73qEC7ZspdfrtS5F0gQxgDtTq9a0LkHShDGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJamRoQVwkkuTPJjkloG2/5rkjiQ/SPL5JEcMLHtvku1J7kzy6mHVJUnjYpg94M8Ap89rux54UVX9S+BHwHsBkpwEnA38evecv0xyyBBrk6TmhhbAVfUN4Gfz2v53Vc12s98C1nfTm4DPVdVjVXUPsB04ZVi1SdI4aDkG/Bbgy930ccC9A8t2dG3PkGRzkm1Jtk1PTw+5REkaniYBnOR9wCzw2f19blVtqaqNVbVx3bp1S1+cJI3I1Kg3mORNwOuA0+qpX8G8Dzh+YLX1XZskLVsj7QEnOR14D/D6qnp0YNF1wNlJVibZAJwI3DzK2iRp1IbWA05yBfBy4OgkO4D30z/rYSVwfRKAb1XVW6vq1iRXAbfRH5p4e1U9MazaJGkcDC2Aq+qcPTRf8izrfwD4wLDqkaRx45VwktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjRjAktSIASxJjUxkAFcVvV6vdRmSJtxEBnCv1+Mtn/wys0/Mti5F0gSbyAAGWLFy7dPm53rFVdWoIkmTZmIDeL7ZXY9ywZatDk1IGhkDeMDUqjWtS5A0QQxgSWrEAJakRoYWwEkuTfJgklsG2o5Kcn2Su7rHI7v2JPl4ku1JfpDk5GHVJUnjYpg94M8Ap89ruwi4oapOBG7o5gFeA5zY/W0GLh5iXZI0FoYWwFX1DeBn85o3AZd105cBZwy0X1593wKOSHLssGqTpHEw6jHgY6rq/m76AeCYbvo44N6B9XZ0bZK0bDX7Eq76Vzzs91UPSTYn2ZZk2/T09BAqk6TRGHUA/3RuaKF7fLBrvw84fmC99V3bM1TVlqraWFUb161bN9RiJWmYRh3A1wHnddPnAdcOtL+xOxviVKA3MFQhScvS1LBeOMkVwMuBo5PsAN4PfBC4Ksn5wI+Bs7rVvwS8FtgOPAq8eVh1SdK4GFoAV9U5e1l02h7WLeDtw6pFksaRV8JJUiMGsCQ1YgBLUiMGsCQ1YgBLUiMLCuAkL1tImyRp4RbaA/7EAtskSQv0rOcBJ3kp8JvAuiQXDiz6FeCQYRYmScvdvi7EOBQ4rFvvlwfaHwbOHFZRkjQJnjWAq+rrwNeTfKaqfjyimiRpIiz0UuSVSbYAJww+p6peOYyiJGkSLDSA/xr4FPBp4InhlSNJk2OhATxbVf5OmyQtoYWehvaFJG9Lcmz3y8ZHJTlqqJVJ0jK30B7w3E3U/3igrYDnLW05kjQ5FhTAVbVh2IVI0qRZUAAneeOe2qvq8qUtR5Imx0KHIF4yML2K/q9afAcwgCVpkRY6BPGOwfkkRwCfG0pFkjQhFns7ykcAx4Ul6QAsdAz4C/TPeoD+TXj+BXDVsIqSpEmw0DHgDw9MzwI/rqodQ6hHkibGgoYgupvy3EH/jmhHAr8YZlGSNAkW+osYZwE3A/8eOAu4KYm3o5SkA7DQIYj3AS+pqgcBkqwDvgZcPazCJGm5W+hZEL80F76df9yP50qS9mChPeCvJPkqcEU3/x+ALw2nJEmaDPv6TbjnA8dU1R8n+XfAb3WL/h747LCLk6TlbF/DCH9B//ffqKprqurCqroQ+Hy3bFGS/GGSW5PckuSKJKuSbEhyU5LtSa5McuhiX1+SDgb7CuBjquqH8xu7thMWs8EkxwHvBDZW1YvoX9hxNvAh4KNV9XxgJ3D+Yl5fkg4W+wrgI55l2eoD2O4UsDrJFLAGuB94JU+dVXEZcMYBvL4kjb19BfC2JP9xfmOSC4BvL2aDVXUf/SvrfkI/eHvda81U1Wy32g7guD09P8nmJNuSbJuenl5MCZI0FvZ1FsS7gc8neQNPBe5G4FDg3y5mg0mOBDbRv5nPDP0f/Dx9oc+vqi3AFoCNGzfWPlaXpLH1rAFcVT8FfjPJK4AXdc3/q6puPIBt/g5wT1VNAyS5BngZcESSqa4XvB647wC2IUljb6H3A94KbF2ibf4EODXJGuDn9G/uvq17/TPp32f4PODaJdqeJI2lkV/NVlU30f+y7TvAD7satgB/AlyYZDvwHOCSUdcmSaO00CvhllRVvR94/7zmu4FTGpQjSU14PwdJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAlqRGDGBJasQAHlBV9Ho9qqp1KZImgAE8YHbXo1ywZSu9Xq91KZImgAE8z9SqNa1LkDQhDGBJasQAlqRGDGBJasQAlqRGmgRwkiOSXJ3kjiS3J3lpkqOSXJ/kru7xyBa1SdKotOoBfwz4SlX9c+A3gNuBi4AbqupE4IZuXpKWrZEHcJLDgd8GLgGoql9U1QywCbisW+0y4IxR1yZJo9SiB7wBmAb+Ksl3k3w6yVrgmKq6v1vnAeCYPT05yeYk25Jsm56eHlHJkrT0WgTwFHAycHFVvRh4hHnDDdW/FniP1wNX1Zaq2lhVG9etW7fkxc1djjwzM+MlyZKGqkUA7wB2VNVN3fzV9AP5p0mOBegeH2xQG7O7HuWtl9/MuRff6CXJkoZq5AFcVQ8A9yZ5Ydd0GnAbcB1wXtd2HnDtqGubs2L1WlasPqzV5iVNiKlG230H8NkkhwJ3A2+m/z+Dq5KcD/wYOKtRbZI0Ek0CuKq+B2zcw6LTRl2LJLXilXCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMCS1IgBLEmNGMB7Mffz9P40vaRhMYD34vFdj3DBlq3+NL2koTGAn8XUqjWtS5C0jBnAktSIASxJjRjAktSIASxJjRjAktSIASxJjTQL4CSHJPluki928xuS3JRke5IrkxzaqjZJGoWWPeB3AbcPzH8I+GhVPR/YCZzfpKoBXg0naZiaBHCS9cDvAp/u5gO8Eri6W+Uy4IwWtQ2a3fWoV8NJGppWPeC/AN4DPNnNPweYqarZbn4HcNyenphkc5JtSbZNT08PvVCvhpM0LCMP4CSvAx6sqm8v5vlVtaWqNlbVxnXr1i1xdZI0OlMNtvky4PVJXgusAn4F+BhwRJKprhe8HrivQW2SNDIj7wFX1Xuran1VnQCcDdxYVW8AtgJndqudB1w76tokaZTG6TzgPwEuTLKd/pjwJY3rkaShajEEsVtV/R3wd9303cApLeuRpFEapx6wJE0UA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA1iSGjGAJakRA3gfqoper0dVtS5F0jJjAO/D7K5HuWDLVnq9XutSJC0zBvACTK1a07oEScuQASxJjUxcAM+N6UpSaxMXwL1ej7d88svMPjHbuhRJE27iAhhgxcq1rUuQpMkMYEkaBwawJDViAEtSIwawJDUy8gBOcnySrUluS3Jrknd17UcluT7JXd3jkaOuTZJGqUUPeBb4o6o6CTgVeHuSk4CLgBuq6kTghm5ekpatkQdwVd1fVd/ppv8JuB04DtgEXNatdhlwxqhrk6RRajoGnOQE4MXATcAxVXV/t+gB4Ji9PGdzkm1Jtk1PT4+kTkkahmYBnOQw4G+Ad1fVw4PLqn/vxz3e/7GqtlTVxqrauG7duhFUKknD0SSAk6ygH76frapruuafJjm2W34s8GCL2iRpVFqcBRHgEuD2qvrIwKLrgPO66fOAa0ddmySN0lSDbb4MOBf4YZLvdW1/CnwQuCrJ+cCPgbMa1CZJIzPyAK6qbwLZy+LTRlnLQg3ewvLwww+n34mXpAPjlXALMLvrUd56+c2ce/GN3ktY0pJpMQRxUFqxei1TU+4uSUvHHrAkNWIAS1IjBvB+qipmZmboXysiSYtnAO+nhx9+mLM/8gW/jJN0wAzgRVixyt+Uk3TgDGBJasQAlqRGDGBJasQAXoS5S5MHz4Tw7AhJ+8sAXoTZXY9ywZatTzsTotfreXaEpP1iAC/S1Ko1z2jz7AhJ+8MAXgKDd0uTpIUygJdAr9fjLZ/8MrNPzLYuRdJBxADeD8/W012x0uEHSfvHAN4Pj+96hHde9k17upKWhAG8n6ZWrn7avOO/khbLAD5Ajv9KWix/4mGRBnu+K1auNYAl7TcDeJHmfieuHt9l+EpaFIcgDsCK1WuZWv3U2Q9zveKZmRmefPLJp12a7KXKkuYzgJfQXK/49/7yBm655ZanXZo8d6nyzMyMQSwJMICX3IrVayHhnZd9k6xY+fRlq9b6ixqSdjOAh2Rq5eo93jWtv2zN7qGKA+0J721owyEPafwZwEO0p7umzbW/9fKbOffiGw+4J7y3u7B5dzZp/BnAQzZ417Snnbq2ei0rVh+2u6c696XdQnutVcXOnTuZmZnZfRe2+b3evbVLGg8G8AjN7nr0GZcyz40J33vvvZx78Y0L7hX3ej3O/LMredMnvrT79ewNSweXsQvgJKcnuTPJ9iQXta7nQM31eufCb/6lzPBUT3XF6sOYWrWWmZkZdu7c+bRT2Hbu3MnOnTt395R7vR4rVq592lhzr9fb6z2J99a+rzHkPZ1Kt5je9GKeO7+2pezJz73W3H5d7GsOo6ZRfFLxU9H+Gdb+GqsATnII8EngNcBJwDlJTmpb1YGZG+/9T5d8Y0EXbDy+6xHe9IkvcdaHr33aKWxn/tmVnPXha3f3lAdfb3+3MejZes3ze+R7atuf7ezvc+fXtpQ9+bl6zvnoF5+2rxfzOktZ06g+qfipaP8Ma3+N25VwpwDbq+pugCSfAzYBty3lRh5/7BGeeOLJ/vTPH+lfzfbYz59avsRtK1YfBvDM9aYOodfr8fiuR/qPP/9/zP78kd3rDAbP/Lb5r7d7G7se3b3O3OvOnx60p23sbZt7q2MhFvPc+dvdW62L8WzvdzGvs5Q1jSqAR7Wt5WBY+ynj9BEkyZnA6VV1QTd/LvCvq+oPBtbZDGzuZl8I3LmfmzkaeGgJyj3YuR/cB3PcD8PfBw9V1enzG8etB7xPVbUF2LLY5yfZVlUbl7Ckg5L7wX0wx/3Qbh+M1RgwcB9w/MD8+q5NkpadcQvg/wOcmGRDkkOBs4HrGtckSUMxVkMQVTWb5A+ArwKHAJdW1a1LvJlFD18sM+4H98Ec90OjfTBWX8JJ0iQZtyEISZoYBrAkNTJRAbzcLnMelOT4JFuT3Jbk1iTv6tqPSnJ9kru6xyO79iT5eLcvfpDk5IHXOq9b/64k57V6T4uV5JAk303yxW5+Q5Kbuvd6ZfcFL0lWdvPbu+UnDLzGe7v2O5O8us07WbwkRyS5OskdSW5P8tJJOxaS/GH3b+GWJFckWTV2x0JVTcQf/S/1/gF4HnAo8H3gpNZ1LeH7OxY4uZv+ZeBH9C/n/nPgoq79IuBD3fRrgS8DAU4FburajwLu7h6P7KaPbP3+9nNfXAj8T+CL3fxVwNnd9KeA3++m3wZ8qps+G7iymz6pOz5WAhu64+aQ1u9rP/fBZcAF3fShwBGTdCwAxwH3AKsHjoE3jduxMEk94N2XOVfVL4C5y5yXhaq6v6q+003/E3A7/YNwE/1/jHSPZ3TTm4DLq+9bwBFJjgVeDVxfVT+rqp3A9cAzruAZV0nWA78LfLqbD/BK4Opulfn7YG7fXA2c1q2/CfhcVT1WVfcA2+kfPweFJIcDvw1cAlBVv6iqGSbsWKB/ltfqJFPAGuB+xuxYmKQAPg64d2B+R9e27HQfn14M3AQcU1X3d4seAI7ppve2Pw72/fQXwHuAJ7v55wAzVTV3l6LB97P7vXbLe936B/s+2ABMA3/VDcV8OslaJuhYqKr7gA8DP6EfvD3g24zZsTBJATwRkhwG/A3w7qp6eHBZ9T9TLdvzDpO8Dniwqr7dupbGpoCTgYur6sXAI/SHHHabgGPhSPq91w3ArwFrGcPe+yQF8LK/zDnJCvrh+9mquqZr/mn3cZLu8cGufW/742DeTy8DXp/k/9IfYnol8DH6H6nnLjoafD+732u3/HDgHzm49wH0e2k7quqmbv5q+oE8ScfC7wD3VNV0VT0OXEP/+BirY2GSAnhZX+bcjVddAtxeVR8ZWHQdMPft9XnAtQPtb+y+AT8V6HUfT78KvCrJkV0v4lVd29irqvdW1fqqOoH+f98bq+oNwFbgzG61+ftgbt+c2a1fXfvZ3TfjG4ATgZtH9DYOWFU9ANyb5IVd02n0b+k6MccC/aGHU5Os6f5tzO2D8ToWWn9bOco/+t/2/oj+N5nva13PEr+336L/kfIHwPe6v9fSH8e6AbgL+BpwVLd+6N/8/h+AHwIbB17rLfS/bNgOvLn1e1vk/ng5T50F8bzuH8124K+BlV37qm5+e7f8eQPPf1+3b+4EXtP6/Szi/f8rYFt3PPwt/bMYJupYAP4LcAdwC/A/6J/JMFbHgpciS1IjkzQEIUljxQCWpEYMYElqxACWpEYMYElqxACWpEYMYElq5P8DbZ1oTg62+/wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax4L4_5qLQNx"
      },
      "source": [
        "Vemos que tenemos varios documentos que tienen más de 1000 palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC97vk_R6LY7"
      },
      "source": [
        "#### Dividir las secuencias en subsecuencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LmORHVLZ4M"
      },
      "source": [
        "Supongamos que el modelo del que disponemos **no puede procesar secuencias de mas de 200 palabras**. ¿Cómo resolver esta limitación y aplicar el modelo sobre este conjunto de datos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v77oYtPm6LY7"
      },
      "source": [
        "Crearemos una funcion `split_to_sequences` que toma un texto de cantidad arbitraria de palabras y lo transforma en un arreglo de M textos o secuencias donde cada secuencia tiene como máximo `sequence_len` palabras. Cada secuencia comienza con `sequence_len - text_len` palabras de la secuencia anterior para poder retener el contexto de la oración, generando así un `rolling window` \n",
        "\n",
        "Como ejemplo, la siguiente imágen muestra un texto donde se aplicó esta transformación utilizando `sequence_len=5` y `text_len=2`\n",
        "\n",
        "<img src='https://github.com/santiagxf/M72109/blob/master/docs/nlp/_images/rolling_text.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wp1ry276LY7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_to_sequences(text, text_len=150, sequence_len=200):\n",
        "    assert(text_len<sequence_len)\n",
        "    \n",
        "    sequences = []\n",
        "    l_parcial = []\n",
        "    \n",
        "    if len(text.split())//text_len>0:\n",
        "        nb_sequences = len(text.split())//text_len\n",
        "    else: \n",
        "        nb_sequences = 1\n",
        "    \n",
        "    for seq in range(nb_sequences):\n",
        "        if seq == 0:\n",
        "            l_parcial = text.split()[:sequence_len]\n",
        "            sequences.append(\" \".join(l_parcial))\n",
        "        else:\n",
        "            l_parcial = text.split()[seq*text_len:seq*text_len + sequence_len]\n",
        "            sequences.append(\" \".join(l_parcial))\n",
        "    \n",
        "    return np.array(sequences)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cFVWe1Z6LY7"
      },
      "source": [
        "Verifiquemos como se aplica en uno de los textos que tenemos disponibles. Buscaremos un texto con 1200 caracteres:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcgx3ZPpFnHJ"
      },
      "source": [
        "long_sequences = df.loc[df.text.str.len() > 1200]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3iyvcm1HQN4",
        "outputId": "a1f1d89a-a2bd-4db2-97bd-4378cf64b7fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_text = long_sequences['text'].iloc[0]\n",
        "print(sample_text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: jhwitten@cs.ruu.nl (Jurriaan Wittenberg)\n",
            "Subject: Re: Magellan Update - 04/16/93\n",
            "Organization: Utrecht University, Dept. of Computer Science\n",
            "Keywords: Magellan, JPL\n",
            "Lines: 29\n",
            "\n",
            "In <19APR199320262420@kelvin.jpl.nasa.gov> baalke@kelvin.jpl.nasa.gov \n",
            "(Ron Baalke) writes:\n",
            "\n",
            ">Forwarded from Doug Griffith, Magellan Project Manager\n",
            ">\n",
            ">                        MAGELLAN STATUS REPORT\n",
            ">                            April 16, 1993\n",
            ">\n",
            ">\n",
            ">2.  Magellan has completed 7225 orbits of Venus and is now 39 days from\n",
            ">the end of Cycle-4 and the start of the Transition Experiment.\n",
            "Sorry I think I missed a bit of info on this Transition Experiment. What is it?\n",
            "\n",
            ">4.  On Monday morning, April 19, the moon will occult Venus and\n",
            ">interrupt the tracking of Magellan for about 68 minutes.\n",
            "Will this mean a loss of data or will the Magellan transmit data later on ??\n",
            "\n",
            "BTW: When will NASA cut off the connection with Magellan?? Not that I am\n",
            "looking forward to that day but I am just curious. I believe it had something\n",
            "to do with the funding from the goverment (or rather _NO_ funding :-)\n",
            "\n",
            "ok that's it for now. See you guys around,\n",
            "Jurriaan.\n",
            " \n",
            "-- \n",
            "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
            "|----=|=-<- - - - - - JHWITTEN@CS.RUU.NL- - - - - - - - - - - - ->-=|=----|\n",
            "|----=|=-<-Jurriaan Wittenberg- - -Department of ComputerScience->-=|=----|\n",
            "|____/|\\_________Utrecht_________________The Netherlands___________/|\\____|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQFVNXxr6LY7",
        "outputId": "1f68fb9b-7bda-4c2f-83cf-363cb865e8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transformed = split_to_sequences(sample_text, 40, 60)\n",
        "\n",
        "for sequence in transformed:\n",
        "  print('********')\n",
        "  print(sequence)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********\n",
            "From: jhwitten@cs.ruu.nl (Jurriaan Wittenberg) Subject: Re: Magellan Update - 04/16/93 Organization: Utrecht University, Dept. of Computer Science Keywords: Magellan, JPL Lines: 29 In <19APR199320262420@kelvin.jpl.nasa.gov> baalke@kelvin.jpl.nasa.gov (Ron Baalke) writes: >Forwarded from Doug Griffith, Magellan Project Manager > > MAGELLAN STATUS REPORT > April 16, 1993 > > >2. Magellan has completed 7225 orbits of Venus and is now 39 days from\n",
            "********\n",
            "> April 16, 1993 > > >2. Magellan has completed 7225 orbits of Venus and is now 39 days from >the end of Cycle-4 and the start of the Transition Experiment. Sorry I think I missed a bit of info on this Transition Experiment. What is it? >4. On Monday morning, April 19, the moon will occult Venus and >interrupt\n",
            "********\n",
            "on this Transition Experiment. What is it? >4. On Monday morning, April 19, the moon will occult Venus and >interrupt the tracking of Magellan for about 68 minutes. Will this mean a loss of data or will the Magellan transmit data later on ?? BTW: When will NASA cut off the connection with Magellan?? Not that I am looking forward\n",
            "********\n",
            "data later on ?? BTW: When will NASA cut off the connection with Magellan?? Not that I am looking forward to that day but I am just curious. I believe it had something to do with the funding from the goverment (or rather _NO_ funding :-) ok that's it for now. See you guys around, Jurriaan. -- -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=- |----=|=-<- -\n",
            "********\n",
            "goverment (or rather _NO_ funding :-) ok that's it for now. See you guys around, Jurriaan. -- -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=- |----=|=-<- - - - - - JHWITTEN@CS.RUU.NL- - - - - - - - - - - - ->-=|=----| |----=|=-<-Jurriaan Wittenberg- - -Department of ComputerScience->-=|=----| |____/|\\_________Utrecht_________________The Netherlands___________/|\\____|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2WZ2Gsa6LY7"
      },
      "source": [
        "Apliquemoslo sobre todo el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE2mANQy6LY7"
      },
      "source": [
        "df.loc[:,'text'] = df['text'].apply(split_to_sequences)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hov3C-Rv6LY8"
      },
      "source": [
        "#### Generando un nuevo dataset para entrenar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbrDVfl06LY8"
      },
      "source": [
        "Hasta el momento disponemos de un dataset donde una de sus columnas es un arreglo de secuencias de texto. Esta estructura de datos no puede ser utilizada con un modelo de procesamiento de texto y por lo tanto es necesario \"aplanarla\". Esto quiere decir que debemos convertir los elementos del arreglo en filas de nuestro dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhT_8IxB6LY8"
      },
      "source": [
        "El método `explode` transforma un data frame donde una de sus columnas es un arreglo, en otro data frame donde los elementos del arreglo se transforman en filas y los restantes valores son duplicados. Este método nos ayudará en este caso a que todas las subsecuencias que se generaron de la misma secuencia reciban el mismo `memorability_score`. El efecto de explode es el siguiente:\n",
        "\n",
        "<img src='https://github.com/santiagxf/M72109/blob/master/docs/nlp/_images/explode.png?raw=1' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mDjItAw6LY8"
      },
      "source": [
        "df = df.explode('text').reset_index(drop=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwN8DNQc6LY8",
        "outputId": "d3796c97-ee27-4bd1-d10f-93753f8af25d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2119, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_D6GPOpIGoZ"
      },
      "source": [
        "> Note como pasamos de tener 1073 filas a tener 2119. Las filas extras que se agregaron corresponden a secuencias de texto que eran demasiado largas y que fueron divididas en subsecuencias, aunque manteniendo la misma anotación de la secuencia original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQfFzkhq6LY8"
      },
      "source": [
        "Resultado final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mPGKjEn6LY8",
        "outputId": "4cec98b2-f67e-4409-e9bc-e47fac719258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: bil@okcforum.osrhe.edu (Bill Conner) Sub...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: jhwitten@cs.ruu.nl (Jurriaan Wittenberg)...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: sysmgr@king.eng.umd.edu (Doug Mohney) Su...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>assumption that he who gets there firstest wit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: pgf@srl03.cacs.usl.edu (Phil G. Fraering...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>From: Nanci Ann Miller &lt;nm0w+@andrew.cmu.edu&gt; ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>From: cain@geomag.gly.fsu.edu (Joe Cain) Subje...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>From: bradfrd2@ncar.ucar.edu (Mark Bradford) S...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>posted at regular intervals, every 30 days. Al...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>and terms, mostly in German (which I read, for...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  category\n",
              "0  From: bil@okcforum.osrhe.edu (Bill Conner) Sub...         0\n",
              "1  From: jhwitten@cs.ruu.nl (Jurriaan Wittenberg)...         1\n",
              "2  From: sysmgr@king.eng.umd.edu (Doug Mohney) Su...         1\n",
              "3  assumption that he who gets there firstest wit...         1\n",
              "4  From: pgf@srl03.cacs.usl.edu (Phil G. Fraering...         1\n",
              "5  From: Nanci Ann Miller <nm0w+@andrew.cmu.edu> ...         0\n",
              "6  From: cain@geomag.gly.fsu.edu (Joe Cain) Subje...         1\n",
              "7  From: bradfrd2@ncar.ucar.edu (Mark Bradford) S...         1\n",
              "8  posted at regular intervals, every 30 days. Al...         1\n",
              "9  and terms, mostly in German (which I read, for...         1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBOeG-bG6LY8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}