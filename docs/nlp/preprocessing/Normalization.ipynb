{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Preprocesamiento de texto\n",
    "======================"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introducción"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cuando hablamos de entrenar un modelo de aprendizaje automático, en general ocupamos una porción de nuestro tiempo en preprocesar los datos para generar representaciones útiles y deshacernos de problemas especificos que podría exhibir nuestro conjunto de datos. En particular, para el procesamiento del lenguaje natural, sabemos que debemos representar nuestras palabras de forma vectorial utilizando un vocabulario. También sabemos que el tamaño del vocabulario es algo que deseamos manejar.\n",
    "\n",
    "Tipicamente, las siguientes técnicas se aplican para procesar el texto:\n",
    "\n",
    " - [Normalización](#Normalización)\n",
    " - [Stemming y Lemmatization](#Stemming-y-Lemmatization)\n",
    " - [Eliminación de stopwords](#Eliminación-de-stopwords)\n",
    " - [Tokenización](#Tokenización)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Para ejecutar este notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/preprocessing/Normalization.txt --quiet --no-clobber\n",
    "!pip install -r Normalization.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalización"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La normalización de texto hace referencia al proceso por el cual transformamos el texto en una única forma canónica común. Normalizar el texto antes de almacenarlo o procesarlo permite liberarnos de preocupaciones posteriores, ya que se garantiza que la entrada sea consistente antes de que se realicen operaciones sobre el mismo. La normalización del texto, sin embargo, requiere saber qué tipo de texto se está normalizando y cómo se procesará posteriormente. Por lo tanto, no existe un procedimiento de normalización universal.\n",
    "\n",
    "A pesar de no existir un proceso univeral, algunas técnicas si son comunes, como por ejemplo eliminar caracteres no alfanuméricos o marcas diacríticas (acentos, dieresis) y sustitución de mayusculas por minúsculas. Otras tareas podrían ser más específicas como ser el tratamiento de direcciones URL o incluso algunas combinaciones de caracteres como ser los emojis, los hashtags, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementación"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para aquellas tareas sencillas, podemos utilizar algunas funciones pre-existentes. Para tareas más especificas, la utilización de expresiones regulares pueden ser de gran utilidad. Las expresiones regulares nos permiten buscar patrones específicos dentro de los textos. Veamos algunas trasnformaciones de texto.\n",
    "\n",
    "> Los siguientes ejemplos utilizan tweets reales extraidos del conjunto de datos [Spanish Corpus of Tweets for Marketing](http://ceur-ws.org/Vol-2111/paper1.pdf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "sample = \"Ecologistas en Acción valora positivamente la decisión de Carrefour España de dejar de vender panga… https://t.co/16RuHAeNhY\"\n",
    "print(sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ecologistas en Acción valora positivamente la decisión de Carrefour España de dejar de vender panga… https://t.co/16RuHAeNhY\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convertir el texto en minusculas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "sample = sample.lower()\n",
    "print(sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ecologistas en acción valora positivamente la decisión de carrefour españa de dejar de vender panga… https://t.co/16ruhaenhy\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Marcas diacríticas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import unidecode\n",
    "\n",
    "sample = unidecode.unidecode(sample)\n",
    "print(sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ecologistas en accion valora positivamente la decision de carrefour espana de dejar de vender panga... https://t.co/16ruhaenhy\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminación de caracteres especiales"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import re\n",
    "\n",
    "charsToKepp = r'[^a-zA-Z0-9\\s]'\n",
    "sample = re.sub(charsToKepp, '', sample)\n",
    "print(sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ecologistas en accion valora positivamente la decision de carrefour espana de dejar de vender panga httpstco16ruhaenhy\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es interesante revisar el ejemplo anterior, dado que el efecto que obtuvo eliminar los caracteres especiales no fué el más indicado. En este caso, quisieramos eliminar las URLs por completo en lugar de solamente los caracteres especiales que están dentro de ellas.\n",
    "\n",
    "> En general, deberiamos invertir el orden de la celda anterior con la celda siguiente (primero eliminar las URLs y luego los caracteres especiales."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import re\n",
    "\n",
    "urls_regex = re.compile('http\\S+')\n",
    "sample = [token for token in sample.split(' ') if not re.match(urls_regex, token)]\n",
    "print(' '.join(sample))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ecologistas en accion valora positivamente la decision de carrefour espana de dejar de vender panga\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como se puede ver, el procesamiento del texto a realizar dependerá mucho del contexto."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming y Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Existen palabras cuyo significado no cambia ya que estan atados a una palabra raiz que les da el significado:\n",
    "\n",
    "> Organizan, organiza, organizando, organizaron\n",
    "\n",
    "**Stemming y Lemmatization** son dos técnicas que generan la palabra raiz dada una palabra. La diferencia que hay entre estas técnicas es que **Lemmatization** utiliza reglas del lenguaje para extraer las palabras raiz y por lo tanto, el resultado son palabras que existen en el vocabulario. Por el contrario, **Stemming** utiliza heuristicas que truncan la palabra hasta su raiz invariable. El resultado son \"psudopalabras\" o mejor conocidos como tokens que no forman una palabra del lenguaje propiamente dicho. Esta técnica, como se puede intuir, es más rápida computacionalmente. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stemming (o en español `derivación`) es el proceso en el que estandarizamos las formas de las palabras a su raíz base independientemente de las inflexiones o cojugación en la que se encuentre."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para demostrar esta técnica utilizaremos la popular libreria de NLP `nltk`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from nltk import stem\n",
    "\n",
    "stemmer = stem.SnowballStemmer(language='spanish')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "words = ['amigos', 'amigo', 'amiga', 'amistad' ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "[stemmer.stem(word) for word in words]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['amig', 'amig', 'amig', 'amist']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "El proceso de `lemmatization` es similar al de `stemming` salvo que al no utilizar reglas del lenguaje para extraer las palabras raiz. Como consecuencia, el resultado es el vocablo raiz propiamente dicho."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para aplicar esta técnica utilizaremos la librería `spaCy`.\n",
    "\n",
    ">**Sobre la libreria spaCy:** Spacy es una libreria para NLP muy polupar actualmente ya que, al contrario de nltk, ofrece formas muy eficientes de hacer solo algunos tipos de operaciones. NLTK es una herramienta más general. Para instalar spaCy en español necesitaran ejecutar:\n",
    "\n",
    "```\n",
    "conda install -c spacy spacy\n",
    "python -m spacy download es_core_news_sm\n",
    "```\n",
    "\n",
    ">Si bien `ntlk` ofrece la opción de hacer Lemmatization, su soporte mayoritariamente es para ingles. La versión en español no es demasiado buena. Si les interesa probarla puede hacerlo a traves del metodo.\n",
    "\n",
    "```\n",
    "nltk.wordnet.lemas(\"palabra\", lang='spa')\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cargamos el modelo en español e instanciamos el parser:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python -m spacy download es_core_news_sm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import es_core_news_sm as spa\n",
    "parser = spa.load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos una funcion que nos ayuden a simplificar el uso de este método:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "words = ['amigos', 'amigo', 'amiga', 'amistad' ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "[lemmatizer(word) for word in words]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['amigo', 'amigar', 'amigo', 'amistar']"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Nota:** La precisión de Lemmatization depende de la implementación. La de español no es demasiado buena. Notar también lo que sucede con la palabra \"amigo\": ¿Es el verbo amigar o el sustantivo amigo?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adicionalmente, `spaCy` procesa el texto [tokenizándolo](#tokenización) en `tokens` y enriqueciendolos con anotaciones."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "words_tagged = parser(' '.join(words))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "for t in words_tagged:\n",
    "    print(t.text+'/'+t.lemma_ + '/'+ t.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "amigos/amigo/NOUN\n",
      "amigo/amigar/NOUN\n",
      "amiga/amigo/VERB\n",
      "amistad/amistar/NOUN\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Eliminación de stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algunas palabras que son extremadamente frecuentes, \"a-priori\" (revisaremos este concepto luego) no son de mucha utilidad para resolver una tarea de clasificación de texto específica. Estas palabras se las conoce como Stop words y, dado que son de poca utilidad, son eliminadas del texto.\n",
    "\n",
    "> **Spoiler Alert:** Mencionamos 'a priori', porque la tendencia general en los ultimos tiempos ha sido ir desde grandes listas de stop words en el order de 200-300 a listas muy pequeñas (10-15 - si es que las hay). Los buscadores, por ejemplo, hoy en día no eliminan estas palabras. Cuando veamos modelos de lenguaje, en realidad las vamos a necesitar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una de las formas más sencillas de eliminar estas palabras es utilizando la libreria `nltk` de la siguiente forma:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "nltk.download('stopwords', quiet=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "spa_stopwords = stopwords.words('spanish')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Revisemos como lucen estas palabras:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "spa_stopwords[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementación"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos implementar facilmente una rútina que elimine estas palabras de un texto de la siguiente forma:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "sample = \"ecologistas en accion valora positivamente la decision de carrefour espana de dejar de vender panga\"\n",
    "print('Antes:', sample)\n",
    "\n",
    "sample = ' '.join([token for token in sample.split(' ') if token not in spa_stopwords])\n",
    "print('Despues:', sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Antes: ecologistas en accion valora positivamente la decision de carrefour espana de dejar de vender panga\n",
      "Despues: ecologistas accion valora positivamente decision carrefour espana dejar vender panga\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenización"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se refiere al proceso de generación de tokens basado en un texto. A alto nivel, se podría ver como la tarea de dividir oraciones en palabras. Un token se diferencia de una palabra en el hecho de que una palabra es una instancia de un token. Existen varias técnicas para separar una oración o texto en general en tokens:\n",
    "\n",
    "> Lectura recomendada: [Diferentes `tokenizers` disponibles en `nltk`](http://www.nltk.org/api/nltk.tokenize.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tomemos un tweet de ejemplo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "sample = \". @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar\"\n",
    "print(sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ". @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instanciaremos un `tokenizer` del tipo `TreebankWordTokenizer`, uno de los más genéricos:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "tokenizer.tokenize(sample)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['.',\n",
       " '@',\n",
       " 'PoliciadeBurgos',\n",
       " '@',\n",
       " 'PCivilBurgos',\n",
       " '@',\n",
       " 'Aytoburgos',\n",
       " 'Mismo',\n",
       " 'peligro',\n",
       " 'c/',\n",
       " 'Rio',\n",
       " 'Viejo',\n",
       " 'junto',\n",
       " 'Mercadona',\n",
       " 'Villimar']"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Intentemos ahora con un `tokenizer` un poco más específico para procesar tweets:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "tokenizer.tokenize(sample)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['.',\n",
       " '@PoliciadeBurgos',\n",
       " '@PCivilBurgos',\n",
       " '@Aytoburgos',\n",
       " 'Mismo',\n",
       " 'peligro',\n",
       " 'c',\n",
       " '/',\n",
       " 'Rio',\n",
       " 'Viejo',\n",
       " 'junto',\n",
       " 'Mercadona',\n",
       " 'Villimar']"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Notar como el tratamiento del arroba resulta distinto dependiendo del `tokenizer` que estamos utilizando."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Otra estrategia\n",
    "El problema de reducir las palabras a sus formatos raiz radica en que en general cada palabra (separada por espacios, puntos, etc) conforma un elemento en nuestro vocabulario y no queremos diferentes elementos de nuestro vocabulario que mapeen al mismo elemento o concepto. Si por el contrario utilizaramos otra estrategia para determinar nuestro vocabulario (o mejor dicho, cada elemento de nuestro vocabulario) entonces este problema quizás no existiría (o se volvería peor).\n",
    "\n",
    "Este tipo de técnicas por lo general intentan representar el vocabulario con \"sub-palabras\" o partes de las palabras como unidad. Un ejemplo de esto es SentencePiece."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creando una rutina de preparación del texto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idealmente, podemos empaquetar todos los pasos relevantes del preprocesamiento de texto en una rutina coherente y consolidada. Esto es importante no solo por cuestiones de practicidad, sino que también es relevante dado que en todos estos pasos **el orden en el que se ejecutan importa**. Una ejecución en un orden distinto al que se pensó o diseño podría lugar a perdida de información. Por ejemplo, ¿que pasaría si quisieramos procesar los *hashtags* de tweets de alguna manera si eliminaramos los caracteres especiales al principio?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una rutina podría ser la siguiente:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "import unidecode\n",
    "import spacy\n",
    "import es_core_news_sm as spa\n",
    "import re\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "parser = spa.load() # Cargamos el parser en español\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
    "stemmer = stem.SnowballStemmer(language='spanish') # Creamos un steammer\n",
    "lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)]) # Creamos un lemmatizer\n",
    "stopwords = set(stopwords.words('spanish')) # Instanciamos las stopwords en español\n",
    "urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
    "\n",
    "def normalize(text):\n",
    "    tokens = tokenizer.tokenize(text.lower()) # Tokenizamos el texto\n",
    "    tokens = [token for token in tokens if not re.match(urls_regex, token)] # Eliminamos URLs\n",
    "    tokens = [token for token in tokens if len(token) > 4] # Eliminamos palabras con menos de 4 letras\n",
    "    tokens = [token for token in tokens if token not in stopwords] # Eliminamos stopwords\n",
    "    tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
    "    tokens = [lemmatizer(token) for token in tokens] # Aplicamos lematization\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego podemos aplicar esta rutina facilmente a nuevo texto:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "sample = \"Vaya estafa de Mercadona. Voy y compro salsa de soja, curry, comino y sal sería buena idea. #Kiev https://t.co/Wej37UxCAs\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "normalize(sample)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['estafar',\n",
       " 'mercadona',\n",
       " 'comprar',\n",
       " 'salsa',\n",
       " 'curry',\n",
       " 'comino',\n",
       " 'bueno',\n",
       " '# kiev']"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}