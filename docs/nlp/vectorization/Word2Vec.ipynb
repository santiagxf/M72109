{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducción\n",
    "------------\n",
    "\n",
    "Word2Vec es un popular algoritmo desarrollado por Tomáš Mikolov en varias publicaciones [Mikolov et al., 2013b,a] que utiliza el concepto de representaciones densas y distribuidas. La idea principal de Word2Vec es que el significado de una palabra puede inferirse del contexto en el que aparece. En este sentido, el texto es **simplemente una secuencia de palabras donde el significado de cada una de ellas se puede entraer de las palabras que están antes y despues de la misma**. Podríamos decir que busca codificar cada palabra utilizando el contexto en el que aparece en lugar de la palabra propiamente dicha.\n",
    "\n",
    "    \"Dime con quien andas y te diré quien eres\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuición\n",
    "----------\n",
    "\n",
    "Si dos palabras distintas tienden a aparecer siempre en el mismo contexto, entonces las mismas deberían de compartir un espacio similar en las representaciones. Esto quiere decir que las representaciones de todo el vocabulario generan un espacio multidimensional donde la posición relativa de cada palabra con las otras tiene significado. Más aún, habría entonces direcciones dentro del espacio con signfiicados especificos.\n",
    "\n",
    "Definición de contexto\n",
    "----------------------\n",
    "\n",
    "La definición de contexto que utilicemos tiene un efecto importante en los resultados de los vectores que generamos para las palabras y en las propiedades que los mismos tienen. En la mayoría de los casos el contexto serán las palabras que rodean a cada palabra en cuestión - normalmente en la forma de una ventana alrededor de la misma - pero podría también ser la oración completa o el parrafo completo en donde aparece. \n",
    "\n",
    "En algunos incluso el contexto puede ser variable, donde el mismo es inferido dependiendo de la estructura sintáctica de donde aparece la palabra.\n",
    "\n",
    "### Ventana móvil\n",
    "\n",
    "Quizás la estrategia más típica es la de una ventana móvil en la cual el contexto está definido por la secuencia de `2m + 1` palabras. La pabra en la posición `m + 1` es la palabra *foco*, y las restantes palabras son las palabras *contexto*.\n",
    "\n",
    "El tamaño de la ventana tiene un efecto importante. Ventanas muy grandes tienden a generar directamente *tópicos* similares mientras que ventanas más pequeñas tenderán a generar vectores que son más funcionales o sintácticas.\n",
    "\n",
    "### Ventana móvil posicional\n",
    "\n",
    "El concepto de contexto utilizando una ventana movil hace que las palabras pierdan la noción del orden. Ni siquiera tenemos información de las palabras que aparecen antes o despues de la palabra foco. Esto se puede resolver utilizando contextos posicionales donde en lugar de codificar la palabra \"perro\" por ejemplo, codificamos \"perro:+2\" haciendo referencia a que la misma está 2 posiciones luego de la palabra foco.\n",
    "\n",
    "\n",
    "¿Cómo aprender estas representaciones?\n",
    "--------------------------------------\n",
    "\n",
    "La forma en la que Word2Vec genera estas representaciones es a través del modelado de una tarea de lenguaje especifica que es aprender una distribución probabilistica de palabras dado un contexto. Es decir que contruiremos un modelo que nos pueda decir, por cada palabra, la probabilidad de que una palabra dada aparezca en su contexto. Por ejemplo, dada la palabra \"sobiética\" la probabilidad de que la palabra \"unión\" o \"Rusia\" esté en el contexto es mucho más alta que la palabra \"perro\".\n",
    "\n",
    "Esto lo podemos lograr de dos formas:\n",
    "\n",
    "### Skip-grams (SG)\n",
    "\n",
    "En esta modalidad, la tarea a resolver consiste en dada una palabra y su contexto en el que aparece, por cada palabra en el contexto predecir cada una de las restantes palabras que la rodean.\n",
    "\n",
    "![Skip-grams](../_images/word2vec_sg.png)\n",
    "\n",
    "Esta variante desacopla las dependencias entre el contexto y cada palabra al asumir que cada elemento del contexto es independiente de cada uno. Esto quiere decir, que nuestro set de datos para resolver la tarea luciría algo como lo siguiente:\n",
    "\n",
    "![Skip-grams: conjunto de entrenamiento](../_images/word2vec_sg_train.png)\n",
    "\n",
    "La técnica de skip-gram fué la popularizada por Mikolov et al. [2013a] y es una de las técnicas más eficientes y róbustas para aprender vectores.\n",
    "\n",
    "### Continous-bag-of-words (CBOW)\n",
    "\n",
    "En esta modalidad, la tarea a resolver consiste en dada un contexto, predecir cada una de las palabras que están dentro del contexto utilizando las palabras restantes.\n",
    "\n",
    "![Continous-bag-of-words](../_images/word2vec_cbow.png)\n",
    "\n",
    "En esta variante el contexto se define como la suma de todas los vectores palabra, aunque perdiendo la información del orden. Esto quiere decir, que nuestro set de datos para resolver la tarea luciría algo como lo siguiente:\n",
    "\n",
    "![Continous-bag-of-words: conjunto de entrenamiento](../_images/word2vec_cbow_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento\n",
    "-------------\n",
    "\n",
    "En cualquiera de las anteriores modalidades, nuestro modelo será una red neuronal que tendrá una sola capa interna (hidden layer) donde la cantidad de unidades de la dicha capa está dada por la dimensionalidad de los vectores que queremos aprender. En el caso de Word2Vec, esta dimensionalidad es 300. Estos vectores los debemos generar para todas las palabras del vocabulario, por lo tanto nuestra capa interna está representada por una matriz de dimensiones *|V| x 300* (una columna por cada unidad neuronal).\n",
    "\n",
    "Al final del proceso, nuestro objetivo entonces es aprender esta matriz de pesos en la red neuronal, con la esperanza de que los vectores que se aprendieron hayan captura de alguna forma la distribución probabilistica de cada una de las palabras. Si nos ponemos a revisar esta configuración, la matriz resultante no es más que una tabla de búsqueda de vectores, que dada una palabra de nuestro vocabulario, nos devuelve el vector resultante.\n",
    "\n",
    "<img src=\"../_images/word2vec_lookup.png\" width=\"500\" style=\"margin: 0 auto 0\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo pre-entrenado en Español\n",
    "-------------------------------\n",
    "\n",
    "Utilizaremos un modelo pre-entrenado de Word2Vec que fué entrenado con un corpus en español (Spanish CoNLL17 corpus). Este modelo se entrenó trabajando con una ventana de 10 palabras, con el algoritmo Continuous bag of words y un negative-sampling de 5 palabras. Este modelo está almacenado en formato binario, por lo que lo cargamos utilizando el objeto `KeyedVectors` y el metodo `load_word2vec_format`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/vectorization/Word2Vec.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r Word2Vec.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento de codigo descarga el modelo preentrenado y lo almacena en el directorio `Models/Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./Models/Word2Vec\n",
    "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/model-es.bin \\\n",
    "    --quiet --no-clobber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar a utilizar este modelo, lo cargaremos desde el directorio anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, io, pathlib\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "path = os.path.abspath('/content/model-es.bin')\n",
    "embeddings = KeyedVectors.load_word2vec_format(datapath(path), binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si exploramos algunas de las propiedades del modelo veremos el tamaño del vocabulario y las dimensiones de nuestros vectores resultantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vocabulario es: 2656057\n",
      "El tamaño de los vectores es: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño del vocabulario es:\", len(embeddings.vocab))\n",
    "print(\"El tamaño de los vectores es:\", embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando las representaciones\n",
    "\n",
    "Podemos comenzar a indagar como lucen las representaciones vectoriales que se aprendieron utilizando el método most_similar. Este método nos devuelve las 10 primeras palabras más similares a la palabra que indicamos. La similaridad se computa utilizando cosine-similarity de las representaciones vectoriales de cada una de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emperatriz', 0.7729767560958862),\n",
       " ('princesa', 0.7713199257850647),\n",
       " ('sofía', 0.7699437141418457),\n",
       " ('coronaciónla', 0.7649692893028259),\n",
       " ('sofíala', 0.7604084014892578),\n",
       " ('virreina', 0.7558672428131104),\n",
       " ('categoría:talavera', 0.7512167692184448),\n",
       " ('isabel', 0.7497586011886597),\n",
       " ('sofíael', 0.735364556312561),\n",
       " ('emperatríz', 0.7345834374427795)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(\"reina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos consultar cuales son las palabras más disimilares a una determinada palabra. Esto lo hacemos especificando el parametro negative dentro del método `most_similar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86222880', 0.37661510705947876),\n",
       " ('cze-', 0.3465925455093384),\n",
       " ('confederaciónuefa', 0.3235578238964081),\n",
       " ('episodionumero=21', 0.3235578238964081),\n",
       " ('borderisrael', 0.3164479732513428),\n",
       " ('categoría:syntermitinae', 0.307456910610199),\n",
       " ('categoría:socialdemocracia', 0.28852400183677673),\n",
       " ('03.05.82', 0.285906583070755),\n",
       " ('categoría:lujanenses', 0.28335660696029663),\n",
       " ('hansa-sur', 0.2776678204536438)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(negative=\"reina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método anterior no es muy util en general, sin embargo nos permite introducir el concepto de aritmética sobre estos mismos vectores. Por ejemplo: Pordiamos consultar cual es la analogia de dos palabras. El siguiente ejemplo se leería así: A lo que Paris es a Francia, ¿cúal es la analogia de Madrid?. Esto se resolveria tomando \"Francia\", quitandole \"Paris\" y agregandole \"Madrid\"\n",
    "\n",
    "<img src=\"../_images/word2vec_vector_space.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'españa'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"francia\", \"madrid\"], negative=[\"paris\"])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barcelona'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"paris\", \"españa\"], negative=[\"francia\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos nos dice que quizás este espacio vectorial no es solamente un espacio donde vectores que están cerca los unos de los otros en el espacio tienen significados similares, sino que en realidad capturan el significado en una forma más produnda. Concretamente, que hay \"direcciones de significados\" en el espacio donde uno de puede mover. Veamos algunos ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la dirección de \"hacedor\"\n",
    " - Conducir es a conductor lo que limpiar es a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trapeador'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"conductor\", \"limpiar\"], negative=[\"conducir\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la dirección \"bebidas\"\n",
    " - El vino es a Francia lo que el whisky es a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bélgica'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"francia\", \"whisky\"], negative=[\"vino\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la dirección \"extremar\"\n",
    " - Bueno es a genial lo que malo es a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'malisimo'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"buenisimo\", \"malo\"], negative=[\"bueno\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización del espacio contino de word2vec\n",
    "---------------------------------------------\n",
    "\n",
    "El siguiente código utiliza un plugin de Tensorflow llamado Tensorboard que permite hacer una proyección en un espacio 3D (para luego hacer una proyección en un espacio 2D - el de la pantalla) de las representaciones de cada una de las palabras que están en el vocabulario. Esta visualización requiere tener instalado TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p '/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "logdir = '/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 19:11:52.646960: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-15 19:11:52.652627: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-15 19:11:52.655446: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    weights = tf.Variable(embeddings.vectors, )\n",
    "    checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "    checkpoint.save(os.path.join(logdir, 'embedding.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente codigo genera los \"labels\" asociados al espacio vectorial que cargamos anteriormente. Pueden descargar estos labels ya preprocesados desde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/meta.tsv \\\n",
    "    --quiet --no-clobber --directory-prefix '/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(logdir + '/vecs.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File vecs.tsv is already generated\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(logdir + '/vecs.tsv') == False:\n",
    "    with io.open(logdir + '/meta.tsv', 'w', encoding='utf-8') as metadata_file:\n",
    "        with io.open(logdir + '/vecs.tsv', 'w', encoding='utf-8') as vectors_file:\n",
    "            for index in tqdm(range(len(embeddings.index2word))):\n",
    "                word = embeddings.index2word[index]\n",
    "                vec = embeddings.vectors[index]\n",
    "                metadata_file.write(word + \"\\n\")\n",
    "                vectors_file.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "else:\n",
    "    print('File vecs.tsv is already generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up config\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'meta.tsv'\n",
    "projector.visualize_embeddings(logdir, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecute en una consola:\n",
    "\n",
    "```\n",
    "tensorboard --logdir /tmp/logdir\n",
    "```\n",
    "\n",
    "Debería poder visualizar en el navegador el espacio vectorial de las diferentes palabras capturadas por el modelo de Word2Vec. Pruebe buscando una palabra en la barra de navegación lateral, como por ejemplo \"avenida\" para ver cuales son las palabras cercanas dentro del espacio vectorial. Utilice el selector `neighbors` para limitar la cantidad de vecinos más próximos que se muestran en la pantalla."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Espacio vectorial para la palabra avenida](../_images/word2vec_tensorboard.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
