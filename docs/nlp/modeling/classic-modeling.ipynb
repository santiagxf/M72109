{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelado clásico de lenguaje natural\n",
    "==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwFoS39i_J_r"
   },
   "source": [
    "## Creando un pipeline de preprocesamiento de texto\n",
    "\n",
    "A pesar de que los métodos anteriores son no supervisados, son de utilidad para el modelado de de problemas no supervisados como supervisados. Para llevar estos métodos a un entorno práctico normalmente se construyen flujos de procesamiento como el que se muestra más abajo. Estos flujos se los llama Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLSZG2Gy_J_r"
   },
   "source": [
    "<img src='https://github.com/santiagxf/M72109/blob/master/NLP/Docs/atap_0406.png?raw=1' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdw5EvPI_J_s"
   },
   "source": [
    "A modo de ejemplo, vamos a utilizar la API de Scikit-Learn para generar cada uno de estos pasos y así construir un modelo que resuelva un problema de negocio de punta a punta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONzBUk9yhH9k"
   },
   "source": [
    "**¿Que es lo que vamos a hacer?**\n",
    "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
    " - Eliminación de stopwords\n",
    " - Tokenización\n",
    " - Stemming y Lemmatization\n",
    " - Procesamiento especico del tema\n",
    " - Creación de features utilizando algun metodo de reducción de dimensionalidad, SVD, LSI, LDA\n",
    "\n",
    ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estamos viendo tweets, algunos casos interesantes podrían ser:\n",
    " - Predecir el sector al que pertenece el tweet: Alimentación, Bebidas, etc.\n",
    " - Predecir el paso en el Marketing Funel al que pertece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
    "    \n",
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/modeling/classic-modeling.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r classic-modeling.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPpqVNrwSdhL"
   },
   "source": [
    "Primero importaremos algunas librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPfF_O0U_J9a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE_O7bEjLebd"
   },
   "source": [
    "## Sobre el set de datos con el que vamos a trabajar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8lcRTa_Li4e"
   },
   "source": [
    "Utilizaremos como ejemplo un set de datos en español que contiene tweets que diferentes usuarios han publicado en relación a diferentes marcas de productos u empresas en el rubro de alimentación, construcción, automoviles, etc. Estos tweets, a su vez, están asociados a una de las diferentes fases en el proceso de ventas (también conocido como Marketing Funel) y por eso están tagueados con las fases de:\n",
    " - Awareness – el cliente es conciente de la existencia de un producto o servicio\n",
    " - Interest – activamente expresa el interes de un producto o servicio\n",
    " - Evaluation – aspira una marca o producto en particular\n",
    " - Purchase – toma el siguiente paso necesario para comprar el producto o servicio\n",
    " - Postpurchase - realización del proceso de compra. El cliente compara la diferencia entre lo que deseaba y lo que obtuvo\n",
    "\n",
    "Referencia: [Spanish Corpus of Tweets for Marketing](http://ceur-ws.org/Vol-2111/paper1.pdf\n",
    "\n",
    "> Nota: La version de este conjunto de datos que utilizaremos aqui es una versión preprocesada del original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gc44Q7do_J9h"
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INJwReUXSs4K"
   },
   "source": [
    "Inspeccionamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Gd6EocPdG5A0",
    "outputId": "6d1df056-08d1-4e7d-89b8-342c80082c2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>MARCA</th>\n",
       "      <th>CANAL</th>\n",
       "      <th>AWARENESS</th>\n",
       "      <th>EVALUATION</th>\n",
       "      <th>PURCHASE</th>\n",
       "      <th>POSTPURCHASE</th>\n",
       "      <th>NC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>IKEA</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#tr Me ofrezco para montar muebles de Ikea - H...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>IKEA</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#VozPópuli Vozpópuli @voz_populi - #LoMásLeido...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>ALCAMPO</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#ZonaTecno Destacado: Todo lo que hay que sabe...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>CARREFOUR</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$Carrefour retira pez #Panga. OCU y grupos x #...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>CARREFOUR</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TEXTO  SECTOR      MARCA  \\\n",
       "0  #tablondeanuncios Funda nordica ikea #madrid h...  RETAIL       IKEA   \n",
       "1  #tr Me ofrezco para montar muebles de Ikea - H...  RETAIL       IKEA   \n",
       "2  #VozPópuli Vozpópuli @voz_populi - #LoMásLeido...  RETAIL    ALCAMPO   \n",
       "3  #ZonaTecno Destacado: Todo lo que hay que sabe...  RETAIL  CARREFOUR   \n",
       "4  $Carrefour retira pez #Panga. OCU y grupos x #...  RETAIL  CARREFOUR   \n",
       "\n",
       "       CANAL  AWARENESS  EVALUATION  PURCHASE  POSTPURCHASE  NC2  \n",
       "0  Microblog          0           0       0.0             0  1.0  \n",
       "1  Microblog          0           0       0.0             0  1.0  \n",
       "2  Microblog          0           0       0.0             0  1.0  \n",
       "3  Microblog          0           0       0.0             0  1.0  \n",
       "4  Microblog          0           0       0.0             0  1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "uXwJS2Og_J9l",
    "outputId": "7b354598-bf6f-41a6-dd63-afe7897e3d68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>SECTOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
       "      <td>RETAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>\"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...</td>\n",
       "      <td>TELCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>#CarlosSlim y Bimbo lanzarán un vehículo eléct...</td>\n",
       "      <td>ALIMENTACION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>‼🏎Toyota #Day, 4ruedas ,1/4 milla, 1 #pasión, ...</td>\n",
       "      <td>AUTOMOCION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>\"- Tú qué.\\n- Yo na.\"\\nConversaciones banco sa...</td>\n",
       "      <td>BANCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>- Cariño, te juro que sólo tenían Cruzcampo en...</td>\n",
       "      <td>BEBIDAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>#adidas #hockey Amenabar 2080 CABA https://t.c...</td>\n",
       "      <td>DEPORTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXTO        SECTOR\n",
       "0     #tablondeanuncios Funda nordica ikea #madrid h...        RETAIL\n",
       "725   \"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...         TELCO\n",
       "964   #CarlosSlim y Bimbo lanzarán un vehículo eléct...  ALIMENTACION\n",
       "1298  ‼🏎Toyota #Day, 4ruedas ,1/4 milla, 1 #pasión, ...    AUTOMOCION\n",
       "1748  \"- Tú qué.\\n- Yo na.\"\\nConversaciones banco sa...         BANCA\n",
       "2348  - Cariño, te juro que sólo tenían Cruzcampo en...       BEBIDAS\n",
       "3023  #adidas #hockey Amenabar 2080 CABA https://t.c...      DEPORTES"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby('SECTOR').head(1)[['TEXTO', 'SECTOR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-TetBhbiGMz"
   },
   "source": [
    "## Creando un paso de Pipeline para procesamiento de texto\n",
    "\n",
    "El paso más complejo que tenemos para crear es quizas el preprocesamiento del texto. Esto lo podemos encapsular en un modulo de Scikit-Learn. Esta libreria tiene 2 tipos de modulos:\n",
    " - Transformers\n",
    " - Estimators\n",
    "\n",
    "Los transformers toman un set de features y devuelven otro set de features, por eso es que reciben el nombre de \"trasformers\", porque basicamente transforman vectores. Los estimators, por el contrario, reciben un set de features y producen un podelo que aproxima, o estima, una variable target. Por este motivo, estos modulos reciben el nombre de \"estimators\".\n",
    "\n",
    "¿Qué modulo les parece que va a implementar nuestro proceso de \"preprocesamiento\" de texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WWuoi17_J_t"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import spacy\n",
    "import es_core_news_sm as spa\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, language='spanish'):\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "        self.parser = spa.load() # Cargamos el parser en español\n",
    "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
    "        self.stemmer = stem.SnowballStemmer(language=language) # Creamos un steammer\n",
    "        self.lemmatizer = lambda word : \" \".join([token.lemma_ for token in self.parser(word)]) # Creamos un lemmatizer\n",
    "        self.stopwords = set(stopwords.words(language)) # Instanciamos las stopwords en español\n",
    "        self.urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if not re.match(self.urls_regex, token)]\n",
    "        tokens = [token for token in tokens if len(token) > 4]\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
    "        tokens = [self.lemmatizer(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for doc in X:\n",
    "          yield ' '.join(self.process_text(text=doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeSpeGyC_J_0"
   },
   "source": [
    "Instanciamos nuestro preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir3pP6II_J_0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/santiagxf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8YY0A5wkPa0"
   },
   "source": [
    "Podemos ver como funciona nuestro modulo de preprocesamiento de texto al llamarlo con la función transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N89kgK3tkYUv",
    "outputId": "f71196b3-f049-4449-da0f-9224062e7fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar\n"
     ]
    }
   ],
   "source": [
    "tweet = tweets['TEXTO'][5]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIZm5SWMkgvI",
    "outputId": "ef2ff58c-0934-4de9-9cdd-6e771a7a5d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mismo peligrar Viejo juntar Mercadona Villimar']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(normalizer.transform([tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKl5iLwpl1Jm"
   },
   "source": [
    "*Nota: Usamso list() solamente porque transform es una operación de tipo lazy. Esto significa que no se ejecuta hasta que enumeramos los resultados*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDuoM0BRmESe"
   },
   "source": [
    "### Construyendo nuestro pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwhSKt53_J_w"
   },
   "source": [
    "Importamos algunas librerias que necesitaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bx5kQr6_J_w"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU4ykD8z_J_3"
   },
   "source": [
    "Instanciamos nuestro vectorizador, en este caso usando el método TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVIdgSc9_J_4"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSnMG5la_J_5"
   },
   "source": [
    "Instanciamos nuestro generador de features, que en este caso son los tópicos que LDA genere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7o18xNsj_J_6"
   },
   "outputs": [],
   "source": [
    "featurizer = LatentDirichletAllocation(n_components=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FumqlRDO_J__"
   },
   "source": [
    "Instanciamos nuestro clasificador que utilizará las features generadas hasta este momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mpzmpo__KAA"
   },
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(max_iter=10000, multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xkn_cWu_KAE"
   },
   "source": [
    "Creamos un pipeline que ejecute todos los pasos en secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAccQHAG_KAE"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
    "                           ('vectorizer', vectorizer),\n",
    "                           ('featurizer', featurizer),\n",
    "                           ('estimator', estimator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6pY5hsJmRzH"
   },
   "source": [
    "### Entrenando nuestro pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wHgLnnYmZH0"
   },
   "source": [
    "En este caso intentaremos predecir el sector al que pertenece un tweet en particular. Para ello, como en todo proceso de machine learning separaremos nuestros datos en training y testing, para poder evaluar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yAc9s1WmjlG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtWhdGYQn4Ey"
   },
   "source": [
    "El método fit intrenará nuestro modelo de punta a punta. Tomará unos minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mp-fZa4E_KAG"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOLZErx-oGIB"
   },
   "source": [
    "Es hora de ver que tan bien le fué a nuestro modelo en esta tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiQLKHrr_KAL"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "hbDgBh5M_KAP",
    "outputId": "fc4359a0-c815-4fd7-9247-f8f6c369751d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       0.00      0.00      0.00       110\n",
      "  AUTOMOCION       0.00      0.00      0.00       148\n",
      "       BANCA       0.28      0.11      0.15       198\n",
      "     BEBIDAS       0.25      0.28      0.26       223\n",
      "    DEPORTES       0.27      0.30      0.28       216\n",
      "      RETAIL       0.21      0.52      0.29       268\n",
      "       TELCO       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.23      1242\n",
      "   macro avg       0.14      0.17      0.14      1242\n",
      "weighted avg       0.18      0.23      0.18      1242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "089VY2OupEVK"
   },
   "source": [
    "¿Les parece que estás métricas son buenas? ¿Se les ocurre como mejorarlo? Algunas ideas:\n",
    " - ¿Quien funcionará mejor? ¿Stemmer o Lemmatization?\n",
    " - ¿Qué será mejor hacer con los hashtags? ¿Quitarlos?\n",
    " - ¿Que cantidad de factores latentes funcionará mejor? ¿7, 10, 200, 300?\n",
    " - ¿Es Logistic Regression el mejor clasificador que podemos probar? ¿Si subimos la cantidad de tópicos que me sería mejor utilizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XfLkgKApJpy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Topic Modeling.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
