{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelado cl√°sico de lenguaje natural\n",
    "==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwFoS39i_J_r"
   },
   "source": [
    "## Creando un pipeline de preprocesamiento de texto\n",
    "\n",
    "A pesar de que los m√©todos anteriores son no supervisados, son de utilidad para el modelado de de problemas no supervisados como supervisados. Para llevar estos m√©todos a un entorno pr√°ctico normalmente se construyen flujos de procesamiento como el que se muestra m√°s abajo. Estos flujos se los llama Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLSZG2Gy_J_r"
   },
   "source": [
    "<img src='https://github.com/santiagxf/M72109/blob/master/NLP/Docs/atap_0406.png?raw=1' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdw5EvPI_J_s"
   },
   "source": [
    "A modo de ejemplo, vamos a utilizar la API de Scikit-Learn para generar cada uno de estos pasos y as√≠ construir un modelo que resuelva un problema de negocio de punta a punta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONzBUk9yhH9k"
   },
   "source": [
    "**¬øQue es lo que vamos a hacer?**\n",
    "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
    " - Eliminaci√≥n de stopwords\n",
    " - Tokenizaci√≥n\n",
    " - Stemming y Lemmatization\n",
    " - Procesamiento especico del tema\n",
    " - Creaci√≥n de features utilizando algun metodo de reducci√≥n de dimensionalidad, SVD, LSI, LDA\n",
    "\n",
    ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estamos viendo tweets, algunos casos interesantes podr√≠an ser:\n",
    " - Predecir el sector al que pertenece el tweet: Alimentaci√≥n, Bebidas, etc.\n",
    " - Predecir el paso en el Marketing Funel al que pertece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
    "    \n",
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/modeling/classic-modeling.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r classic-modeling.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPpqVNrwSdhL"
   },
   "source": [
    "Primero importaremos algunas librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPfF_O0U_J9a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE_O7bEjLebd"
   },
   "source": [
    "## Sobre el set de datos con el que vamos a trabajar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8lcRTa_Li4e"
   },
   "source": [
    "Utilizaremos como ejemplo un set de datos en espa√±ol que contiene tweets que diferentes usuarios han publicado en relaci√≥n a diferentes marcas de productos u empresas en el rubro de alimentaci√≥n, construcci√≥n, automoviles, etc. Estos tweets, a su vez, est√°n asociados a una de las diferentes fases en el proceso de ventas (tambi√©n conocido como Marketing Funel) y por eso est√°n tagueados con las fases de:\n",
    " - Awareness ‚Äì el cliente es conciente de la existencia de un producto o servicio\n",
    " - Interest ‚Äì activamente expresa el interes de un producto o servicio\n",
    " - Evaluation ‚Äì aspira una marca o producto en particular\n",
    " - Purchase ‚Äì toma el siguiente paso necesario para comprar el producto o servicio\n",
    " - Postpurchase - realizaci√≥n del proceso de compra. El cliente compara la diferencia entre lo que deseaba y lo que obtuvo\n",
    "\n",
    "Referencia: [Spanish Corpus of Tweets for Marketing](http://ceur-ws.org/Vol-2111/paper1.pdf\n",
    "\n",
    "> Nota: La version de este conjunto de datos que utilizaremos aqui es una versi√≥n preprocesada del original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gc44Q7do_J9h"
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INJwReUXSs4K"
   },
   "source": [
    "Inspeccionamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Gd6EocPdG5A0",
    "outputId": "6d1df056-08d1-4e7d-89b8-342c80082c2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>MARCA</th>\n",
       "      <th>CANAL</th>\n",
       "      <th>AWARENESS</th>\n",
       "      <th>EVALUATION</th>\n",
       "      <th>PURCHASE</th>\n",
       "      <th>POSTPURCHASE</th>\n",
       "      <th>NC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>IKEA</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#tr Me ofrezco para montar muebles de Ikea - H...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>IKEA</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeido...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>ALCAMPO</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#ZonaTecno Destacado: Todo lo que hay que sabe...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>CARREFOUR</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$Carrefour retira pez #Panga. OCU y grupos x #...</td>\n",
       "      <td>RETAIL</td>\n",
       "      <td>CARREFOUR</td>\n",
       "      <td>Microblog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TEXTO  SECTOR      MARCA  \\\n",
       "0  #tablondeanuncios Funda nordica ikea #madrid h...  RETAIL       IKEA   \n",
       "1  #tr Me ofrezco para montar muebles de Ikea - H...  RETAIL       IKEA   \n",
       "2  #VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeido...  RETAIL    ALCAMPO   \n",
       "3  #ZonaTecno Destacado: Todo lo que hay que sabe...  RETAIL  CARREFOUR   \n",
       "4  $Carrefour retira pez #Panga. OCU y grupos x #...  RETAIL  CARREFOUR   \n",
       "\n",
       "       CANAL  AWARENESS  EVALUATION  PURCHASE  POSTPURCHASE  NC2  \n",
       "0  Microblog          0           0       0.0             0  1.0  \n",
       "1  Microblog          0           0       0.0             0  1.0  \n",
       "2  Microblog          0           0       0.0             0  1.0  \n",
       "3  Microblog          0           0       0.0             0  1.0  \n",
       "4  Microblog          0           0       0.0             0  1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "uXwJS2Og_J9l",
    "outputId": "7b354598-bf6f-41a6-dd63-afe7897e3d68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>SECTOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
       "      <td>RETAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>\"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...</td>\n",
       "      <td>TELCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>#CarlosSlim y Bimbo lanzar√°n un veh√≠culo el√©ct...</td>\n",
       "      <td>ALIMENTACION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>‚ÄºüèéToyota #Day, 4ruedas ,1/4 milla, 1 #pasi√≥n, ...</td>\n",
       "      <td>AUTOMOCION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>\"- T√∫ qu√©.\\n- Yo na.\"\\nConversaciones banco sa...</td>\n",
       "      <td>BANCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>- Cari√±o, te juro que s√≥lo ten√≠an Cruzcampo en...</td>\n",
       "      <td>BEBIDAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>#adidas #hockey Amenabar 2080 CABA https://t.c...</td>\n",
       "      <td>DEPORTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXTO        SECTOR\n",
       "0     #tablondeanuncios Funda nordica ikea #madrid h...        RETAIL\n",
       "725   \"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...         TELCO\n",
       "964   #CarlosSlim y Bimbo lanzar√°n un veh√≠culo el√©ct...  ALIMENTACION\n",
       "1298  ‚ÄºüèéToyota #Day, 4ruedas ,1/4 milla, 1 #pasi√≥n, ...    AUTOMOCION\n",
       "1748  \"- T√∫ qu√©.\\n- Yo na.\"\\nConversaciones banco sa...         BANCA\n",
       "2348  - Cari√±o, te juro que s√≥lo ten√≠an Cruzcampo en...       BEBIDAS\n",
       "3023  #adidas #hockey Amenabar 2080 CABA https://t.c...      DEPORTES"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby('SECTOR').head(1)[['TEXTO', 'SECTOR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-TetBhbiGMz"
   },
   "source": [
    "## Creando un paso de Pipeline para procesamiento de texto\n",
    "\n",
    "El paso m√°s complejo que tenemos para crear es quizas el preprocesamiento del texto. Esto lo podemos encapsular en un modulo de Scikit-Learn. Esta libreria tiene 2 tipos de modulos:\n",
    " - Transformers\n",
    " - Estimators\n",
    "\n",
    "Los transformers toman un set de features y devuelven otro set de features, por eso es que reciben el nombre de \"trasformers\", porque basicamente transforman vectores. Los estimators, por el contrario, reciben un set de features y producen un podelo que aproxima, o estima, una variable target. Por este motivo, estos modulos reciben el nombre de \"estimators\".\n",
    "\n",
    "¬øQu√© modulo les parece que va a implementar nuestro proceso de \"preprocesamiento\" de texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WWuoi17_J_t"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import spacy\n",
    "import es_core_news_sm as spa\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, language='spanish'):\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "        self.parser = spa.load() # Cargamos el parser en espa√±ol\n",
    "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
    "        self.stemmer = stem.SnowballStemmer(language=language) # Creamos un steammer\n",
    "        self.lemmatizer = lambda word : \" \".join([token.lemma_ for token in self.parser(word)]) # Creamos un lemmatizer\n",
    "        self.stopwords = set(stopwords.words(language)) # Instanciamos las stopwords en espa√±ol\n",
    "        self.urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if not re.match(self.urls_regex, token)]\n",
    "        tokens = [token for token in tokens if len(token) > 4]\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
    "        tokens = [self.lemmatizer(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for doc in X:\n",
    "          yield ' '.join(self.process_text(text=doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeSpeGyC_J_0"
   },
   "source": [
    "Instanciamos nuestro preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir3pP6II_J_0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/santiagxf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8YY0A5wkPa0"
   },
   "source": [
    "Podemos ver como funciona nuestro modulo de preprocesamiento de texto al llamarlo con la funci√≥n transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N89kgK3tkYUv",
    "outputId": "f71196b3-f049-4449-da0f-9224062e7fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar\n"
     ]
    }
   ],
   "source": [
    "tweet = tweets['TEXTO'][5]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIZm5SWMkgvI",
    "outputId": "ef2ff58c-0934-4de9-9cdd-6e771a7a5d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mismo peligrar Viejo juntar Mercadona Villimar']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(normalizer.transform([tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKl5iLwpl1Jm"
   },
   "source": [
    "*Nota: Usamso list() solamente porque transform es una operaci√≥n de tipo lazy. Esto significa que no se ejecuta hasta que enumeramos los resultados*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDuoM0BRmESe"
   },
   "source": [
    "### Construyendo nuestro pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwhSKt53_J_w"
   },
   "source": [
    "Importamos algunas librerias que necesitaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bx5kQr6_J_w"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU4ykD8z_J_3"
   },
   "source": [
    "Instanciamos nuestro vectorizador, en este caso usando el m√©todo TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVIdgSc9_J_4"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSnMG5la_J_5"
   },
   "source": [
    "Instanciamos nuestro generador de features, que en este caso son los t√≥picos que LDA genere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7o18xNsj_J_6"
   },
   "outputs": [],
   "source": [
    "featurizer = LatentDirichletAllocation(n_components=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FumqlRDO_J__"
   },
   "source": [
    "Instanciamos nuestro clasificador que utilizar√° las features generadas hasta este momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mpzmpo__KAA"
   },
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(max_iter=10000, multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xkn_cWu_KAE"
   },
   "source": [
    "Creamos un pipeline que ejecute todos los pasos en secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAccQHAG_KAE"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
    "                           ('vectorizer', vectorizer),\n",
    "                           ('featurizer', featurizer),\n",
    "                           ('estimator', estimator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6pY5hsJmRzH"
   },
   "source": [
    "### Entrenando nuestro pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wHgLnnYmZH0"
   },
   "source": [
    "En este caso intentaremos predecir el sector al que pertenece un tweet en particular. Para ello, como en todo proceso de machine learning separaremos nuestros datos en training y testing, para poder evaluar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yAc9s1WmjlG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtWhdGYQn4Ey"
   },
   "source": [
    "El m√©todo fit intrenar√° nuestro modelo de punta a punta. Tomar√° unos minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mp-fZa4E_KAG"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOLZErx-oGIB"
   },
   "source": [
    "Es hora de ver que tan bien le fu√© a nuestro modelo en esta tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiQLKHrr_KAL"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "hbDgBh5M_KAP",
    "outputId": "fc4359a0-c815-4fd7-9247-f8f6c369751d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       0.00      0.00      0.00       110\n",
      "  AUTOMOCION       0.00      0.00      0.00       148\n",
      "       BANCA       0.28      0.11      0.15       198\n",
      "     BEBIDAS       0.25      0.28      0.26       223\n",
      "    DEPORTES       0.27      0.30      0.28       216\n",
      "      RETAIL       0.21      0.52      0.29       268\n",
      "       TELCO       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.23      1242\n",
      "   macro avg       0.14      0.17      0.14      1242\n",
      "weighted avg       0.18      0.23      0.18      1242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "089VY2OupEVK"
   },
   "source": [
    "¬øLes parece que est√°s m√©tricas son buenas? ¬øSe les ocurre como mejorarlo? Algunas ideas:\n",
    " - ¬øQuien funcionar√° mejor? ¬øStemmer o Lemmatization?\n",
    " - ¬øQu√© ser√° mejor hacer con los hashtags? ¬øQuitarlos?\n",
    " - ¬øQue cantidad de factores latentes funcionar√° mejor? ¬ø7, 10, 200, 300?\n",
    " - ¬øEs Logistic Regression el mejor clasificador que podemos probar? ¬øSi subimos la cantidad de t√≥picos que me ser√≠a mejor utilizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XfLkgKApJpy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Topic Modeling.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
