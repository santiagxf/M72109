{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modelado Clásico - Actividad 1"
      ],
      "metadata": {
        "id": "it108cAwxRJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic modeling es una técnica de aprendizaje automático no supervisado donde intentados descubrir tópicos que son abstractos al texto pero que pueden describir una colección de documentos. Es importante marcar que estos \"tópicos\" no son necesariamente equivalentes a la interpretación coloquial de tópicos, sino que responden a un patrón que emerge de las palabras que están en los documentos.\n",
        "\n",
        "La suposición básica para Topic Modeling es que cada documento está representado por una mescla de tópicos, y cada tópico consite en una conlección de palabras."
      ],
      "metadata": {
        "id": "7VYPdzDTxRJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direcciones\n",
        "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
        "\n",
        " - Eliminación de stopwords\n",
        " - Tokenización\n",
        " - Stemming y Lemmatization\n",
        " - Procesamiento especico del tema\n",
        " - Creación de features utilizando algun metodo de reducción de dimensionalidad, SVD, LSI, LDA\n",
        "\n",
        ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estermos analizando tweets, predeciremos el sector al que pertenece el tweet: Alimentación, Bebidas, etc.\n",
        " \n",
        "En esta actividad les proponemos realizar cambios en alguna de las etapas del procesamiento para modificar la performance del modelo resultante y evaluar que cambios generan el mejor modelo resultante."
      ],
      "metadata": {
        "id": "VomDdG0PxRJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/santiagxf/M72109/blob/master/docs/nlp/_images/classic_pipeline.png?raw=1' />"
      ],
      "metadata": {
        "id": "YrD82rvfxRJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para ejecutar este notebook"
      ],
      "metadata": {
        "id": "5iw-sykcZWAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ],
      "metadata": {
        "id": "2ke-vHIJZWAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
        "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "    \n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/classic/topic-modeling.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r topic-modeling.txt --quiet"
      ],
      "outputs": [],
      "metadata": {
        "id": "21Xgn3NqxRJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "outputs": [],
      "metadata": {
        "id": "OOGkFv6YZWAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importaremos algunas librerias necesarias"
      ],
      "metadata": {
        "id": "WPpqVNrwSdhL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "metadata": {
        "id": "zPfF_O0U_J9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobre el set de datos con el que vamos a trabajar"
      ],
      "metadata": {
        "id": "lE_O7bEjLebd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizaremos como ejemplo un set de datos en español que contiene tweets que diferentes usuarios han publicado en relación a diferentes marcas de productos u empresas en el rubro de alimentación, construcción, automoviles, etc. Estos tweets, a su vez, están asociados a una de las diferentes fases en el proceso de ventas (también conocido como Marketing Funel) y por eso están tagueados con las fases de:\n",
        " - Awareness – el cliente es conciente de la existencia de un producto o servicio\n",
        " - Interest – activamente expresa el interes de un producto o servicio\n",
        " - Evaluation – aspira una marca o producto en particular\n",
        " - Purchase – toma el siguiente paso necesario para comprar el producto o servicio\n",
        " - Postpurchase - realización del proceso de compra. El cliente compara la diferencia entre lo que deseaba y lo que obtuvo\n",
        "\n",
        "Referencia: [Spanish Corpus of Tweets for Marketing](http://ceur-ws.org/Vol-2111/paper1.pdf\n",
        "\n",
        "> Nota: La version de este conjunto de datos que utilizaremos aqui es una versión preprocesada del original."
      ],
      "metadata": {
        "id": "H8lcRTa_Li4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "mnFmjMFkxRJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desarrollo"
      ],
      "metadata": {
        "id": "eYwp3XEcZWAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creando nuestros sets de datos de entrenamiento y testing"
      ],
      "metadata": {
        "id": "se8eN3hDZWAl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
        "                                                    test_size=0.33, \n",
        "                                                    stratify=tweets['SECTOR'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "fZ6wpf2wxRKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construcción del modelo: Pasos"
      ],
      "metadata": {
        "id": "1n_LptW1xRJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 1:** Instanciamos nuestro preprocesamiento de texto"
      ],
      "metadata": {
        "id": "fm4omFKExRJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import unidecode\n",
        "import spacy\n",
        "import es_core_news_sm as spa\n",
        "import re\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk import stem\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
        "    def __init__(self):\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "        self.parser = spa.load() # Cargamos el parser en español\n",
        "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
        "        self.stemmer = stem.SnowballStemmer(language='spanish') # Creamos un steammer\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words('spanish')) # Instanciamos las stopwords en español\n",
        "        self.urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
        "    \n",
        "    # Creamos una función lemmatizer\n",
        "    def lemmatizer(self, word):\n",
        "        return \" \".join([token.lemma_ for token in self.parser(word)])\n",
        "    \n",
        "    def process_text(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text) # Pasamos a tokens\n",
        "        tokens = [token for token in tokens if not re.match(self.urls_regex, token)] # Eliminamos URLs\n",
        "        tokens = [token for token in tokens if len(token) > 3] # Quitamos palabras con menos de 3 caractéres\n",
        "        tokens = [token for token in tokens if token not in self.stopwords] # Eliminamos stopwords\n",
        "        tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
        "        tokens = [self.lemmatizer(token) for token in tokens] # Aplicamos lematization\n",
        "        return tokens\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        for doc in X:\n",
        "            yield ' '.join(self.process_text(text=doc))"
      ],
      "outputs": [],
      "metadata": {
        "id": "uSJelhRfxRKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "normalizer = TextNormalizer()"
      ],
      "outputs": [],
      "metadata": {
        "id": "rbt-OdJFxRKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 2:** Instanciamos nuestro vectorizador, en este caso usando el método TF-IDF"
      ],
      "metadata": {
        "id": "XFY6eUe4xRKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
      ],
      "outputs": [],
      "metadata": {
        "id": "5oMARrKNxRKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 3:** Instanciamos nuestro generador de features"
      ],
      "metadata": {
        "id": "atOW2STAxRKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# featurizer = "
      ],
      "outputs": [],
      "metadata": {
        "id": "FUvY-6AjxRKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 4:** Instanciamos nuestro clasificador que utilizará las features generadas hasta este momento"
      ],
      "metadata": {
        "id": "gdi6dwy_xRKL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# estimator ="
      ],
      "outputs": [],
      "metadata": {
        "id": "wspdz2OZxRKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "fBLDj74FZWAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensamblamos el pipeline"
      ],
      "metadata": {
        "id": "uk5oDgUpxRKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
        "#                           ('vectorizer', vectorizer),\n",
        "#                           ('featurizer', featurizer),\n",
        "#                           ('estimator', estimator)])"
      ],
      "outputs": [],
      "metadata": {
        "id": "eVA3dR60xRKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación"
      ],
      "metadata": {
        "id": "NCHelkwGxRKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación:** Entrenamos el modelo y testeamos su performance"
      ],
      "metadata": {
        "id": "CXQsgKLaxRKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# model = pipeline.fit(X=X_train, y=y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OeSBrVY3xRKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# predictions = model.predict(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zbZSy77rxRKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# print(classification_report(y_test, predictions))"
      ],
      "outputs": [],
      "metadata": {
        "id": "uJ94y_xpxRKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Responda las siguentes preguntas\n",
        "\n",
        "> **Pista:** ¿Como podrían explorar esta multiplicidad de opciones de forma automática?\n",
        "\n",
        "En particular deberan contestar las siguientes preguntas:\n",
        "\n",
        " - ¿Que métodos de reducción de dimensionalidad resultan mejores?"
      ],
      "metadata": {
        "id": "nttneC2gZiHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Haga doble click para ingresar su respuesta*"
      ],
      "metadata": {
        "id": "sUvK--QbZyEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ¿Que numero de componentes hace sentido para estre problema?"
      ],
      "metadata": {
        "id": "QOY_rjcyZx3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Haga doble click para ingresar su respuesta*"
      ],
      "metadata": {
        "id": "noL3w-Y2ZxnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ¿Que tipo de modelos resultan mejores a la hora de ser utilizados como clasificadores? (estimator)"
      ],
      "metadata": {
        "id": "7GOJdmagZxRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Haga doble click para ingresar su respuesta*"
      ],
      "metadata": {
        "id": "xjs4fdWdZ3Ae"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Topic Modeling - Actividad.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}