{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f626a08",
   "metadata": {},
   "source": [
    "Modelos basados en N-Gramas\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2d0f6",
   "metadata": {},
   "source": [
    "Introducción\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0b362",
   "metadata": {},
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658132eb",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da532cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ngram-cnn.txt'\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
    "\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/normalization.py \\\n",
    "    --quiet --no-clobber --directory-prefix ./m72109/nlp/\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/transformation.py \\\n",
    "    --quiet --no-clobber --directory-prefix ./m72109/nlp/\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/ngram-cnn.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r ngram-cnn.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8aba5",
   "metadata": {},
   "source": [
    "Importamos algunas librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8906372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3deb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7db69",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bcdf53",
   "metadata": {},
   "source": [
    "Al igual que con Topic Modeling, nuestro primer paso es preprocesar el texto. Para focalizarnos en Word2Vec en este modulo, les preparé un modulo TweetTextNormalizer que hará todo el preprocesamiento por nosotros. Pueden explorar los parametros que recibe el constructor de esta clase para ver que opciones podemos configurar como Stemmer, Lemmatization, etc.\n",
    "\n",
    "En lo particular, estamos creando un TweetTextNormalizer que:\n",
    " - Aplicará un tokenizer especifico para Twitter\n",
    " - Eliminará stop words\n",
    " - Aplicará lemmatization\n",
    " - Eliminará URLs\n",
    " - Eliminará acentos\n",
    " - Eliminará las mayusculas\n",
    " \n",
    "Adicionalmente, el parametro text_to_sequence=True indica que la salida de este proceso no serán oraciones sino que tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524d3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from m72109.nlp.normalization import TweetTextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3670521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = TweetTextNormalizer(preserve_case=False, return_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd37dd4",
   "metadata": {},
   "source": [
    "## Vectorización de las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ba760",
   "metadata": {},
   "source": [
    "En las actividades anteriores utilizamos siempre un TF-IDF vectorizer para generar los vectores. En esta oportunidad utilizaremos Word2Vec utilizando un modelo pre-entrenado para el idioma español. Adicionalmente, vemos que este vectorizer tiene el parametro sequence_to_idx en Verdadero. Esto significa que no queremos que como salida obtengamos los vectores de Word2Vec, sino que queremos \"el indice\" que se corresponde a la palabra en una matriz de indice-palabra/vectores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31cedede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from m72109.nlp.transformation import Word2VecVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e4e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2VecVectorizer(model_path='Models/Word2Vec/model-es.bin', sequence_to_idx=True)\n",
    "embedding_weights = w2v.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ecdb3",
   "metadata": {},
   "source": [
    "### Ajustando la longitud de las secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a809f",
   "metadata": {},
   "source": [
    "Los modelos basados en secuencias pueden adaptarse a cualquier longitud de secuencia, sin embargo, los parametros de nuestras redes neuronales deberan ser fijos. Para esto definiermos una longitud máxima de la secuencia que vamos analizar. Para esto podemos utilizar un valor especifico o utilizar el valor máximo de tokens que hay en nuestro corpus.\n",
    "\n",
    "La siguiente clase PadSequenceTransformer es un modulo que les preparé para simplificar este procesamiento. El mismo se encarga de ajustar cualquier secuencia para que tenga exactamente max_seq_len. Cuando la lingitud es mejor, se completan con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36743d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from m72109.nlp.transformation import PadSequenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f20ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5a1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = PadSequenceTransformer(max_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa470a9b",
   "metadata": {},
   "source": [
    "El método get_weights() construye la matríz de indice-palabra/vector que luego será utilizado para encontrar los vectores correspondientes de cada palabra. Esta matriz tiene dimensiones m x n, donde m es la cantidad de palabras del vocabulario y n la dimensión de los vectores de word2vec. En este caso trabajamos con vectores de dimensionalidad 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5b51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = w2v.vocab_size\n",
    "embedding_dim = w2v.emdedding_size\n",
    "\n",
    "batch_size = 50\n",
    "feature_maps = 100\n",
    "n_gram = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f3830dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len, vocab_size, embedding_dim, embedding_weights):\n",
    "    model = keras.Sequential() #(batch_size, max_len)\n",
    "    model.add(keras.layers.Embedding(vocab_size, embedding_dim, input_shape=(max_len,)), trainable=False, mask_zero=True) #(batch_size, max_len, embedding)\n",
    "    model.add(keras.layers.Conv1D(filters = feature_maps, kernel_size = n_gram, activation = keras.activations.relu))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size = batch_size - n_gram + 1, strides = None, padding = 'valid'))\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(7, activation=keras.activations.softmax))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba06bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=build_model, \n",
    "    epochs=50,\n",
    "    max_len=max_seq_len,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=w2v.emdedding_size,\n",
    "    embedding_weights=embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbed749",
   "metadata": {},
   "source": [
    "## Creando nuestro pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9578d",
   "metadata": {},
   "source": [
    "Construimos nuestro pipeline como siempre combinando los diferentes pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea29d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
    "                           ('vectorizer', w2v),\n",
    "                           ('padder', seq2seq),\n",
    "                           ('estimator', estimator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ea0d7",
   "metadata": {},
   "source": [
    "Entrenamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d461f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5bf9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfde69",
   "metadata": {},
   "source": [
    "## Evalución de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825b56a",
   "metadata": {},
   "source": [
    "Probamos su performance utilizando el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85109ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a337409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
