{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQGxT74Sdis4"
      },
      "source": [
        "Modelos basados en secuencias con Word2Vec\n",
        "=========================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkwnMQ-DXuNk"
      },
      "source": [
        "Introducción\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjuHJnQOdis6"
      },
      "source": [
        "Los modelos basados en secuencias tienen la fortaleza que toman una secuencia de token (en un determinado orden) y generan una salida dependiendo del tipo de problema que se trate.\n",
        " - Seq2Class: Toman una secuencia de tokens y generan una clase\n",
        " - Seq2Seq: Toman una secuencia de token y generan otra secuencia de tokens.\n",
        "\n",
        "Vimos que cuando aplicamos técnicas de Topic Modeling, intentamos reducir la cantidad de dimensiones de nuestras representaciones de palabras para luego utilizar un clasificador para resolver la tarea en cuestión. Sin embargo, la suponsición básica de ese tipo de modelos es que un texto no es mas que una distribución de palabras (bag of words). Sin embargo, nosotros sabemos que un texto es una secuencia de palabras donde importa el orden. Para capturar este tipo de propiedades podemos utilizar modelos basados en secuencias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcyc_TQ6dis7"
      },
      "source": [
        "### Para ejecutar este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVZuIbB-XuNn"
      },
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "syRzC-FPXuNn"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
        "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/normalization.py \\\n",
        "    --quiet --no-clobber --directory-prefix ./m72109/nlp/\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/transformation.py \\\n",
        "    --quiet --no-clobber --directory-prefix ./m72109/nlp/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/sequences-word2vec.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r sequences-word2vec.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl18LLmkeqjx"
      },
      "source": [
        "Descargamos nuestros vectores de word2vec en español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LJVkiH2lesN1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./Models/Word2Vec\n",
        "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/model-es.bin \\\n",
        "    --quiet --no-clobber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ntcs1AlpfckX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rRpBRahditI"
      },
      "source": [
        "Instalamos las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgVf9-V7ditI",
        "outputId": "c943650a-abc0-408c-f102-396048dd4778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-17 23:01:25.369405: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-17 23:01:27.459300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC7vGpjqditL"
      },
      "source": [
        "Cargamos el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pmJpenUkditM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwRwMqoSditT"
      },
      "source": [
        "## Preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrYYZ0FditT"
      },
      "source": [
        "Al igual que con Topic Modeling, nuestro primer paso es preprocesar el texto. Para focalizarnos en Word2Vec en este modulo, les preparé un modulo TweetTextNormalizer que hará todo el preprocesamiento por nosotros. Pueden explorar los parametros que recibe el constructor de esta clase para ver que opciones podemos configurar como Stemmer, Lemmatization, etc.\n",
        "\n",
        "En lo particular, estamos creando un TweetTextNormalizer que:\n",
        " - Aplicará un tokenizer especifico para Twitter\n",
        " - Eliminará stop words\n",
        " - Aplicará lemmatization\n",
        " - Eliminará URLs\n",
        " - Eliminará acentos\n",
        " - Eliminará las mayusculas\n",
        "\n",
        "Adicionalmente, el parametro text_to_sequence=True indica que la salida de este proceso no serán oraciones sino que tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jnuzl8qbditU"
      },
      "outputs": [],
      "source": [
        "from m72109.nlp.normalization import TweetTextNormalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kFYzMdiCditX"
      },
      "outputs": [],
      "source": [
        "normalizer = TweetTextNormalizer(preserve_case=False, return_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sD5jK6ndita"
      },
      "source": [
        "Transformemos el texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldKc0udIditb",
        "outputId": "3bab56ef-06fe-4657-f225-3dd203d67246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3763/3763 [04:13<00:00, 14.86it/s]\n"
          ]
        }
      ],
      "source": [
        "tweets_text = normalizer.transform(tweets['TEXTO'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdoLyHg9ditd"
      },
      "source": [
        "## Vectorización de las palabras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9iuTrwXditd"
      },
      "source": [
        "En las actividades anteriores utilizamos siempre un TF-IDF vectorizer para generar los vectores. En esta oportunidad utilizaremos Word2Vec utilizando un modelo pre-entrenado para el idioma español. Adicionalmente, vemos que este vectorizer tiene el parametro sequence_to_idx en Verdadero. Esto significa que no queremos que como salida obtengamos los vectores de Word2Vec, sino que queremos \"el indice\" que se corresponde a la palabra en una matriz de indice-palabra/vectores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jE-SlBWddite"
      },
      "outputs": [],
      "source": [
        "from m72109.nlp.transformation import Word2VecVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkWxVLPYditg",
        "outputId": "c7abe4c2-8242-43cc-90a6-20725cce867d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
            "  warnings.warn(\n",
            "100%|██████████| 2656058/2656058 [00:09<00:00, 267102.64it/s]\n"
          ]
        }
      ],
      "source": [
        "w2v = Word2VecVectorizer(model='/content/model-es.bin', sequence_to_idx=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_text = w2v.transform(tweets_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3LWu-54d1Oy",
        "outputId": "353fe857-7796-4695-b5ef-55847ea7ffb2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3763/3763 [00:00<00:00, 19669.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNY37iIditm"
      },
      "source": [
        "## Construirmos un modelo basado en secuencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhfhArdbdit5"
      },
      "source": [
        "### Ajustando la longitud de las secuencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpivJzsdit6"
      },
      "source": [
        "Los modelos basados en secuencias pueden adaptarse a cualquier longitud de secuencia, sin embargo, los parametros de nuestras redes neuronales deberan ser fijos. Para esto definiermos una longitud máxima de la secuencia que vamos analizar. Para esto podemos utilizar un valor especifico o utilizar el valor máximo de tokens que hay en nuestro corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q2zBOXUdit6"
      },
      "source": [
        "La siguiente clase PadSequenceTransformer es un modulo que les preparé para simplificar este procesamiento. El mismo se encarga de ajustar cualquier secuencia para que tenga exactamente max_seq_len. Cuando la lingitud es mejor, se completan con ceros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NeVilfLMdit7"
      },
      "outputs": [],
      "source": [
        "from m72109.nlp.transformation import PadSequenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u-2T5o8Rdit9"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_text = seq2seq.transform(tweets_text)"
      ],
      "metadata": {
        "id": "Ja1tQUEhhDJ4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbwWFXUXuNv"
      },
      "source": [
        "### Construyendo el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnhHx7Pddito"
      },
      "source": [
        "Para construir nuestro modelo, utilizaremos TensorFlow. En particular utilizaremos la API de Keras que nos permite componer modelos de redes neuronales como una secuencia de pasos o capas que se conectan en una dirección.\n",
        "\n",
        "Utilizemos los siguientes tipos de capas:\n",
        "\n",
        " *  **Embedding:** Esta capa transforma vectores que representan indices dentro de una matriz en representaciones vectoriales densas. Básicamente en este caso nos resolverá la busqueda de las representaciones vectoriales para nuestras palabras.\n",
        " * **SpatialDropout1D:** Este tipo de capas ayudan a promover la independencia entre filtros (feature maps). Funciona en forma analoga a Dropout pero en lugar de desconectar elementos individuales, desconecta el filtro completo.\n",
        " * **LSTM:** Long Short-Term Memory layer - Hochreiter 1997\n",
        " * **Dense:** Una típica capa de una red neuronal completamente conectada (fully connected)\n",
        "\n",
        "Algunos detalles para notar:\n",
        "\n",
        " * `loss='sparse_categorical_crossentropy'`, este problema de clasificación (crossentropy) de más de una clase (categorical). Sin embargo, nuestro output produce probabilidades de cada una de las clases posibles (7) en forma one-hot encoding.\n",
        " * `metrics=['accuracy']`: Si bien nuestra metrica es accuracy, Keras hará un promedio ponderado del accuracy de cada clase. Este es el comportamiento por defecto."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = w2v.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wphj0eNl43g",
        "outputId": "714a7422-d8d7-4ace-b076-d20d512f928a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2656058/2656058 [00:10<00:00, 241610.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> El método `get_weights()` construye la matríz de indice-palabra/vector que luego será utilizado para encontrar los vectores correspondientes de cada palabra. Esta matriz tiene dimensiones m x n, donde m es la cantidad de palabras del vocabulario y n la dimensión de los vectores de word2vec. En este caso trabajamos con vectores de dimensionalidad 100."
      ],
      "metadata": {
        "id": "3FSKPr-al-8W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LQ2C980jditp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, SpatialDropout1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nNrawZnAditr"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Embedding(w2v.vocab_size, w2v.emdedding_size,\n",
        "              weights=[embedding_weights],\n",
        "              trainable=False,\n",
        "              mask_zero=True),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(w2v.emdedding_size),\n",
        "    Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWc2N0O-ditw"
      },
      "source": [
        "Podemos inspeccionar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmk-mnMzditw",
        "outputId": "fe46704e-bf70-40e9-9a3c-2c06fa3715d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 100)         265605800 \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, None, 100)         0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7)                 707       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 265686907 (1013.52 MB)\n",
            "Trainable params: 81107 (316.82 KB)\n",
            "Non-trainable params: 265605800 (1013.21 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de continuar, separemos nuestro conjunto de datos en entrenamiento y testing y codifiquemos la variable a predecir:"
      ],
      "metadata": {
        "id": "G_WubiNOiiyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "tweets_sector = encoder.fit_transform(tweets['SECTOR'])"
      ],
      "metadata": {
        "id": "mcZVsSjhiOBu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RKrEadrGditO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets_text, tweets_sector,\n",
        "                                                    test_size=0.33,\n",
        "                                                    stratify=tweets_sector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "webM_rFCdiuD"
      },
      "source": [
        "Entrenamos nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJYPslBwdiuE",
        "outputId": "24aef330-8427-4dd3-9c97-eb301c753183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "79/79 [==============================] - 11s 107ms/step - loss: 1.3717 - accuracy: 0.5256\n",
            "Epoch 2/50\n",
            "79/79 [==============================] - 9s 109ms/step - loss: 0.7200 - accuracy: 0.7949\n",
            "Epoch 3/50\n",
            "79/79 [==============================] - 9s 108ms/step - loss: 0.5580 - accuracy: 0.8429\n",
            "Epoch 4/50\n",
            "79/79 [==============================] - 9s 116ms/step - loss: 0.4785 - accuracy: 0.8643\n",
            "Epoch 5/50\n",
            "79/79 [==============================] - 9s 114ms/step - loss: 0.4193 - accuracy: 0.8782\n",
            "Epoch 6/50\n",
            "79/79 [==============================] - 9s 113ms/step - loss: 0.3813 - accuracy: 0.8862\n",
            "Epoch 7/50\n",
            "79/79 [==============================] - 9s 117ms/step - loss: 0.3569 - accuracy: 0.8921\n",
            "Epoch 8/50\n",
            "79/79 [==============================] - 9s 111ms/step - loss: 0.3190 - accuracy: 0.8981\n",
            "Epoch 9/50\n",
            "79/79 [==============================] - 11s 143ms/step - loss: 0.3297 - accuracy: 0.8933\n",
            "Epoch 10/50\n",
            "79/79 [==============================] - 9s 109ms/step - loss: 0.2837 - accuracy: 0.9060\n",
            "Epoch 11/50\n",
            "79/79 [==============================] - 9s 110ms/step - loss: 0.2885 - accuracy: 0.9100\n",
            "Epoch 12/50\n",
            "79/79 [==============================] - 7s 92ms/step - loss: 0.2583 - accuracy: 0.9139\n",
            "Epoch 13/50\n",
            "79/79 [==============================] - 8s 105ms/step - loss: 0.2394 - accuracy: 0.9223\n",
            "Epoch 14/50\n",
            "79/79 [==============================] - 8s 98ms/step - loss: 0.2356 - accuracy: 0.9246\n",
            "Epoch 15/50\n",
            "79/79 [==============================] - 8s 95ms/step - loss: 0.2308 - accuracy: 0.9290\n",
            "Epoch 16/50\n",
            "79/79 [==============================] - 8s 104ms/step - loss: 0.2131 - accuracy: 0.9302\n",
            "Epoch 17/50\n",
            "79/79 [==============================] - 8s 96ms/step - loss: 0.2006 - accuracy: 0.9353\n",
            "Epoch 18/50\n",
            "79/79 [==============================] - 8s 105ms/step - loss: 0.1981 - accuracy: 0.9377\n",
            "Epoch 19/50\n",
            "79/79 [==============================] - 8s 102ms/step - loss: 0.1792 - accuracy: 0.9425\n",
            "Epoch 20/50\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 0.1774 - accuracy: 0.9425\n",
            "Epoch 21/50\n",
            "79/79 [==============================] - 8s 107ms/step - loss: 0.1809 - accuracy: 0.9437\n",
            "Epoch 22/50\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.1769 - accuracy: 0.9409\n",
            "Epoch 23/50\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 0.1558 - accuracy: 0.9516\n",
            "Epoch 24/50\n",
            "79/79 [==============================] - 9s 117ms/step - loss: 0.1581 - accuracy: 0.9496\n",
            "Epoch 25/50\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.1677 - accuracy: 0.9461\n",
            "Epoch 26/50\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.1335 - accuracy: 0.9599\n",
            "Epoch 27/50\n",
            "79/79 [==============================] - 8s 104ms/step - loss: 0.1409 - accuracy: 0.9564\n",
            "Epoch 28/50\n",
            "79/79 [==============================] - 7s 90ms/step - loss: 0.1294 - accuracy: 0.9607\n",
            "Epoch 29/50\n",
            "79/79 [==============================] - 8s 108ms/step - loss: 0.1164 - accuracy: 0.9635\n",
            "Epoch 30/50\n",
            "79/79 [==============================] - 8s 96ms/step - loss: 0.1178 - accuracy: 0.9635\n",
            "Epoch 31/50\n",
            "79/79 [==============================] - 8s 101ms/step - loss: 0.1282 - accuracy: 0.9580\n",
            "Epoch 32/50\n",
            "79/79 [==============================] - 8s 106ms/step - loss: 0.1132 - accuracy: 0.9647\n",
            "Epoch 33/50\n",
            "79/79 [==============================] - 7s 90ms/step - loss: 0.1011 - accuracy: 0.9722\n",
            "Epoch 34/50\n",
            "79/79 [==============================] - 8s 107ms/step - loss: 0.0918 - accuracy: 0.9738\n",
            "Epoch 35/50\n",
            "79/79 [==============================] - 7s 91ms/step - loss: 0.1047 - accuracy: 0.9687\n",
            "Epoch 36/50\n",
            "79/79 [==============================] - 8s 107ms/step - loss: 0.0851 - accuracy: 0.9730\n",
            "Epoch 37/50\n",
            "79/79 [==============================] - 8s 107ms/step - loss: 0.1100 - accuracy: 0.9651\n",
            "Epoch 38/50\n",
            "79/79 [==============================] - 7s 92ms/step - loss: 0.0942 - accuracy: 0.9714\n",
            "Epoch 39/50\n",
            "79/79 [==============================] - 8s 105ms/step - loss: 0.0833 - accuracy: 0.9762\n",
            "Epoch 40/50\n",
            "79/79 [==============================] - 7s 89ms/step - loss: 0.0795 - accuracy: 0.9726\n",
            "Epoch 41/50\n",
            "79/79 [==============================] - 9s 110ms/step - loss: 0.0835 - accuracy: 0.9730\n",
            "Epoch 42/50\n",
            "79/79 [==============================] - 8s 105ms/step - loss: 0.0732 - accuracy: 0.9758\n",
            "Epoch 43/50\n",
            "79/79 [==============================] - 7s 92ms/step - loss: 0.0795 - accuracy: 0.9754\n",
            "Epoch 44/50\n",
            "79/79 [==============================] - 8s 106ms/step - loss: 0.0744 - accuracy: 0.9778\n",
            "Epoch 45/50\n",
            "79/79 [==============================] - 7s 90ms/step - loss: 0.0715 - accuracy: 0.9798\n",
            "Epoch 46/50\n",
            "79/79 [==============================] - 8s 105ms/step - loss: 0.0574 - accuracy: 0.9841\n",
            "Epoch 47/50\n",
            "79/79 [==============================] - 8s 106ms/step - loss: 0.0592 - accuracy: 0.9833\n",
            "Epoch 48/50\n",
            "79/79 [==============================] - 7s 90ms/step - loss: 0.0609 - accuracy: 0.9794\n",
            "Epoch 49/50\n",
            "79/79 [==============================] - 8s 103ms/step - loss: 0.0627 - accuracy: 0.9818\n",
            "Epoch 50/50\n",
            "79/79 [==============================] - 7s 87ms/step - loss: 0.0502 - accuracy: 0.9841\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3DtoSNTdiuJ"
      },
      "source": [
        "## Evalución de los resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFmZCv5KdiuK"
      },
      "source": [
        "Probamos su performance utilizando el test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqM4G2BVdiuL",
        "outputId": "c73fb62d-b8dc-44b0-c958-c44330b57460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39/39 [==============================] - 2s 44ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "afB-SEKdlKFV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el reporte:"
      ],
      "metadata": {
        "id": "FXuC61Dplufm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSIlnu1zdiuN",
        "outputId": "1c736b25-eeb8-474b-f72a-be2bf1326044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.99      0.90      0.94       110\n",
            "  AUTOMOCION       0.84      0.94      0.89       148\n",
            "       BANCA       0.95      0.90      0.93       198\n",
            "     BEBIDAS       0.91      0.91      0.91       223\n",
            "    DEPORTES       0.95      0.95      0.95       216\n",
            "      RETAIL       0.89      0.92      0.91       268\n",
            "       TELCO       0.94      0.86      0.90        79\n",
            "\n",
            "    accuracy                           0.92      1242\n",
            "   macro avg       0.93      0.91      0.92      1242\n",
            "weighted avg       0.92      0.92      0.92      1242\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions, target_names=encoder.classes_))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Word2Vec - Secuencias.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}