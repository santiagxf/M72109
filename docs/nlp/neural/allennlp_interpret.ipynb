{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKENeWEEwKIb"
   },
   "source": [
    "Salency maps para NLP utilizando AllenNLP\n",
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JctlpI10jJ15"
   },
   "source": [
    "Introducción\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT_t9OxYwKIc"
   },
   "source": [
    "AllenNLP es un framework general de aprendizaje profundo para NLP, establecido por el mundialmente famoso Allen Institute for AI Lab. Contiene modelos de referencia de última generación que se ejecutan sobre el `PyTorch`. AllenNLP es una librería que ademas busca implementar abstracciones que permitan el rápido desarrollo de modelos y reutilización de componentes al despegarse de detalles de implementación de cada modelo.\n",
    "\n",
    "En este ejemplo, veremos como utilizar esta librería para generar salency maps utilizando los gradientes de las prediciones. Esto nos permita interpretar las predicciones de nuestros modelos basados en `transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dcyc_TQ6dis7"
   },
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJFcNrbmjJ17"
   },
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgK8b6e_jJ17",
    "outputId": "1f9ecd9f-3197-4f6e-e564-b8b040b37321"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
    "\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/TextDataset.py \\\n",
    "    --quiet --no-clobber --directory-prefix ./Utils/\n",
    "    \n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/allennlp_interpret.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r allennlp_interpret.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ejecuta en Google Colab, adicionalmente deberá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U google-cloud-storage==1.40.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargaremos un modelo previamente entrenando el el problema de clasificación de Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://santiagxf.blob.core.windows.net/public/models/tweet_classification_bert.zip --no-clobber --quiet\n",
    "!unzip -qq tweet_classification_bert.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gBXNzwYwKIu"
   },
   "source": [
    "Cargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I8vqJD9JwKIv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando un modelo entreando con Transformers en AllenNLP\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXgQvCX2wKJi"
   },
   "source": [
    "`allennlp` es un framework compatible con la libraría `transformers` lo cual resulta atractivo a la hora de utilizar modelos que son entrenados en una para luego llevarlo a la otra. Veamos entonces como podemos hacer para cargar el modelo que tenemos previamente entrenado para la clasificación de tweets utilizando una arquitectura `BERT` dentro de este framework. En particular, nuestro modelo se persistió en el directorio \"tweet_classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tweet_classification_bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando un objeto Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos algunos elementos que necesitaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "3ZQPaNL2s23z"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.data import DatasetReader, Instance, Batch\n",
    "from allennlp.data.fields import Field, LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.data.vocabulary import PreTrainedTokenizer, Vocabulary\n",
    "from allennlp.models import BasicClassifier, Model\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer\n",
    "from allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos todos los elementos que son necesarios para utilizar esta libreria. Todos ellos son generados a partir del modelo que persistimos en `transformers`. La utilidad de cada uno de estos módulos esta fuera del alcance de este curso pero recomendamos revisar la documentación de AllenNLP para más información sobre cual es su rol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "wNQ6adFH0w9-"
   },
   "outputs": [],
   "source": [
    "transformer_vocab = Vocabulary.from_pretrained_transformer(model_name)\n",
    "transformer_tokenizer = PretrainedTransformerTokenizer(model_name)\n",
    "transformer_encoder = BertPooler(model_name)\n",
    "\n",
    "token_indexer = PretrainedTransformerIndexer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params(\n",
    "    {\n",
    "     \"token_embedders\": {\n",
    "        \"tokens\": {\n",
    "          \"type\": \"pretrained_transformer\",\n",
    "          \"model_name\": model_name,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    ")\n",
    "\n",
    "token_embedder = BasicTextFieldEmbedder.from_params(vocab=transformer_vocab, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo a partir de todos los componentes que cargamos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "YJS_5MUHT6ae"
   },
   "outputs": [],
   "source": [
    "model = BasicClassifier(vocab=transformer_vocab, text_field_embedder=token_embedder, seq2vec_encoder=transformer_encoder, dropout=0.1, num_labels=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuremos el modelo para trabajar en modo `inferencia`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AllenNLP utiliza un objeto llamado `DatasetReader` que le permite crear `Instance`'s de datos que son suministradas al modelo. Esta abstracción permite realizar cualquier preprocesamiento que es necesario antes de enviar los datos al modelo. Debemos generar nuestro propia implementación para el caso de clasificación utilizando un modelo basado en *transformers*. La siguiente clase realiza esto: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "soe8I3wuxY4o"
   },
   "outputs": [],
   "source": [
    "from allennlp.data import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "gQ2EaLOTs4o2"
   },
   "outputs": [],
   "source": [
    "class ClassificationTransformerReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Tokenizer,\n",
    "        token_indexer: TokenIndexer,\n",
    "        max_tokens: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers: Dict[str, TokenIndexer] = { \"tokens\": token_indexer }\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if self.max_tokens:\n",
    "            tokens = tokens[: self.max_tokens]\n",
    "        \n",
    "        inputs = TextField(tokens, self.token_indexers)\n",
    "        fields: Dict[str, Field] = { \"tokens\": inputs }\n",
    "            \n",
    "        if label:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "            \n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el `DatasetReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "USOdKLOLxcsL"
   },
   "outputs": [],
   "source": [
    "dataset_reader = ClassificationTransformerReader(tokenizer=transformer_tokenizer, token_indexer=token_indexer, max_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provemos que nuestro modelo funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unamos todas las piezas que generamos hasta el momento probandolo sobre una instancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "11t8d8z3BWjH"
   },
   "outputs": [],
   "source": [
    "sample_text = \"No supimos como salir del supermercado luego de tantas vueltas\"\n",
    "instance = dataset_reader.text_to_instance(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "R2Id7l50BbWL"
   },
   "outputs": [],
   "source": [
    "from allennlp.nn import util\n",
    "\n",
    "dataset = Batch([instance])\n",
    "dataset.index_instances(transformer_vocab)\n",
    "model_input = util.move_to_device(dataset.as_tensor_dict(), model._get_prediction_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.make_output_human_readable(model(**model_input))\n",
    "print(outputs['probs'].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que en el conjunto de datos de entrenamiento, las etiquetas se distribuyen como sigue:\n",
    "\n",
    "```\n",
    "{\n",
    "    'ALIMENTACION': 0,\n",
    "    'AUTOMOCION': 1,\n",
    "    'BANCA': 2,\n",
    "    'BEBIDAS': 3,\n",
    "    'DEPORTES': 4,\n",
    "    'RETAIL': 5,\n",
    "    'TELCO': 6\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretando nuestras predicciones\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos nuestro modelo correctamente cargado, veamos como podemos interpretar una predicción computando el salency map a partir de los gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "NTX6HgF6sXrs"
   },
   "outputs": [],
   "source": [
    "from allennlp.interpret.saliency_interpreters import SimpleGradient\n",
    "from allennlp.predictors import Predictor, TextClassifierPredictor\n",
    "\n",
    "predictor = TextClassifierPredictor(model, dataset_reader)\n",
    "interpreter = SimpleGradient(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busquemos un tweet para interpretar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: BBVA remolca el crecimiento de la banca española pese a los obstáculos https://t.co/wF9tOqxB5D \n",
      "Label: BANCA\n"
     ]
    }
   ],
   "source": [
    "sample_text_idx = 2071\n",
    "sample_text = tweets['TEXTO'][sample_text_idx]\n",
    "sample_label = tweets['SECTOR'][sample_text_idx]\n",
    "\n",
    "print(\"Texto:\", sample_text, \"\\Sector:\", sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos los gradientes para cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "j_i_OjI_edIS"
   },
   "outputs": [],
   "source": [
    "inputs = {\"sentence\": sample_text }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "7_00Ya8ZelBk",
    "outputId": "1ec1b8fe-741a-44da-ab1c-6d35c2f6e313"
   },
   "outputs": [],
   "source": [
    "interpretation = interpreter.saliency_interpret_from_json(inputs)\n",
    "grads = interpretation['instance_1']['grad_input_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos graficar los resultados utilizando un mapa de calor marcando con colores más intensos aquellos tokens que tienen mayor impacto en las predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background-color:rgba(255,0,0,0.023973679291077122)'>[CLS] </span><span style='background-color:rgba(255,0,0,0.04897258439852681)'>@ </span><span style='background-color:rgba(255,0,0,0.003461454720505409)'>afectados </span><span style='background-color:rgba(255,0,0,0.02091576096284071)'>##bb </span><span style='background-color:rgba(255,0,0,0.018777627214177528)'>##s </span><span style='background-color:rgba(255,0,0,0.031570827555892855)'>gracias </span><span style='background-color:rgba(255,0,0,0.0016718006309883002)'>por </span><span style='background-color:rgba(255,0,0,0.0034003259600348073)'>vuestro </span><span style='background-color:rgba(255,0,0,0.06144485286567216)'>apoyo </span><span style='background-color:rgba(255,0,0,0.005402004359105598)'>. </span><span style='background-color:rgba(255,0,0,0.01802461645015915)'>@ </span><span style='background-color:rgba(255,0,0,0.061875498507208804)'>movi </span><span style='background-color:rgba(255,0,0,0.024941960716525262)'>##sta </span><span style='background-color:rgba(255,0,0,0.13591464547869064)'>##r </span><span style='background-color:rgba(255,0,0,0.11568165706100288)'>@ </span><span style='background-color:rgba(255,0,0,0.026843567739396487)'>telefónica </span><span style='background-color:rgba(255,0,0,0.04155317701714767)'>me </span><span style='background-color:rgba(255,0,0,0.020817662853791805)'>han </span><span style='background-color:rgba(255,0,0,0.005203374227524732)'>proporcionado </span><span style='background-color:rgba(255,0,0,0.0007301853839047058)'>un </span><span style='background-color:rgba(255,0,0,0.015294630686846507)'>sam </span><span style='background-color:rgba(255,0,0,0.026412237223066567)'>##su </span><span style='background-color:rgba(255,0,0,0.06879373175012778)'>##ng </span><span style='background-color:rgba(255,0,0,0.060360859349322865)'>galax </span><span style='background-color:rgba(255,0,0,0.009029374533876937)'>##y </span><span style='background-color:rgba(255,0,0,0.024829015338365158)'>s </span><span style='background-color:rgba(255,0,0,0.011436398744010385)'>##5 </span><span style='background-color:rgba(255,0,0,0.011620545021001981)'>, </span><span style='background-color:rgba(255,0,0,0.0033746346510469495)'>que </span><span style='background-color:rgba(255,0,0,0.0024233566030202997)'>ya </span><span style='background-color:rgba(255,0,0,0.013814822628132545)'>tengo </span><span style='background-color:rgba(255,0,0,0.0029324966630722986)'>en </span><span style='background-color:rgba(255,0,0,0.013570686308404035)'>mi </span><span style='background-color:rgba(255,0,0,0.005596539415660109)'>poder </span><span style='background-color:rgba(255,0,0,0.01920838170998595)'>. </span><span style='background-color:rgba(255,0,0,0.040125054327352805)'>[SEP] </span>"
      ],
      "metadata": {
        "id": "Nid9rdAGGK1d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JctlpI10jJ15"
      },
      "source": [
        "Introducción\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT_t9OxYwKIc"
      },
      "source": [
        "AllenNLP es un framework general de aprendizaje profundo para NLP, establecido por el mundialmente famoso Allen Institute for AI Lab. Contiene modelos de referencia de última generación que se ejecutan sobre el `PyTorch`. AllenNLP es una librería que ademas busca implementar abstracciones que permitan el rápido desarrollo de modelos y reutilización de componentes al despegarse de detalles de implementación de cada modelo.\n",
        "\n",
        "En este ejemplo, veremos como utilizar esta librería para generar salency maps utilizando los gradientes de las prediciones. Esto nos permita interpretar las predicciones de nuestros modelos basados en `transformers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcyc_TQ6dis7"
      },
      "source": [
        "### Para ejecutar este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFcNrbmjJ17"
      },
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgK8b6e_jJ17"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
        "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/TextDataset.py \\\n",
        "    --quiet --no-clobber --directory-prefix ./Utils/\n",
        "    \n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/allennlp_interpret.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r allennlp_interpret.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yzp9NUWGJ35"
      },
      "source": [
        "Si ejecuta en Google Colab, adicionalmente deberá:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_d1qP02GJ37"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-cloud-storage==1.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1uJHmZFGJ38"
      },
      "source": [
        "Descargaremos un modelo previamente entrenando el el problema de clasificación de Tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm8O-FF5GJ39"
      },
      "outputs": [],
      "source": [
        "!wget https://santiagxf.blob.core.windows.net/public/models/tweet_classification_bert.zip --no-clobber --quiet\n",
        "!unzip tweet_classification_bert.zip -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gBXNzwYwKIu"
      },
      "source": [
        "Cargamos el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8vqJD9JwKIv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYj9a_U9GJ4E"
      },
      "source": [
        "Cargando un modelo entreando con Transformers en AllenNLP\n",
        "---------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXgQvCX2wKJi"
      },
      "source": [
        "`allennlp` es un framework compatible con la libraría `transformers` lo cual resulta atractivo a la hora de utilizar modelos que son entrenados en una para luego llevarlo a la otra. Veamos entonces como podemos hacer para cargar el modelo que tenemos previamente entrenado para la clasificación de tweets utilizando una arquitectura `BERT` dentro de este framework. En particular, nuestro modelo se persistió en el directorio \"tweet_classification\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPdxLD3cGJ4F"
      },
      "outputs": [],
      "source": [
        "model_name = \"tweet_classification\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tUwXt_-GJ4F"
      },
      "source": [
        "### Creando un objeto Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUELM6d5GJ4G"
      },
      "source": [
        "Importamos algunos elementos que necesitaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZQPaNL2s23z"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Iterable, List\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.data import DatasetReader, Instance, Batch\n",
        "from allennlp.data.fields import Field, LabelField, TextField\n",
        "from allennlp.data.token_indexers import TokenIndexer\n",
        "from allennlp.data.tokenizers import Tokenizer\n",
        "from allennlp.data.vocabulary import PreTrainedTokenizer, Vocabulary\n",
        "from allennlp.models import BasicClassifier, Model\n",
        "from allennlp.modules.token_embedders import PretrainedTransformerEmbedder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer\n",
        "from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer\n",
        "from allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3NpbJ44GJ4I"
      },
      "source": [
        "Cargaremos todos los elementos que son necesarios para utilizar esta libreria. Todos ellos son generados a partir del modelo que persistimos en `transformers`. La utilidad de cada uno de estos módulos esta fuera del alcance de este curso pero recomendamos revisar la documentación de AllenNLP para más información sobre cual es su rol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQ6adFH0w9-"
      },
      "outputs": [],
      "source": [
        "transformer_vocab = Vocabulary.from_pretrained_transformer(model_name)\n",
        "transformer_tokenizer = PretrainedTransformerTokenizer(model_name)\n",
        "transformer_encoder = BertPooler(model_name)\n",
        "\n",
        "token_indexer = PretrainedTransformerIndexer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOp2w6qPGJ4J"
      },
      "outputs": [],
      "source": [
        "params = Params(\n",
        "    {\n",
        "     \"token_embedders\": {\n",
        "        \"tokens\": {\n",
        "          \"type\": \"pretrained_transformer\",\n",
        "          \"model_name\": model_name,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        ")\n",
        "\n",
        "token_embedder = BasicTextFieldEmbedder.from_params(vocab=transformer_vocab, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOVpVEbOGJ4J"
      },
      "source": [
        "Creadmos el modelo a partir de todos los componentes que cargamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJS_5MUHT6ae"
      },
      "outputs": [],
      "source": [
        "model = BasicClassifier(vocab=transformer_vocab, text_field_embedder=token_embedder, seq2vec_encoder=transformer_encoder, dropout=0.1, num_labels=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCDosFR6GJ4K"
      },
      "source": [
        "### Creamos un DatasetReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqshVuUbGJ4K"
      },
      "source": [
        "AllenNLP utiliza un objeto llamado `DatasetReader` que le permite crear `Instance`'s de datos que son suministradas al modelo. Esta abstracción permite realizar cualquier preprocesamiento que es necesario antes de enviar los datos al modelo. Debemos generar nuestro propia implementación para el caso de clasificación utilizando un modelo basado en *transformers*. La siguiente clase realiza esto: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soe8I3wuxY4o"
      },
      "outputs": [],
      "source": [
        "from allennlp.data import DatasetReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ2EaLOTs4o2"
      },
      "outputs": [],
      "source": [
        "class ClassificationTransformerReader(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer,\n",
        "        token_indexer: TokenIndexer,\n",
        "        max_tokens: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_indexers: Dict[str, TokenIndexer] = { \"tokens\": token_indexer }\n",
        "        self.max_tokens = max_tokens\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if self.max_tokens:\n",
        "            tokens = tokens[: self.max_tokens]\n",
        "        \n",
        "        inputs = TextField(tokens, self.token_indexers)\n",
        "        fields: Dict[str, Field] = { \"tokens\": inputs }\n",
        "            \n",
        "        if label:\n",
        "            fields[\"label\"] = LabelField(label)\n",
        "            \n",
        "        return Instance(fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR4MGYsnGJ4L"
      },
      "source": [
        "Instanciamos el `DatasetReader`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USOdKLOLxcsL"
      },
      "outputs": [],
      "source": [
        "dataset_reader = ClassificationTransformerReader(tokenizer=transformer_tokenizer, token_indexer=token_indexer, max_tokens=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmBspUFoGJ4M"
      },
      "source": [
        "### Provemos que nuestro modelo funciona"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHzVe4v4GJ4M"
      },
      "source": [
        "Unamos todas las piezas que generamos hasta el momento probandolo sobre una instancia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11t8d8z3BWjH"
      },
      "outputs": [],
      "source": [
        "sample_text = \"No supimos como salir del supermercado luego de tantas vueltas\"\n",
        "instance = dataset_reader.text_to_instance(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2Id7l50BbWL"
      },
      "outputs": [],
      "source": [
        "from allennlp.nn import util\n",
        "\n",
        "dataset = Batch([instance])\n",
        "dataset.index_instances(transformer_vocab)\n",
        "model_input = util.move_to_device(dataset.as_tensor_dict(), model._get_prediction_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F3oza-CGJ4M",
        "outputId": "7eb75ede-f7a1-48d9-b27a-08c9239a1d46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4)"
            ]
          },
          "execution_count": 319,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model.make_output_human_readable(model(**model_input))\n",
        "print(outputs['probs'].argmax())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWW0v7EgGJ4O"
      },
      "source": [
        "Recordemos que en el conjunto de datos de entrenamiento, las etiquetas se distribuyen como sigue:\n",
        "\n",
        "```\n",
        "{\n",
        "    'ALIMENTACION': 0,\n",
        "    'AUTOMOCION': 1,\n",
        "    'BANCA': 2,\n",
        "    'BEBIDAS': 3,\n",
        "    'DEPORTES': 4,\n",
        "    'RETAIL': 5,\n",
        "    'TELCO': 6\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksqLIp9kGJ4O"
      },
      "source": [
        "Interpretando nuestras predicciones\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mAnhCV_GJ4P"
      },
      "source": [
        "Una vez que tenemos nuestro modelo correctamente cargado, veamos como podemos interpretar una predicción computando el salency map a partir de los gradientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTX6HgF6sXrs"
      },
      "outputs": [],
      "source": [
        "from allennlp.interpret.saliency_interpreters import SimpleGradient\n",
        "from allennlp.predictors import Predictor, TextClassifierPredictor\n",
        "\n",
        "predictor = TextClassifierPredictor(model, dataset_reader)\n",
        "interpreter = SimpleGradient(predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHREOchPGJ4P"
      },
      "source": [
        "Busquemos un tweet para interpretar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl_EM75HGJ4Q",
        "outputId": "0ab84cb4-ee9a-4fb2-f907-66901f0c6aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples: BBVA remolca el crecimiento de la banca española pese a los obstáculos https://t.co/wF9tOqxB5D \n",
            "Label: BANCA\n"
          ]
        }
      ],
      "source": [
        "sample_text_idx = 2071\n",
        "sample_text = tweets['TEXTO'][sample_text_idx]\n",
        "sample_label = tweets['SECTOR'][sample_text_idx]\n",
        "\n",
        "print(\"Texto:\", sample_text, \"\\Sector:\", sample_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmQcgHfFGJ4Q"
      },
      "source": [
        "Calculemos los gradientes para cada token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_i_OjI_edIS"
      },
      "outputs": [],
      "source": [
        "inputs = {\"sentence\": sample_text }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_00Ya8ZelBk"
      },
      "outputs": [],
      "source": [
        "interpretation = interpreter.saliency_interpret_from_json(inputs)\n",
        "grads = interpretation['instance_1']['grad_input_1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvqGAen_GJ4Q"
      },
      "source": [
        "Podemos graficar los resultados utilizando un mapa de calor marcando con colores más intensos aquellos tokens que tienen mayor impacto en las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsyP6sIQGJ4R",
        "outputId": "0f919200-6a04-49d4-f887-4204167eac3b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span style='background-color:rgba(255,0,0,0.023973679291077122)'>[CLS] </span><span style='background-color:rgba(255,0,0,0.04897258439852681)'>@ </span><span style='background-color:rgba(255,0,0,0.003461454720505409)'>afectados </span><span style='background-color:rgba(255,0,0,0.02091576096284071)'>##bb </span><span style='background-color:rgba(255,0,0,0.018777627214177528)'>##s </span><span style='background-color:rgba(255,0,0,0.031570827555892855)'>gracias </span><span style='background-color:rgba(255,0,0,0.0016718006309883002)'>por </span><span style='background-color:rgba(255,0,0,0.0034003259600348073)'>vuestro </span><span style='background-color:rgba(255,0,0,0.06144485286567216)'>apoyo </span><span style='background-color:rgba(255,0,0,0.005402004359105598)'>. </span><span style='background-color:rgba(255,0,0,0.01802461645015915)'>@ </span><span style='background-color:rgba(255,0,0,0.061875498507208804)'>movi </span><span style='background-color:rgba(255,0,0,0.024941960716525262)'>##sta </span><span style='background-color:rgba(255,0,0,0.13591464547869064)'>##r </span><span style='background-color:rgba(255,0,0,0.11568165706100288)'>@ </span><span style='background-color:rgba(255,0,0,0.026843567739396487)'>telefónica </span><span style='background-color:rgba(255,0,0,0.04155317701714767)'>me </span><span style='background-color:rgba(255,0,0,0.020817662853791805)'>han </span><span style='background-color:rgba(255,0,0,0.005203374227524732)'>proporcionado </span><span style='background-color:rgba(255,0,0,0.0007301853839047058)'>un </span><span style='background-color:rgba(255,0,0,0.015294630686846507)'>sam </span><span style='background-color:rgba(255,0,0,0.026412237223066567)'>##su </span><span style='background-color:rgba(255,0,0,0.06879373175012778)'>##ng </span><span style='background-color:rgba(255,0,0,0.060360859349322865)'>galax </span><span style='background-color:rgba(255,0,0,0.009029374533876937)'>##y </span><span style='background-color:rgba(255,0,0,0.024829015338365158)'>s </span><span style='background-color:rgba(255,0,0,0.011436398744010385)'>##5 </span><span style='background-color:rgba(255,0,0,0.011620545021001981)'>, </span><span style='background-color:rgba(255,0,0,0.0033746346510469495)'>que </span><span style='background-color:rgba(255,0,0,0.0024233566030202997)'>ya </span><span style='background-color:rgba(255,0,0,0.013814822628132545)'>tengo </span><span style='background-color:rgba(255,0,0,0.0029324966630722986)'>en </span><span style='background-color:rgba(255,0,0,0.013570686308404035)'>mi </span><span style='background-color:rgba(255,0,0,0.005596539415660109)'>poder </span><span style='background-color:rgba(255,0,0,0.01920838170998595)'>. </span><span style='background-color:rgba(255,0,0,0.040125054327352805)'>[SEP] </span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "from IPython.display import HTML\n",
        "\n",
        "html = \"\"\n",
        "for idx, token in enumerate(tokenizer.tokenize(inputs['sentence'])):\n",
        "    html += \"<span style='background-color:rgba(255,0,0,{})'>{} </span>\".format(grads[idx],token)\n",
        "    \n",
        "HTML(html)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "allennlp_interpret.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "nlp-py38"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - PyTorch",
      "language": "python",
      "name": "azureml_py38_pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}