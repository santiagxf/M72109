{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKENeWEEwKIb"
      },
      "source": [
        "Salency maps para NLP utilizando AllenNLP\n",
        "========================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JctlpI10jJ15"
      },
      "source": [
        "Introducción\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT_t9OxYwKIc"
      },
      "source": [
        "AllenNLP es un framework general de aprendizaje profundo para NLP, establecido por el mundialmente famoso Allen Institute for AI Lab. Contiene modelos de referencia de última generación que se ejecutan sobre el `PyTorch`. AllenNLP es una librería que ademas busca implementar abstracciones que permitan el rápido desarrollo de modelos y reutilización de componentes al despegarse de detalles de implementación de cada modelo.\n",
        "\n",
        "En este ejemplo, veremos como utilizar esta librería para generar salency maps utilizando los gradientes de las prediciones. Esto nos permita interpretar las predicciones de nuestros modelos basados en `transformers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcyc_TQ6dis7"
      },
      "source": [
        "### Para ejecutar este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFcNrbmjJ17"
      },
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgK8b6e_jJ17"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
        "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/TextDataset.py \\\n",
        "    --quiet --no-clobber --directory-prefix ./Utils/\n",
        "    \n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/allennlp_interpret.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r allennlp_interpret.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YjjRwLK4leb"
      },
      "source": [
        "Si ejecuta en Google Colab, adicionalmente deberá:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuxJI9Cd4lec"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-cloud-storage==1.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1nl84gT4led"
      },
      "source": [
        "Descargaremos un modelo previamente entrenando el el problema de clasificación de Tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3yqczZze4lee"
      },
      "outputs": [],
      "source": [
        "!wget https://santiagxf.blob.core.windows.net/public/models/tweet_classification_bert.zip --no-clobber --quiet\n",
        "!unzip -qq tweet_classification_bert.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gBXNzwYwKIu"
      },
      "source": [
        "Cargamos el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I8vqJD9JwKIv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0uIrAVo4leg"
      },
      "source": [
        "Cargando un modelo entreando con Transformers en AllenNLP\n",
        "---------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXgQvCX2wKJi"
      },
      "source": [
        "`allennlp` es un framework compatible con la libraría `transformers` lo cual resulta atractivo a la hora de utilizar modelos que son entrenados en una para luego llevarlo a la otra. Veamos entonces como podemos hacer para cargar el modelo que tenemos previamente entrenado para la clasificación de tweets utilizando una arquitectura `BERT` dentro de este framework. En particular, nuestro modelo se persistió en el directorio \"tweet_classification\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dxqTaKSc4leh"
      },
      "outputs": [],
      "source": [
        "model_name = \"tweet_classification_bert\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyidNud34leh"
      },
      "source": [
        "### Creando un objeto Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnhc4x7y4leh"
      },
      "source": [
        "Importamos algunos elementos que necesitaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ZQPaNL2s23z"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Iterable, List\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.data import DatasetReader, Instance, Batch\n",
        "from allennlp.data.fields import Field, LabelField, TextField\n",
        "from allennlp.data.token_indexers import TokenIndexer\n",
        "from allennlp.data.tokenizers import Tokenizer\n",
        "from allennlp.data.vocabulary import PreTrainedTokenizer, Vocabulary\n",
        "from allennlp.models import BasicClassifier, Model\n",
        "from allennlp.modules.token_embedders import PretrainedTransformerEmbedder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer\n",
        "from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer\n",
        "from allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x-D9aoo4lei"
      },
      "source": [
        "Cargaremos todos los elementos que son necesarios para utilizar esta libreria. Todos ellos son generados a partir del modelo que persistimos en `transformers`. La utilidad de cada uno de estos módulos esta fuera del alcance de este curso pero recomendamos revisar la documentación de AllenNLP para más información sobre cual es su rol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wNQ6adFH0w9-",
        "outputId": "44b6ecf0-482d-40c6-c25a-5474389fa608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at tweet_classification_bert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "transformer_vocab = Vocabulary.from_pretrained_transformer(model_name)\n",
        "transformer_tokenizer = PretrainedTransformerTokenizer(model_name)\n",
        "transformer_encoder = BertPooler(model_name)\n",
        "\n",
        "token_indexer = PretrainedTransformerIndexer(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ¿Notan el mensaje de advertencia? Lo correjiremos mas adelante."
      ],
      "metadata": {
        "id": "6itC7VYOAXix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zAKKNxvA4lei"
      },
      "outputs": [],
      "source": [
        "params = Params(\n",
        "    {\n",
        "     \"token_embedders\": {\n",
        "        \"tokens\": {\n",
        "          \"type\": \"pretrained_transformer\",\n",
        "          \"model_name\": model_name,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        ")\n",
        "\n",
        "token_embedder = BasicTextFieldEmbedder.from_params(vocab=transformer_vocab, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk1vYpLr4lej"
      },
      "source": [
        "Creamos el modelo a partir de todos los componentes que cargamos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YJS_5MUHT6ae"
      },
      "outputs": [],
      "source": [
        "model = BasicClassifier(vocab=transformer_vocab, text_field_embedder=token_embedder, seq2vec_encoder=transformer_encoder, dropout=0.1, num_labels=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, cargaremos los pesos del clasificador."
      ],
      "metadata": {
        "id": "Ip3mg1ohADNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "classifier = BertForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "VYyMjxrr7xxl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model._classification_layer.weight = classifier.classifier.weight\n",
        "model._classification_layer.bias = classifier.classifier.bias"
      ],
      "metadata": {
        "id": "GJ8adT549XJB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQGw48X24lej"
      },
      "source": [
        "Configuremos el modelo para trabajar en modo `inferencia`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.eval()"
      ],
      "metadata": {
        "id": "v0PotD72AL4k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFv8KS924lej"
      },
      "source": [
        "### Creamos un DatasetReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6OKKiho4lek"
      },
      "source": [
        "AllenNLP utiliza un objeto llamado `DatasetReader` que le permite crear `Instance`'s de datos que son suministradas al modelo. Esta abstracción permite realizar cualquier preprocesamiento que es necesario antes de enviar los datos al modelo. Debemos generar nuestro propia implementación para el caso de clasificación utilizando un modelo basado en *transformers*. La siguiente clase realiza esto: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "soe8I3wuxY4o"
      },
      "outputs": [],
      "source": [
        "from allennlp.data import DatasetReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gQ2EaLOTs4o2"
      },
      "outputs": [],
      "source": [
        "class ClassificationTransformerReader(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer,\n",
        "        token_indexer: TokenIndexer,\n",
        "        max_tokens: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_indexers: Dict[str, TokenIndexer] = { \"tokens\": token_indexer }\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if self.max_tokens:\n",
        "            tokens = tokens[: self.max_tokens]\n",
        "        \n",
        "        inputs = TextField(tokens, self.token_indexers)\n",
        "        fields: Dict[str, Field] = { \"tokens\": inputs }\n",
        "            \n",
        "        if label:\n",
        "            fields[\"label\"] = LabelField(label)\n",
        "            \n",
        "        return Instance(fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-hKKiLm4lek"
      },
      "source": [
        "Instanciamos el `DatasetReader`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "USOdKLOLxcsL"
      },
      "outputs": [],
      "source": [
        "dataset_reader = ClassificationTransformerReader(tokenizer=transformer_tokenizer, token_indexer=token_indexer, max_tokens=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh4OWx144lek"
      },
      "source": [
        "### Provemos que nuestro modelo funciona"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UISHOmKE4lek"
      },
      "source": [
        "Unamos todas las piezas que generamos hasta el momento probandolo sobre una instancia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "11t8d8z3BWjH"
      },
      "outputs": [],
      "source": [
        "sample_text = \"Movistar no tiene señal ni en su casa\"\n",
        "instance = dataset_reader.text_to_instance(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R2Id7l50BbWL"
      },
      "outputs": [],
      "source": [
        "from allennlp.nn import util\n",
        "\n",
        "dataset = Batch([instance])\n",
        "dataset.index_instances(transformer_vocab)\n",
        "model_input = util.move_to_device(dataset.as_tensor_dict(), model._get_prediction_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e81OlQO64lel",
        "outputId": "709f5c58-6f8f-4c7e-c1b4-a2b82c017da7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6)\n"
          ]
        }
      ],
      "source": [
        "outputs = model.make_output_human_readable(model(**model_input))\n",
        "print(outputs['probs'].argmax())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3TU4m2O4lem"
      },
      "source": [
        "Recordemos que en el conjunto de datos de entrenamiento, las etiquetas se distribuyen como sigue:\n",
        "\n",
        "```\n",
        "{\n",
        "    'ALIMENTACION': 0,\n",
        "    'AUTOMOCION': 1,\n",
        "    'BANCA': 2,\n",
        "    'BEBIDAS': 3,\n",
        "    'DEPORTES': 4,\n",
        "    'RETAIL': 5,\n",
        "    'TELCO': 6\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kB6qBzO4lem"
      },
      "source": [
        "Interpretando nuestras predicciones\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FavSqB7C4lem"
      },
      "source": [
        "Una vez que tenemos nuestro modelo correctamente cargado, veamos como podemos interpretar una predicción computando el salency map a partir de los gradientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NTX6HgF6sXrs"
      },
      "outputs": [],
      "source": [
        "from allennlp.interpret.saliency_interpreters import SimpleGradient\n",
        "from allennlp.predictors import Predictor, TextClassifierPredictor\n",
        "\n",
        "predictor = TextClassifierPredictor(model, dataset_reader)\n",
        "interpreter = SimpleGradient(predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5vXo70d4lem"
      },
      "source": [
        "Busquemos un tweet para interpretar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GMOYNNQB4lem",
        "outputId": "1dd2389d-e8b1-4a84-d2f8-4b2ccf08fc4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: @HyundaiPeru con Grupo Primax realiza este verano servicios de Inspección Digital Gratuita a vehículos Hyundai en e… https://t.co/TZ4XFziOd3 \\Sector: AUTOMOCION\n"
          ]
        }
      ],
      "source": [
        "sample_text_idx = 1522\n",
        "sample_text = tweets['TEXTO'][sample_text_idx]\n",
        "sample_label = tweets['SECTOR'][sample_text_idx]\n",
        "\n",
        "print(\"Texto:\", sample_text, \"\\Sector:\", sample_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCv3OzOq4len"
      },
      "source": [
        "Calculemos los gradientes para cada token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "j_i_OjI_edIS"
      },
      "outputs": [],
      "source": [
        "inputs = {\"sentence\": sample_text }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_00Ya8ZelBk",
        "outputId": "157ab610-f92e-45f0-a4aa-f240794a76f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        }
      ],
      "source": [
        "interpretation = interpreter.saliency_interpret_from_json(inputs)\n",
        "grads = interpretation['instance_1']['grad_input_1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gF--Q8v4len"
      },
      "source": [
        "Podemos graficar los resultados utilizando un mapa de calor marcando con colores más intensos aquellos tokens que tienen mayor impacto en las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TuQqxlWE4len",
        "outputId": "8e39a932-e59c-4e8f-f187-ff2ed1d87d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style='background-color:rgba(255,0,0,0.010647506430338951)'>[CLS] </span><span style='background-color:rgba(255,0,0,0.04666200321385881)'>@ </span><span style='background-color:rgba(255,0,0,0.013139357762542103)'>hyun </span><span style='background-color:rgba(255,0,0,0.04518228138353431)'>##da </span><span style='background-color:rgba(255,0,0,0.0076429721057869535)'>##ipe </span><span style='background-color:rgba(255,0,0,0.07119898618214722)'>##ru </span><span style='background-color:rgba(255,0,0,0.002673972142011466)'>con </span><span style='background-color:rgba(255,0,0,0.004281027037289998)'>grupo </span><span style='background-color:rgba(255,0,0,0.010333845621557975)'>prima </span><span style='background-color:rgba(255,0,0,0.012868667375205858)'>##x </span><span style='background-color:rgba(255,0,0,0.040070184225767)'>realiza </span><span style='background-color:rgba(255,0,0,0.013816976969074139)'>este </span><span style='background-color:rgba(255,0,0,0.03377504929210778)'>verano </span><span style='background-color:rgba(255,0,0,0.014016338147428802)'>servicios </span><span style='background-color:rgba(255,0,0,0.006983436397508779)'>de </span><span style='background-color:rgba(255,0,0,0.051477350086849595)'>inspección </span><span style='background-color:rgba(255,0,0,0.0174908517071645)'>digital </span><span style='background-color:rgba(255,0,0,0.002906396640953119)'>gratuita </span><span style='background-color:rgba(255,0,0,0.010806993455828516)'>a </span><span style='background-color:rgba(255,0,0,0.06032129357705654)'>vehículos </span><span style='background-color:rgba(255,0,0,0.0587005115715426)'>hyun </span><span style='background-color:rgba(255,0,0,0.062454782104120456)'>##da </span><span style='background-color:rgba(255,0,0,0.11225121651895766)'>##i </span><span style='background-color:rgba(255,0,0,0.006888041608746488)'>en </span><span style='background-color:rgba(255,0,0,0.008245922699679328)'>e </span><span style='background-color:rgba(255,0,0,0.005439644552480311)'>[UNK] </span><span style='background-color:rgba(255,0,0,0.003001777268622132)'>h </span><span style='background-color:rgba(255,0,0,0.003914400689565244)'>##tt </span><span style='background-color:rgba(255,0,0,0.016320490070523743)'>##ps </span><span style='background-color:rgba(255,0,0,0.035113836436315576)'>: </span><span style='background-color:rgba(255,0,0,0.003220181195903055)'>/ </span><span style='background-color:rgba(255,0,0,0.01264785017976728)'>/ </span><span style='background-color:rgba(255,0,0,0.01856679501689027)'>t </span><span style='background-color:rgba(255,0,0,0.006398906118027325)'>. </span><span style='background-color:rgba(255,0,0,0.003609261808856383)'>co </span><span style='background-color:rgba(255,0,0,0.0474240948668388)'>/ </span><span style='background-color:rgba(255,0,0,0.01821909965438636)'>t </span><span style='background-color:rgba(255,0,0,0.009109335450027239)'>##z </span><span style='background-color:rgba(255,0,0,0.01531333394460455)'>##4 </span><span style='background-color:rgba(255,0,0,0.012573497904165166)'>##x </span><span style='background-color:rgba(255,0,0,0.016406416970182903)'>##f </span><span style='background-color:rgba(255,0,0,0.005969628043556197)'>##zio </span><span style='background-color:rgba(255,0,0,0.006804289417288981)'>##d </span><span style='background-color:rgba(255,0,0,0.007881139036984586)'>##3 </span><span style='background-color:rgba(255,0,0,0.0272301684459344)'>[SEP] </span>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import math\n",
        "from IPython.display import HTML\n",
        "\n",
        "html = \"\"\n",
        "for idx, token in enumerate(transformer_tokenizer.tokenize(inputs['sentence'])):\n",
        "    html += \"<span style='background-color:rgba(255,0,0,{})'>{} </span>\".format(grads[idx],token)\n",
        "    \n",
        "HTML(html)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "allennlp_interpret.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "nlp-py38"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - PyTorch",
      "language": "python",
      "name": "azureml_py38_pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}