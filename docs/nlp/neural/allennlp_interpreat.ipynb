{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Fine-tunning.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "nlp-py38"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKENeWEEwKIb"
      },
      "source": [
        "\n",
        "BERT en un problema de clasificación\n",
        "===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JctlpI10jJ15"
      },
      "source": [
        "Introducción\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT_t9OxYwKIc"
      },
      "source": [
        "Los modelos basados en transformers nos pueden ayudar a resolver varios tipos de problemas. Desde problemas de clasificación y regresión hasta tareas más complejas como resumen de textos o generación de leguaje condicionado. Veamos como resolver el problema de clasificación de tweets sobre el que hemos estado trabajando anteriormente pero ahora utilizando el modelo BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcyc_TQ6dis7"
      },
      "source": [
        "### Para ejecutar este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFcNrbmjJ17"
      },
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgK8b6e_jJ17"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
        "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/TextDataset.py \\\n",
        "    --quiet --no-clobber --directory-prefix ./Utils/\n",
        "    \n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/BERT.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r BERT.txt --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntcs1AlpfckX"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gBXNzwYwKIu"
      },
      "source": [
        "Cargamos el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8vqJD9JwKIv"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-9nkTp9wKIy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
        "                                                    test_size=0.33, \n",
        "                                                    stratify=tweets['SECTOR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExV73PU4Ak1I"
      },
      "source": [
        "### Verificando el hardware disponible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gather": {
          "logged": 1604083146665
        },
        "id": "ULqO3R6_Am2I"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(\"Este notebook se está ejecutando en\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g27A6eGswKI2"
      },
      "source": [
        "Transferencia de Aprendizaje y Fine-tuning\n",
        "------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTeXlnTxwKI2"
      },
      "source": [
        "En general, existen 2 estrategias para utilizar modelos de lenguaje pre-entrenados en una tarea especifica:\n",
        " - Feature-based\n",
        " - Fine-tunning\n",
        " \n",
        "Las técnicas que se conocen como **Feature-based** utiliza arquitecturas especificas para resolver cada una de las tareas de NLP, en donde los pesos de las representaciones vectoriales están \"congeladas\" y no son parámetros que el modelo deba optimizar. En consecuencia, estos modelos son más rápidos de entrenar y permiten aplicar arquitecturas especificas que sean diferenciales en cada una de las tareas.\n",
        " \n",
        "Por el otro lado, las técnicas que emplean **Fine-tunning** tiene la flexibilidad de poder adaptar sus representaciones al permitir que todos los parametros sean optimizados en la tarea en particular. Además, estas arquitecturas permiten resolver multiples problemas de NLP utilizando una mínima cantidad de parametros específicos para la tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZo4aWuBwKI3"
      },
      "source": [
        "## BETO: BERT en español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7-UGaY4wKI3"
      },
      "source": [
        "Al igual que con `word2vec`, entrenar un modelo de lenguaje requiere de una gran cantidad de datos sumado a un poder de computo interesante (cuando BERT fué publicado en 2018, tomó 4 días entrenar el modelo usando 16 TPUs. Si se hubiera entrenado en 8 GPUs hubiera tomado entre 40–70 días). Por este motivo, utilizaremos un modelo pre-entrenado para un cuerpo de texto en español. Este modelo, BETO, fué entrenado sobre un gran corpora de textos. Pueden encontrar más información sobre en [el sitio web del autor](https://github.com/dccuchile/beto)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPouZ0G0wKI4"
      },
      "source": [
        "### Tokenizers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZyJe94cwKI4"
      },
      "source": [
        "BERT utiliza su propio tokenizer que está basado en WordPiece. Este tokenizer tiene un vocabulario de 30.000 tokens donde cada secuencia comienza con un token especial [CLS]. Recuerden que los tokenizers dependen del modelo con el que estamos trabajando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9JNa2s5wKI5"
      },
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer = transformers.BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', \n",
        "                                                           do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKcYot86wKI8"
      },
      "source": [
        "*Noten que el tokenizer depende del modelo que estamos utilizando*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1u3qFaSwKI9"
      },
      "source": [
        "## Crando un modelo de clasificación basado en BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmeXBEMwKI9"
      },
      "source": [
        "Trataremos de resolver entonces el mismo problema de clasificación con el que veniamos trabajando: clasificar los tweets dependiendo del sector al que pertenecen.Recordemos que tenemos 7 categorias distintas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdSw_zU_wKI-"
      },
      "source": [
        "tweets['SECTOR'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci385tLBwKJC"
      },
      "source": [
        "Necesitaremos contar con el numero de categorias para nuestro clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdhVi44BwKJC"
      },
      "source": [
        "num_labels=len(tweets['SECTOR'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5nqY-WbwKJH"
      },
      "source": [
        "Antes de hacer fine-tunning de nuestro modelo, tenemos que instanciar el modelo sobre el cual queremos aplicar esta técnica. Para ello instanciaremos el modelo base el cual no está entrenado en ninguna tarea en particular. De hecho, si habilitan las alertas en este notebook, verán que cuando se carga el modelo, la libreria HuggingFace les advierte sobre esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5egwTZzwKJH"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', \n",
        "                                                      num_labels=num_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU6ut9cNwKJL"
      },
      "source": [
        "Construiremos nuestro dataset sobre el que queremos entrenar el modelo. Recuerden que ya habíamos separado el set de datos en porciones para entrenar y para testear el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E4hlfckwKJM"
      },
      "source": [
        "### Como entrenar modelos con Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt82_r33wKJN"
      },
      "source": [
        "La librería transformers puede entrenar modelos tanto utilizando TensorFlow como PyTorch como backend. En nuestro caso utilizaremos PyTorch simplemente porque generaremos código un poco más compacto, pero pueden utilizar el backend con el que más cómodos se sientan:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiONF7_mwKJN"
      },
      "source": [
        "### Set de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjg3zyxvwKJO"
      },
      "source": [
        "Para utilizar el objeto `Trainer` que provee `transformers`, necesitamos crear un objetivo de tipo `Dataset`. `PyTorch` implementa este objeto el modulo `torch.utils.data.Dataset`. Para simplificar la tarea, disponemos de una clase que hace todo el procesamiento de datos y generación de los conjuntos de datos utilizando dicho modulo. Pueden encontrar esta implementación en `Utils.ClassificationDataset`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APkLobytwKJP"
      },
      "source": [
        "from Utils.TextDataset import ClassificationDataset\n",
        "\n",
        "train_dataset = ClassificationDataset(examples=X_train, labels=y_train, tokenizer=tokenizer)\n",
        "val_dataset = ClassificationDataset(examples=X_test, labels=y_test, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIHoFsEwKJT"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwTMGmSxwKJV"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "145AAAsPwKJW"
      },
      "source": [
        "Especificamos los parametros con los que entrenaremos nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrboy-qrwKJX"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directorio de trabajo del Trainer\n",
        "    num_train_epochs=3,              # Numero total de epochs sobre el que entrenaremos\n",
        "    per_device_train_batch_size=16,  # Tamaño del batch de datos por cada dispositivo de entrenamiento\n",
        "    per_device_eval_batch_size=64,   # Tamaño del batch de datos que usaremos para evaluación\n",
        "    warmup_steps=500,                # Numero de pasos que se usaran para determinar la politica de Learning Rate\n",
        "    weight_decay=0.01,               # Weight decay\n",
        "    logging_dir='./logs',            # Directorio de logs\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4c5WqRJwKJZ"
      },
      "source": [
        "Instanciamos el Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t18ZsFYFwKJZ"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # modelo sobre el que haremos fine tunning\n",
        "    args=training_args,                  # parametros del entrenamiento\n",
        "    train_dataset=train_dataset,         # set de datos de entrenamiento\n",
        "    eval_dataset=val_dataset             # set de datos de evaluación\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBTCaOqMwKJd"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX2YQs40jJ2O"
      },
      "source": [
        "Podemos inspeccionar nuestro modelo utilizando la herramienta `TensorBoard`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSiH5nDTwKJf"
      },
      "source": [
        "!tensorboard --logdir ./logs --bind_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaU5R8A3wscf"
      },
      "source": [
        "En Google Colab, podemos ver `TensorBoard` usando el siguiente `Magic`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfGvVj9VwxA1"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXgQvCX2wKJi"
      },
      "source": [
        "Verifiquemos la performance de nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPvxE9x6wKJi"
      },
      "source": [
        "predictions = trainer.predict(test_dataset=val_dataset).predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eP2FkgEwKJk"
      },
      "source": [
        "Para evaluar el modelo, primero deberemos obtener cual es la categoria que obtuvo la mayor probabilidad en la clasificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9sUT-SewKJl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = np.argmax(predictions, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_N1xUx8wKJn"
      },
      "source": [
        "Convertimos los IDs de las categorias a los labels correctos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60SPOSwwKJn"
      },
      "source": [
        "all_labels = val_dataset.get_labels()\n",
        "predictions_label = [all_labels[idx] for idx in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mAS5C3OwKJs"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8i-kiyu_82"
      },
      "source": [
        "trainer.save_model('tweet_classification')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-esA9Pb1HXJ",
        "outputId": "22aaf9f4-ab87-4b9c-e200-dab4012b2ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.save_pretrained('tweet_classification')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizer config file saved in tweet_classification/tokenizer_config.json\n",
            "Special tokens file saved in tweet_classification/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tweet_classification/tokenizer_config.json',\n",
              " 'tweet_classification/special_tokens_map.json',\n",
              " 'tweet_classification/vocab.txt',\n",
              " 'tweet_classification/added_tokens.json',\n",
              " 'tweet_classification/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6SYF1CW1Klt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhyE2m0ivD92"
      },
      "source": [
        "%%writefile tweet_classification.jsonnet\n",
        "\n",
        "local transformer_model = \"tweet_classification\";\n",
        "local transformer_dim = 768;\n",
        "\n",
        "{\n",
        "  \"dataset_reader\":{\n",
        "    \"type\": \"boolq\",\n",
        "    \"token_indexers\": {\n",
        "      \"tokens\": {\n",
        "        \"type\": \"pretrained_transformer\",\n",
        "        \"model_name\": transformer_model,\n",
        "      }\n",
        "    },\n",
        "    \"tokenizer\": {\n",
        "      \"type\": \"pretrained_transformer\",\n",
        "      \"model_name\": transformer_model,\n",
        "    }\n",
        "  },\n",
        "  \"train_data_path\": \"https://storage.googleapis.com/allennlp-public-data/BoolQ.zip!BoolQ/train.jsonl\",\n",
        "  \"validation_data_path\": \"https://storage.googleapis.com/allennlp-public-data/BoolQ.zip!BoolQ/val.jsonl\",\n",
        "  \"test_data_path\": \"https://storage.googleapis.com/allennlp-public-data/BoolQ.zip!BoolQ/test.jsonl\",\n",
        "  \"model\": {\n",
        "    \"type\": \"basic_classifier\",\n",
        "    \"text_field_embedder\": {\n",
        "      \"token_embedders\": {\n",
        "        \"tokens\": {\n",
        "          \"type\": \"pretrained_transformer\",\n",
        "          \"model_name\": transformer_model,\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"seq2vec_encoder\": {\n",
        "       \"type\": \"bert_pooler\",\n",
        "       \"pretrained_model\": transformer_model,\n",
        "       \"dropout\": 0.1,\n",
        "    },\n",
        "    \"namespace\": \"tags\",\n",
        "    \"num_labels\": 7,\n",
        "  },\n",
        "  \"data_loader\": {\n",
        "    \"batch_sampler\": {\n",
        "      \"type\": \"bucket\",\n",
        "      \"sorting_keys\": [\"tokens\"],\n",
        "      \"batch_size\" : 2\n",
        "    }\n",
        "  },\n",
        "  \"trainer\": {\n",
        "    \"num_epochs\": 10,\n",
        "    \"validation_metric\": \"+accuracy\",\n",
        "    \"learning_rate_scheduler\": {\n",
        "      \"type\": \"slanted_triangular\",\n",
        "      \"num_epochs\": 10,\n",
        "      \"num_steps_per_epoch\": 3088,\n",
        "      \"cut_frac\": 0.06\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "      \"type\": \"huggingface_adamw\",\n",
        "      \"lr\": 1e-5,\n",
        "      \"weight_decay\": 0.1,\n",
        "    },\n",
        "    \"num_gradient_accumulation_steps\": 16,\n",
        "  },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBMwet1nwDzL"
      },
      "source": [
        "predictor = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-6RPbOojxwQ"
      },
      "source": [
        "trained_model = trainer.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29IvNhAskyA4"
      },
      "source": [
        "indexed_tokens = tokenizer.encode(\"Nunca mas compro en Mercadona!\")\n",
        "tokens_tensor = torch.tensor([indexed_tokens]).to(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8VpqDjfllsP"
      },
      "source": [
        "preds = trained_model(tokens_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdXcRNjwmaYO"
      },
      "source": [
        "score, indices = torch.max(preds.logits, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSfIVEEkluUD"
      },
      "source": [
        "score.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqH3gAdFqVwU"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SesCfrcrN_R"
      },
      "source": [
        "pip install -U google-cloud-storage==1.40.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQPaNL2s23z"
      },
      "source": [
        "from typing import Dict, Iterable, List\n",
        "\n",
        "from allennlp.data import DatasetReader, Instance\n",
        "from allennlp.data.fields import Field, LabelField, TextField\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6CZ-b00j9g"
      },
      "source": [
        "import allennlp"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQfVriC0J6i"
      },
      "source": [
        "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wREe3M71QjN"
      },
      "source": [
        "from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQ6adFH0w9-",
        "outputId": "c9b0d20c-840a-45ad-dad4-acec7e37bc0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = PretrainedTransformerTokenizer(\"tweet_classification\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Didn't find file tweet_classification/added_tokens.json. We won't load it.\n",
            "loading file tweet_classification/vocab.txt\n",
            "loading file tweet_classification/tokenizer.json\n",
            "loading file None\n",
            "loading file tweet_classification/special_tokens_map.json\n",
            "loading file tweet_classification/tokenizer_config.json\n",
            "Didn't find file tweet_classification/added_tokens.json. We won't load it.\n",
            "loading file tweet_classification/vocab.txt\n",
            "loading file tweet_classification/tokenizer.json\n",
            "loading file None\n",
            "loading file tweet_classification/special_tokens_map.json\n",
            "loading file tweet_classification/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXKCyWoP1WHh"
      },
      "source": [
        "indexer = PretrainedTransformerIndexer(\"tweet_classification\")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ2EaLOTs4o2"
      },
      "source": [
        "@DatasetReader.register(\"classification-transformers\")\n",
        "class ClassificationTransformerReader(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        max_tokens: int = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if self.max_tokens:\n",
        "            tokens = tokens[: self.max_tokens]\n",
        "        text_field = TextField(tokens, self.token_indexers)\n",
        "        fields: Dict[str, Field] = {\"text\": text_field}\n",
        "        if label:\n",
        "            fields[\"label\"] = LabelField(label)\n",
        "        return Instance(fields)\n",
        "\n",
        "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
        "        with open(file_path, \"r\") as lines:\n",
        "            for line in lines:\n",
        "                text, sentiment = line.strip().split(\"\\t\")\n",
        "                yield self.text_to_instance(text, sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B28J8jCpmkQq"
      },
      "source": [
        "from allennlp.interpret.saliency_interpreters import SimpleGradient\n",
        "from allennlp.predictors import Predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soe8I3wuxY4o"
      },
      "source": [
        "from allennlp.data import DatasetReader\n",
        "from allennlp import pretrained_transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USOdKLOLxcsL"
      },
      "source": [
        "dr = DatasetReader.from_params('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTX6HgF6sXrs"
      },
      "source": [
        "predictor = Predictor(trained_model, dataset_reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djiGHlzpqeun"
      },
      "source": [
        "interpreter = SimpleGradient()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwpecDy2jvTl"
      },
      "source": [
        "def plot_saliency(loaded_model, pure_txt, text_class, pred_labels, text_sequence):\n",
        "    text_class = 'Positive' if text_class==1 else 'Negative'\n",
        "    pred_labels = 'Positive' if pred_labels==1 else 'Negative'\n",
        "\n",
        "    input_tensors = [loaded_model.input, K.learning_phase()]\n",
        "    model_input = loaded_model.layers[2].input # the input for convolution layer\n",
        "    model_output = loaded_model.output[0][1]\n",
        "    gradients = loaded_model.optimizer.get_gradients(model_output,model_input)\n",
        "    compute_gradients = K.function(inputs=input_tensors, outputs=gradients)\n",
        "    matrix = compute_gradients([text_sequence.reshape(1,30), text_class])[0][0]\n",
        "    matrix = matrix[:len(pure_txt),:]\n",
        "\n",
        "    matrix_magnify=np.zeros((matrix.shape[0]*10,matrix.shape[1]))\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(10):\n",
        "            matrix_magnify[i*10+j,:]=matrix[i,:]\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.imshow(normalize_array(np.absolute(matrix_magnify)), interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.yticks(np.arange(5, matrix.shape[0]*10, 10), pure_txt, weight='bold',fontsize=24)\n",
        "    plt.xticks(np.arange(0, matrix.shape[1], 50), weight='bold',fontsize=24)\n",
        "    plt.title('True Label: \"{}\" Predicted Label: \"{}\"'.format(text_class,pred_labels), weight='bold',fontsize=24)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}