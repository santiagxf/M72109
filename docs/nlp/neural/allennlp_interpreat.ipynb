{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Fine-tunning.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "nlp-py38"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ad447417a964e458bafed0b78dee53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1f73e798942049d1806feeadeaa07d90",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1df3f9d761294bfda067a7192de51c91",
              "IPY_MODEL_9284d0dbaf53438592985960212a2d91",
              "IPY_MODEL_842807823f3749d4b10261a521ca29dc"
            ]
          }
        },
        "1f73e798942049d1806feeadeaa07d90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1df3f9d761294bfda067a7192de51c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b5d319e509614742bad2d0544726ab62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfb112e64cba4978ba9eb349bc0096c5"
          }
        },
        "9284d0dbaf53438592985960212a2d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_54b7be9f29a4453c8252e882cce8f26e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 439621341,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 439621341,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8558a6f8f279491ba0026bec0e8893cf"
          }
        },
        "842807823f3749d4b10261a521ca29dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3e75c8ce6eb4fe699bbc58a5ca1333e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 419M/419M [00:13&lt;00:00, 35.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_645f552b6cc6494fb87784adf16b6191"
          }
        },
        "b5d319e509614742bad2d0544726ab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfb112e64cba4978ba9eb349bc0096c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54b7be9f29a4453c8252e882cce8f26e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8558a6f8f279491ba0026bec0e8893cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3e75c8ce6eb4fe699bbc58a5ca1333e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "645f552b6cc6494fb87784adf16b6191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKENeWEEwKIb"
      },
      "source": [
        "\n",
        "BERT en un problema de clasificación\n",
        "===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JctlpI10jJ15"
      },
      "source": [
        "Introducción\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT_t9OxYwKIc"
      },
      "source": [
        "Los modelos basados en transformers nos pueden ayudar a resolver varios tipos de problemas. Desde problemas de clasificación y regresión hasta tareas más complejas como resumen de textos o generación de leguaje condicionado. Veamos como resolver el problema de clasificación de tweets sobre el que hemos estado trabajando anteriormente pero ahora utilizando el modelo BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcyc_TQ6dis7"
      },
      "source": [
        "### Para ejecutar este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFcNrbmjJ17"
      },
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgK8b6e_jJ17",
        "outputId": "1f9ecd9f-3197-4f6e-e564-b8b040b37321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
        "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/TextDataset.py \\\n",
        "    --quiet --no-clobber --directory-prefix ./Utils/\n",
        "    \n",
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/BERT.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r BERT.txt --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 33.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntcs1AlpfckX"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gBXNzwYwKIu"
      },
      "source": [
        "Cargamos el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8vqJD9JwKIv"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-9nkTp9wKIy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
        "                                                    test_size=0.33, \n",
        "                                                    stratify=tweets['SECTOR'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExV73PU4Ak1I"
      },
      "source": [
        "### Verificando el hardware disponible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gather": {
          "logged": 1604083146665
        },
        "id": "ULqO3R6_Am2I",
        "outputId": "9f9f2677-f209-43b9-980d-264a3957f4d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(\"Este notebook se está ejecutando en\", device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Este notebook se está ejecutando en cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g27A6eGswKI2"
      },
      "source": [
        "Transferencia de Aprendizaje y Fine-tuning\n",
        "------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTeXlnTxwKI2"
      },
      "source": [
        "En general, existen 2 estrategias para utilizar modelos de lenguaje pre-entrenados en una tarea especifica:\n",
        " - Feature-based\n",
        " - Fine-tunning\n",
        " \n",
        "Las técnicas que se conocen como **Feature-based** utiliza arquitecturas especificas para resolver cada una de las tareas de NLP, en donde los pesos de las representaciones vectoriales están \"congeladas\" y no son parámetros que el modelo deba optimizar. En consecuencia, estos modelos son más rápidos de entrenar y permiten aplicar arquitecturas especificas que sean diferenciales en cada una de las tareas.\n",
        " \n",
        "Por el otro lado, las técnicas que emplean **Fine-tunning** tiene la flexibilidad de poder adaptar sus representaciones al permitir que todos los parametros sean optimizados en la tarea en particular. Además, estas arquitecturas permiten resolver multiples problemas de NLP utilizando una mínima cantidad de parametros específicos para la tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZo4aWuBwKI3"
      },
      "source": [
        "## BETO: BERT en español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7-UGaY4wKI3"
      },
      "source": [
        "Al igual que con `word2vec`, entrenar un modelo de lenguaje requiere de una gran cantidad de datos sumado a un poder de computo interesante (cuando BERT fué publicado en 2018, tomó 4 días entrenar el modelo usando 16 TPUs. Si se hubiera entrenado en 8 GPUs hubiera tomado entre 40–70 días). Por este motivo, utilizaremos un modelo pre-entrenado para un cuerpo de texto en español. Este modelo, BETO, fué entrenado sobre un gran corpora de textos. Pueden encontrar más información sobre en [el sitio web del autor](https://github.com/dccuchile/beto)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPouZ0G0wKI4"
      },
      "source": [
        "### Tokenizers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZyJe94cwKI4"
      },
      "source": [
        "BERT utiliza su propio tokenizer que está basado en WordPiece. Este tokenizer tiene un vocabulario de 30.000 tokens donde cada secuencia comienza con un token especial [CLS]. Recuerden que los tokenizers dependen del modelo con el que estamos trabajando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9JNa2s5wKI5",
        "outputId": "edeb028a-b0a3-46a9-be0c-87153774d3bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer = transformers.BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', \n",
        "                                                           do_lower_case=True)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eebf656e2fb33420d0d3f12a0650df76137cfd2251e04587d7d926fba30ab1b0.bfb98b35b81356261ec63a5ff66aa147928e2c8f4d09be77fc850582a1000498\n",
            "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/85478b69412001fdb7b4cb1f5e5c5e49df292e7de8a8a27c465348fd70e817e3.1fea6aa627ed25376d8778ace0885102803fe6651fb5638d1cea57cae8ccfa7f\n",
            "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/78141ed1e8dcc5ff370950397ca0d1c5c9da478f54ec14544187d8a93eff1a26.f982506b52498d4adb4bd491f593dc92b2ef6be61bfdbe9d30f53f963f9f5b66\n",
            "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/75654903071ce2eb376ae88599e5a32c926746e653c5f59fa8c72ede82bb45e5.97aaa6cf1585446e253a70715325df5cdf1791627e0480c0084d0dff6c5ebbf8\n",
            "loading configuration file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/2416dab24674c27b5521594d6aa0929fc843a024c96711b1b5015cdff867291f.afa3630b664b4bd3e82d41660bdb96ec13236bbceadb0ae7c45c7c19f58652c7\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3CwhMx3h-FB",
        "outputId": "d5845cb8-de4f-40ee-a42c-df9e45d9ecfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.encode_plus(text=\"la casa estaba vacia\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [4, 1032, 1635, 1594, 15912, 30956, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKcYot86wKI8"
      },
      "source": [
        "*Noten que el tokenizer depende del modelo que estamos utilizando*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1u3qFaSwKI9"
      },
      "source": [
        "## Crando un modelo de clasificación basado en BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmeXBEMwKI9"
      },
      "source": [
        "Trataremos de resolver entonces el mismo problema de clasificación con el que veniamos trabajando: clasificar los tweets dependiendo del sector al que pertenecen.Recordemos que tenemos 7 categorias distintas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdSw_zU_wKI-",
        "outputId": "b7471951-f696-481b-98a2-f1828d82d91d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tweets['SECTOR'].unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['RETAIL', 'TELCO', 'ALIMENTACION', 'AUTOMOCION', 'BANCA',\n",
              "       'BEBIDAS', 'DEPORTES'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci385tLBwKJC"
      },
      "source": [
        "Necesitaremos contar con el numero de categorias para nuestro clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdhVi44BwKJC"
      },
      "source": [
        "num_labels=len(tweets['SECTOR'].unique())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5nqY-WbwKJH"
      },
      "source": [
        "Antes de hacer fine-tunning de nuestro modelo, tenemos que instanciar el modelo sobre el cual queremos aplicar esta técnica. Para ello instanciaremos el modelo base el cual no está entrenado en ninguna tarea en particular. De hecho, si habilitan las alertas en este notebook, verán que cuando se carga el modelo, la libreria HuggingFace les advierte sobre esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5egwTZzwKJH",
        "outputId": "4c2d94b3-d614-414e-e975-a04245db1c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "0ad447417a964e458bafed0b78dee53e",
            "1f73e798942049d1806feeadeaa07d90",
            "1df3f9d761294bfda067a7192de51c91",
            "9284d0dbaf53438592985960212a2d91",
            "842807823f3749d4b10261a521ca29dc",
            "b5d319e509614742bad2d0544726ab62",
            "cfb112e64cba4978ba9eb349bc0096c5",
            "54b7be9f29a4453c8252e882cce8f26e",
            "8558a6f8f279491ba0026bec0e8893cf",
            "a3e75c8ce6eb4fe699bbc58a5ca1333e",
            "645f552b6cc6494fb87784adf16b6191"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', \n",
        "                                                      num_labels=num_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ad447417a964e458bafed0b78dee53e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/419M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU6ut9cNwKJL"
      },
      "source": [
        "Construiremos nuestro dataset sobre el que queremos entrenar el modelo. Recuerden que ya habíamos separado el set de datos en porciones para entrenar y para testear el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E4hlfckwKJM"
      },
      "source": [
        "### Como entrenar modelos con Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt82_r33wKJN"
      },
      "source": [
        "La librería transformers puede entrenar modelos tanto utilizando TensorFlow como PyTorch como backend. En nuestro caso utilizaremos PyTorch simplemente porque generaremos código un poco más compacto, pero pueden utilizar el backend con el que más cómodos se sientan:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiONF7_mwKJN"
      },
      "source": [
        "### Set de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjg3zyxvwKJO"
      },
      "source": [
        "Para utilizar el objeto `Trainer` que provee `transformers`, necesitamos crear un objetivo de tipo `Dataset`. `PyTorch` implementa este objeto el modulo `torch.utils.data.Dataset`. Para simplificar la tarea, disponemos de una clase que hace todo el procesamiento de datos y generación de los conjuntos de datos utilizando dicho modulo. Pueden encontrar esta implementación en `Utils.ClassificationDataset`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APkLobytwKJP"
      },
      "source": [
        "from Utils.TextDataset import ClassificationDataset\n",
        "\n",
        "train_dataset = ClassificationDataset(examples=X_train, labels=y_train, tokenizer=tokenizer)\n",
        "val_dataset = ClassificationDataset(examples=X_test, labels=y_test, tokenizer=tokenizer)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIHoFsEwKJT"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwTMGmSxwKJV"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "145AAAsPwKJW"
      },
      "source": [
        "Especificamos los parametros con los que entrenaremos nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrboy-qrwKJX"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directorio de trabajo del Trainer\n",
        "    num_train_epochs=3,              # Numero total de epochs sobre el que entrenaremos\n",
        "    per_device_train_batch_size=16,  # Tamaño del batch de datos por cada dispositivo de entrenamiento\n",
        "    per_device_eval_batch_size=64,   # Tamaño del batch de datos que usaremos para evaluación\n",
        "    warmup_steps=500,                # Numero de pasos que se usaran para determinar la politica de Learning Rate\n",
        "    weight_decay=0.01,               # Weight decay\n",
        "    logging_dir='./logs',            # Directorio de logs\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4c5WqRJwKJZ"
      },
      "source": [
        "Instanciamos el Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t18ZsFYFwKJZ"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # modelo sobre el que haremos fine tunning\n",
        "    args=training_args,                  # parametros del entrenamiento\n",
        "    train_dataset=train_dataset,         # set de datos de entrenamiento\n",
        "    eval_dataset=val_dataset             # set de datos de evaluación\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBTCaOqMwKJd",
        "outputId": "e14c5b59-6c46-4944-a24f-7c7ba6f93c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 2521\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 474\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [474/474 03:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.762200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.512200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.094200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.046600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=474, training_loss=0.5152663399901571, metrics={'train_runtime': 233.7099, 'train_samples_per_second': 32.361, 'train_steps_per_second': 2.028, 'total_flos': 310937225724000.0, 'train_loss': 0.5152663399901571, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXgQvCX2wKJi"
      },
      "source": [
        "Verifiquemos la performance de nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPvxE9x6wKJi",
        "outputId": "3c0f684f-994e-4c43-fbec-8827aa73918c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "predictions = trainer.predict(test_dataset=val_dataset).predictions"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 1242\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eP2FkgEwKJk"
      },
      "source": [
        "Para evaluar el modelo, primero deberemos obtener cual es la categoria que obtuvo la mayor probabilidad en la clasificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9sUT-SewKJl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = np.argmax(predictions, axis=1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_N1xUx8wKJn"
      },
      "source": [
        "Convertimos los IDs de las categorias a los labels correctos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60SPOSwwKJn"
      },
      "source": [
        "all_labels = val_dataset.get_labels()\n",
        "predictions_label = [all_labels[idx] for idx in predictions]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mAS5C3OwKJs",
        "outputId": "b1df80a0-22d3-45f4-c8af-df1f2608d26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions_label))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.99      1.00      1.00       110\n",
            "  AUTOMOCION       1.00      0.99      1.00       148\n",
            "       BANCA       0.99      0.98      0.99       198\n",
            "     BEBIDAS       1.00      0.98      0.99       223\n",
            "    DEPORTES       0.96      1.00      0.98       216\n",
            "      RETAIL       0.99      0.98      0.98       268\n",
            "       TELCO       0.96      0.97      0.97        79\n",
            "\n",
            "    accuracy                           0.99      1242\n",
            "   macro avg       0.99      0.99      0.99      1242\n",
            "weighted avg       0.99      0.99      0.99      1242\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b-FbkcZOvE6"
      },
      "source": [
        "Persistimos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8i-kiyu_82",
        "outputId": "3e9edc21-54c1-4bb8-f585-3e0317433196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainer.save_model('tweet_classification')\n",
        "tokenizer.save_pretrained('tweet_classification')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to tweet_classification\n",
            "Configuration saved in tweet_classification/config.json\n",
            "Model weights saved in tweet_classification/pytorch_model.bin\n",
            "tokenizer config file saved in tweet_classification/tokenizer_config.json\n",
            "Special tokens file saved in tweet_classification/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tweet_classification/tokenizer_config.json',\n",
              " 'tweet_classification/special_tokens_map.json',\n",
              " 'tweet_classification/vocab.txt',\n",
              " 'tweet_classification/added_tokens.json',\n",
              " 'tweet_classification/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6SYF1CW1Klt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhyE2m0ivD92"
      },
      "source": [
        "%%writefile tweet_classification.jsonnet\n",
        "\n",
        "local transformer_model = \"tweet_classification\";\n",
        "local transformer_dim = 768;\n",
        "\n",
        "{\n",
        "  \"dataset_reader\":{\n",
        "    \"type\": \"boolq\",\n",
        "    \"token_indexers\": {\n",
        "      \"tokens\": {\n",
        "        \"type\": \"pretrained_transformer\",\n",
        "        \"model_name\": transformer_model,\n",
        "      }\n",
        "    },\n",
        "    \"tokenizer\": {\n",
        "      \"type\": \"pretrained_transformer\",\n",
        "      \"model_name\": transformer_model,\n",
        "    }\n",
        "  },\n",
        "  \"train_data_path\": \"https://storage.googleapis.com/allennlp-public-data/BoolQ.zip!BoolQ/train.jsonl\",\n",
        "  \"validation_data_path\": \"https://storage.googleapis.com/allennlp-public-data/BoolQ.zip!BoolQ/val.jsonl\",\n",
        "  \"test_data_path\": \"https://storage.googleapis.com/allennlp-public-data/BoolQ.zip!BoolQ/test.jsonl\",\n",
        "  \"model\": {\n",
        "    \"type\": \"basic_classifier\",\n",
        "    \"text_field_embedder\": {\n",
        "      \"token_embedders\": {\n",
        "        \"tokens\": {\n",
        "          \"type\": \"pretrained_transformer\",\n",
        "          \"model_name\": transformer_model,\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"seq2vec_encoder\": {\n",
        "       \"type\": \"bert_pooler\",\n",
        "       \"pretrained_model\": transformer_model,\n",
        "       \"dropout\": 0.1,\n",
        "    },\n",
        "    \"namespace\": \"tags\",\n",
        "    \"num_labels\": 7,\n",
        "  },\n",
        "  \"data_loader\": {\n",
        "    \"batch_sampler\": {\n",
        "      \"type\": \"bucket\",\n",
        "      \"sorting_keys\": [\"tokens\"],\n",
        "      \"batch_size\" : 2\n",
        "    }\n",
        "  },\n",
        "  \"trainer\": {\n",
        "    \"num_epochs\": 10,\n",
        "    \"validation_metric\": \"+accuracy\",\n",
        "    \"learning_rate_scheduler\": {\n",
        "      \"type\": \"slanted_triangular\",\n",
        "      \"num_epochs\": 10,\n",
        "      \"num_steps_per_epoch\": 3088,\n",
        "      \"cut_frac\": 0.06\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "      \"type\": \"huggingface_adamw\",\n",
        "      \"lr\": 1e-5,\n",
        "      \"weight_decay\": 0.1,\n",
        "    },\n",
        "    \"num_gradient_accumulation_steps\": 16,\n",
        "  },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBMwet1nwDzL"
      },
      "source": [
        "predictor = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-6RPbOojxwQ"
      },
      "source": [
        "trained_model = trainer.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29IvNhAskyA4"
      },
      "source": [
        "indexed_tokens = tokenizer.encode(\"Nunca mas compro en Mercadona!\")\n",
        "tokens_tensor = torch.tensor([indexed_tokens]).to(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM6q5L0qSuXI",
        "outputId": "87205541-24b6-479b-829e-e512d6894920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.tokenize(\"Nunca mas compro en Mercadona!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[CLS], nunca, mas, compro, en, mercado, ##na, !, [SEP]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8VpqDjfllsP"
      },
      "source": [
        "preds = trained_model(tokens_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdXcRNjwmaYO"
      },
      "source": [
        "score, indices = torch.max(preds.logits, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqH3gAdFqVwU",
        "outputId": "bdbf05ef-5371-476c-dc94-fd891dbaf409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install -U google-cloud-storage==1.40.0"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.8.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: torch<1.11.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision<0.12.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu111)\n",
            "Collecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 39.5 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 53.1 MB/s \n",
            "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.6-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 46.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 43.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: transformers<4.13,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.12.3)\n",
            "Collecting cached-path<0.4.0,>=0.3.1\n",
            "  Downloading cached_path-0.3.2-py3-none-any.whl (26 kB)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: filelock<3.4,>=3.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.1)\n",
            "Collecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 20 kB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<0.4.0,>=0.3.1->allennlp) (1.18.1)\n",
            "Collecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.19.12-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.23.0,>=1.22.12\n",
            "  Downloading botocore-1.22.12-py3-none-any.whl (8.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.12->boto3<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 48.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.26.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.6.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.12.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.13,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.13,>=4.1->allennlp) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.13,>=4.1->allennlp) (0.0.46)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.1.0-py3-none-any.whl (19 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 54.2 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (2.0.7)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 55.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 37.1 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 38.3 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 52.5 MB/s \n",
            "\u001b[?25hCollecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-3.0.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.4.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.6.0-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (5.2.2)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.13,>=4.1->allennlp) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=5eb3c8a3ab061a435f64ec671cddadc05e676849f63c702aaab5c73eaac75200\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=f3ab699beb098b4ece680a9259e4c51f48a4c74ee1eafbc5d96f5514a4e8971e\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=ee1d27e189adc9c988d0655ba373cbdccdf3c97004fd125e3c02c10ffebecb14\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388669 sha256=482172973af6a2416c9d799e669560dd84e2813aefd6f6ca28db76e1aa264edc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=5d25aa4ae2bee51ed3b88a9d2d4519d15a49764e17e631e405e8d971dcab4e0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=72e06d43600a30f96c3e56fce1f024aba7b1f9d4f5b8aa162bbe5733d3bf5f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=86f1424d4b81ad91e900e0aab8a9041d825a88fcc0561ee01250b7ac986a5c91\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=8169a03af7b36f00d397931ac37cc34928af2855749d04c0d447ba9b85f1d7ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=6c3790ba2add2c02d19cdc483db9d43fa7a8a392e0e4e5f1028da0783c330f3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=9f773638ceb67cbfbbfe4375ffd65cb93c7fd1b9e763c56668fc9bb259c46842\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=f6a010cc8aad574f4e97673db9c085f229806bed1c5b5efc8a53f3bf47a646d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: urllib3, jaraco.functools, tempora, multidict, jmespath, jaraco.text, jaraco.classes, frozenlist, zc.lockfile, yarl, smmap, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, botocore, asynctest, async-timeout, aiosignal, s3transfer, python-docx, pdfminer.six, gitdb, fsspec, feedparser, cherrypy, backports.csv, aiohttp, yaspin, xxhash, subprocess32, shortuuid, sentry-sdk, patternfork-nosql, pathtools, overrides, munch, iso-639, GitPython, docker-pycreds, configparser, boto3, wandb, tensorboardX, sqlitedict, sentencepiece, jsonnet, fairscale, datasets, checklist, cached-path, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.8.0 aiosignal-1.2.0 allennlp-2.8.0 async-timeout-4.0.0 asynctest-0.13.0 backports.csv-1.0.7 base58-2.1.1 boto3-1.19.12 botocore-1.22.12 cached-path-0.3.2 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.1.0 cryptography-35.0.0 datasets-1.15.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 frozenlist-1.2.0 fsspec-2021.11.0 gitdb-4.0.9 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.4.0 jaraco.text-3.6.0 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.2.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20211012 portend-3.0.0 python-docx-0.8.11 s3transfer-0.5.0 sentencepiece-0.1.96 sentry-sdk-1.4.3 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-5.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.2 tensorboardX-2.4 urllib3-1.25.11 wandb-0.12.6 xxhash-2.0.2 yarl-1.7.2 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-storage==1.40.0\n",
            "  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 104 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2.0dev,>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage==1.40.0) (1.35.0)\n",
            "Collecting google-resumable-media<2.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.4.1\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage==1.40.0) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage==1.40.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage==1.40.0) (4.7.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage==1.40.0) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage==1.40.0) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage==1.40.0) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage==1.40.0) (1.26.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage==1.40.0) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage==1.40.0) (21.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage==1.40.0) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage==1.40.0) (1.53.0)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage==1.40.0) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage==1.40.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==1.40.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==1.40.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==1.40.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==1.40.0) (1.25.11)\n",
            "Installing collected packages: google-crc32c, google-resumable-media, google-cloud-core, google-cloud-storage\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 1.3.3 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-core-1.7.2 google-cloud-storage-1.40.0 google-crc32c-1.3.0 google-resumable-media-1.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQPaNL2s23z"
      },
      "source": [
        "from typing import Dict, Iterable, List\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.data import DatasetReader, Instance, TextFieldTensors\n",
        "from allennlp.data.fields import Field, LabelField, TextField, ArrayField, TensorField, TransformerTextField\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o2fjaFnc-qS"
      },
      "source": [
        "from allennlp.data.vocabulary import PreTrainedTokenizer, Vocabulary"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq8VYMCPYPfj"
      },
      "source": [
        "from allennlp.modules.token_embedders import PretrainedTransformerEmbedder"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE_mE-tXYeQK"
      },
      "source": [
        "params = Params({\"model_name\": \"tweet_classification\"})\n",
        "embedder = PretrainedTransformerEmbedder.from_params(params)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txCIjG4PTxYw"
      },
      "source": [
        "from allennlp.models import BasicClassifier, Model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQfVriC0J6i"
      },
      "source": [
        "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer\n",
        "from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer\n",
        "from allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQ6adFH0w9-"
      },
      "source": [
        "tokenizer = PretrainedTransformerTokenizer(\"tweet_classification\")\n",
        "indexer = PretrainedTransformerIndexer(\"tweet_classification\")\n",
        "encoder = BertPooler(\"tweet_classification\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka-ToFjcio0X"
      },
      "source": [
        "values = indexer.tokens_to_indices(tokenizer.tokenize(\"la casa estaba vacia\"), vocabulary=model.vocab)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTr7dQYAPyw",
        "outputId": "dd02562c-04b5-4b0f-d007-0c7e2510163a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "values"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mask': [True, True, True, True, True, True, True],\n",
              " 'token_ids': [4, 1032, 1635, 1594, 15912, 30956, 5],\n",
              " 'type_ids': [0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJS_5MUHT6ae"
      },
      "source": [
        "model = BasicClassifier(vocab=Vocabulary(), text_field_embedder=embedder, seq2vec_encoder=encoder, dropout=0.1, num_labels=7)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soe8I3wuxY4o"
      },
      "source": [
        "from allennlp.data import DatasetReader"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf06oHnX6hEX"
      },
      "source": [
        "from allennlp_models.classification.dataset_readers import boolq"
      ],
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ2EaLOTs4o2"
      },
      "source": [
        "class ClassificationTransformerReader(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer,\n",
        "        token_indexers: Dict[str, TokenIndexer],\n",
        "        vocab,\n",
        "        max_tokens: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.max_tokens = max_tokens\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        transformer_input = indexer.tokens_to_indices(tokens, vocabulary=self.vocab)\n",
        "        if self.max_tokens:\n",
        "            tokens = tokens[: self.max_tokens]\n",
        "        \n",
        "        #text_field = TextField(tokens, self.token_indexers)\n",
        "        #fields: Dict[str, Field] = {\"tokens\": text_field }\n",
        "        inputs = TransformerTextField(input_ids=transformer_input[\"token_ids\"],\n",
        "                                      token_type_ids=transformer_input[\"type_ids\"],\n",
        "                                      attention_mask=transformer_input[\"mask\"])\n",
        "        fields: Dict[str, Field] = {\"tokens\": inputs }\n",
        "        if label:\n",
        "            fields[\"label\"] = LabelField(label)\n",
        "        instance = Instance(fields)\n",
        "        instance.indexed = True\n",
        "        return instance"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B28J8jCpmkQq"
      },
      "source": [
        "from allennlp.interpret.saliency_interpreters import SimpleGradient\n",
        "from allennlp.predictors import Predictor, TextClassifierPredictor"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USOdKLOLxcsL"
      },
      "source": [
        "dataset_reader = ClassificationTransformerReader(tokenizer=tokenizer, token_indexers={\"tokens\": indexer}, vocab=model.vocab, max_tokens=400)"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11t8d8z3BWjH"
      },
      "source": [
        "instance = dataset_reader.text_to_instance(\"la vaca esta afuera\")"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Id7l50BbWL"
      },
      "source": [
        "dataset = Batch([instance])"
      ],
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKVCkM-yBmFQ"
      },
      "source": [
        "dataset.index_instances(model.vocab)"
      ],
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-GYJtprBqXu",
        "outputId": "b9da5067-96e6-408b-e43a-8399ff8d6189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "from allennlp.nn import util\n",
        "model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-362-dc8ac0baeecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cuda_device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVQSvtlOB0qU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTX6HgF6sXrs"
      },
      "source": [
        "predictor = TextClassifierPredictor(model, dataset_reader)"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djiGHlzpqeun"
      },
      "source": [
        "interpreter = SimpleGradient(predictor)"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_i_OjI_edIS"
      },
      "source": [
        "inputs = {\"sentence\": \"a very well-made, funny and entertaining picture.\"}"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_00Ya8ZelBk",
        "outputId": "1ec1b8fe-741a-44da-ab1c-6d35c2f6e313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "interpretation = interpreter.saliency_interpret_from_json(inputs)"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-357-232674d5e00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterpretation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaliency_interpret_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/interpret/saliency_interpreters/simple_gradient.py\u001b[0m in \u001b[0;36msaliency_interpret_from_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mnormalized\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msanitized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mlabeled_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_to_labeled_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0minstances_with_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mjson_to_labeled_instances\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_token_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mnew_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions_to_labeled_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensors\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minto\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_output_human_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/models/basic_classifier.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, label, metadata)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0mscalar\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0moptimised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0membedded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_field_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_field_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwpecDy2jvTl"
      },
      "source": [
        "def plot_saliency(loaded_model, pure_txt, text_class, pred_labels, text_sequence):\n",
        "    text_class = 'Positive' if text_class==1 else 'Negative'\n",
        "    pred_labels = 'Positive' if pred_labels==1 else 'Negative'\n",
        "\n",
        "    input_tensors = [loaded_model.input, K.learning_phase()]\n",
        "    model_input = loaded_model.layers[2].input # the input for convolution layer\n",
        "    model_output = loaded_model.output[0][1]\n",
        "    gradients = loaded_model.optimizer.get_gradients(model_output,model_input)\n",
        "    compute_gradients = K.function(inputs=input_tensors, outputs=gradients)\n",
        "    matrix = compute_gradients([text_sequence.reshape(1,30), text_class])[0][0]\n",
        "    matrix = matrix[:len(pure_txt),:]\n",
        "\n",
        "    matrix_magnify=np.zeros((matrix.shape[0]*10,matrix.shape[1]))\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(10):\n",
        "            matrix_magnify[i*10+j,:]=matrix[i,:]\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.imshow(normalize_array(np.absolute(matrix_magnify)), interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.yticks(np.arange(5, matrix.shape[0]*10, 10), pure_txt, weight='bold',fontsize=24)\n",
        "    plt.xticks(np.arange(0, matrix.shape[1], 50), weight='bold',fontsize=24)\n",
        "    plt.title('True Label: \"{}\" Predicted Label: \"{}\"'.format(text_class,pred_labels), weight='bold',fontsize=24)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgva2dUum_Ia"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}