{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQGxT74Sdis4"
   },
   "source": [
    "Actividad 2: Modelos basados en secuencias con Word2Vec\n",
    "======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducción\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjuHJnQOdis6"
   },
   "source": [
    "Los modelos basados en secuencias tienen la fortaleza que toman una secuencia de token (en un determinado orden) y generan una salida dependiendo del tipo de problema que se trate.\n",
    " - Seq2Class: Toman una secuencia de tokens y generan una clase\n",
    " - Seq2Seq: Toman una secuencia de token y generan otra secuencia de tokens. \n",
    " \n",
    "Vimos como podemos generar un modelo de secuencia utilizando `Word2Vec` y redes LSTM. Sin embargo ¿Les parece que conseguimos una buena performance?\n",
    "\n",
    "En esta actividad les proponemos ver como podemos mejorar la performance de este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dcyc_TQ6dis7"
   },
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/normalization.py \\\n",
    "    --quiet --no-clobber --directory-prefix ./m72109/nlp/\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/transformation.py \\\n",
    "    --quiet --no-clobber --directory-prefix ./m72109/nlp/\n",
    "    \n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/sequences-word2vec.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r sequences-word2vec.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gl18LLmkeqjx"
   },
   "source": [
    "Descargamos nuestros vectores de word2vec en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "LJVkiH2lesN1",
    "outputId": "601bd944-7669-47dc-b0ca-7aa67938c492"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./Models/Word2Vec\n",
    "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/model-es.bin \\\n",
    "    --quiet --no-clobber --directory-prefix ./Models/Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ntcs1AlpfckX"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rRpBRahditI"
   },
   "source": [
    "Instalamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "UgVf9-V7ditI",
    "outputId": "4a451c22-3615-471e-dd17-22b4c56b589f"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CC7vGpjqditL"
   },
   "source": [
    "Cargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmJpenUkditM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKrEadrGditO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direcciones\n",
    "-----------\n",
    "\n",
    "¿Como puede hacer para mejorar la performance del modelo original que creamos en clase? Explore diferentes alternativas que lo llevarán a una mejor performance. En particular:\n",
    "\n",
    "- Remplazar la capa LSTM por una capa de tipo bidireccional. ¿Mejora?\n",
    "- ¿Que sucede con el pre-procesamiento? ¿Serviría modificar algo?\n",
    "    - Pista: Explore los parámteros de TweetNormalizer\n",
    "    \n",
    "Haga las modificaciones que crea pertinente y revise que propuestas mejoran la performance. Utilice la siguiente estructura de solución como ayuda, pero sientase libre de explorar otra.\n",
    "\n",
    "> **Importante:** No es necesario realizar tuneo de hiper-parametros para resolver este ejercicio, solo utilice su intuición para introducir modificaciones que deberían de llevarlo a un mejor resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwRwMqoSditT"
   },
   "source": [
    "### Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jnuzl8qbditU"
   },
   "outputs": [],
   "source": [
    "from m72109.nlp.normalization import TweetTextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFYzMdiCditX"
   },
   "outputs": [],
   "source": [
    "normalizer = TweetTextNormalizer(preserve_case=False, return_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdoLyHg9ditd"
   },
   "source": [
    "### Vectorización de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jE-SlBWddite"
   },
   "outputs": [],
   "source": [
    "from m72109.nlp.transformation import Word2VecVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "vkWxVLPYditg",
    "outputId": "50655397-0e29-455d-b6db-3d3454968785"
   },
   "outputs": [],
   "source": [
    "w2v = Word2VecVectorizer(model_path='Models/Word2Vec/model-es.bin', sequence_to_idx=True)\n",
    "embedding_weights = w2v.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBNY37iIditm"
   },
   "source": [
    "### Construirmos un modelo basado en secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhfhArdbdit5"
   },
   "source": [
    "#### Ajustando la longitud de las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeVilfLMdit7"
   },
   "outputs": [],
   "source": [
    "from m72109.nlp.transformation import PadSequenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-2T5o8Rdit9"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpfZiKM_dit_"
   },
   "outputs": [],
   "source": [
    "seq2seq = PadSequenceTransformer(max_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construyendo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ2C980jditp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNrawZnAditr"
   },
   "outputs": [],
   "source": [
    "def build_model(sequence_len, vocab_size, emdedding_size, embedding_weights):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, emdedding_size,\n",
    "                  weights=[embedding_weights],\n",
    "                  trainable=False,\n",
    "                  mask_zero=True),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(emdedding_size),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8hTWi3tdit0"
   },
   "outputs": [],
   "source": [
    "estimator = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=build_model, \n",
    "    epochs=50,\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=w2v.vocab_size,\n",
    "    emdedding_size=w2v.emdedding_size,\n",
    "    embedding_weights=embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpwiD4GGdiuB"
   },
   "source": [
    "### Creando nuestro pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0V5zF-wdiuB"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
    "                           ('vectorizer', w2v),\n",
    "                           ('padder', seq2seq),\n",
    "                           ('estimator', estimator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sJYPslBwdiuE",
    "outputId": "3c5e3f32-ec0c-4432-d03a-fe9b45fc66dc"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3DtoSNTdiuJ"
   },
   "source": [
    "### Evalución de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFmZCv5KdiuK"
   },
   "source": [
    "Probamos su performance utilizando el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "kqM4G2BVdiuL",
    "outputId": "6882201a-67a9-4f31-b33b-4aa1bf816380"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "XSIlnu1zdiuN",
    "outputId": "1a80c583-9afc-434c-eedf-33f36c0846cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       0.96      0.85      0.90       110\n",
      "  AUTOMOCION       0.87      0.78      0.82       148\n",
      "       BANCA       0.84      0.82      0.83       198\n",
      "     BEBIDAS       0.88      0.89      0.88       223\n",
      "    DEPORTES       0.76      0.85      0.80       216\n",
      "      RETAIL       0.79      0.80      0.80       268\n",
      "       TELCO       0.95      0.94      0.94        79\n",
      "\n",
      "    accuracy                           0.84      1242\n",
      "   macro avg       0.86      0.85      0.85      1242\n",
      "weighted avg       0.84      0.84      0.84      1242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word2Vec - Secuencias.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
