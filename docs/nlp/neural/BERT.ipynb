{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpgOtduwsIty"
   },
   "source": [
    "BERT: Bidirectional Encoder Representations from Transformers\n",
    "============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducción\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec es un modelo de tipo \"context-free\", lo que significa que cada palabra recibe un único vector que la representa. Esto implica que por ejemplo la palabra \"banco\" recibirá la misma representación en las oraciones *Los domingos no abre el banco* y *Estabamos tan cansados que nos sentamos en un banco*.\n",
    "\n",
    "BERT, sin embargo, es un modelo contextual, lo que significa que la representación que se genera de una palabra depende del contexto en el que aparece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pwKjjVLXsIuO"
   },
   "source": [
    "### Como funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3zWLYU0sIuP"
   },
   "source": [
    "Si recordamos de cuando introducimos word2vec, vimos que las representaciones de las palabras se obtenian al entrenar una red neuronal en una tarea \"falsa\" que era predecir una palabra dado el contexto en el que aparece. Este contexto lo especificabamos como una ventana de palabras. Los modelos basados en lenguaje, llevan esta tarea un paso más adelante y tratan de predecir la siguiente palabra dada una secuencia de tokens.\n",
    "\n",
    "En el caso de BERT, está pre-entrenado utilizando 2 tareas distintas:\n",
    "\n",
    " - **Masked LM:** BERT está basado en una técnica llamada Masked LM (MLM) la cual, en lugar de intentar predecir la siguiente palabra dada una secuencia de palabras, aleatoriamente enmascara palabras en la oración para luego intentar predecirlas desde el contexto. Para hacerlo utiliza el contexto completo de la oración, tanto hacia adelante como hacía atras (por esto se llama bidireccional). En practica, BERT enmascara aproximadamente el 15% de los tokens en una secuencia.\n",
    " - **NSP (Next Sentence Prediction):** Muchas tareas en NLP requieren el entendimiento de las relaciones entre varias oraciones o secuencias. BERT captura estas relaciones al estar entrenado para predecir la siguiente oración en el cuerpo. En realidad BERT utiliza 50% del tiempo efectivamente la siguiente oración para la tarea de NSP y la taguea con el token IsNext, mientras que el otro 50% utiliza una oración aleatoria del texto y la taguea con el token NoNext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dcyc_TQ6dis7"
   },
   "source": [
    "### Para ejecutar este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este notebook, instale las siguientes librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv \\\n",
    "    --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
    "    \n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/docs/nlp/neural/BERT.txt \\\n",
    "    --quiet --no-clobber\n",
    "!pip install -r BERT.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ntcs1AlpfckX"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CC7vGpjqditL"
   },
   "source": [
    "Cargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmJpenUkditM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ailv1y8usIuQ"
   },
   "source": [
    "## Explorando un modelo pre-entrenado con BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAWVIpOasIuR"
   },
   "source": [
    "Una de las formas más sencillas de trabajar con el modelo BERT es utilizando la libreria transformers de [Hugging Face](https://huggingface.co/) la cual ofrece una forma muy conveniente de acceder a modelos de NLP en diferentes lenguajes e incluso entrenados para tareas especificas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-G9VhuAosIuS"
   },
   "source": [
    "Podemos instalar esta libreria desde pip de la siguiente forma. Este paso ya lo realizamos en la sección de instalación de este notebook\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "> Esta librería ya fué instalada en la preparación del notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxeO0eUSsIuT"
   },
   "source": [
    "### BETO: BERT en español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdLkgbBPsIuU"
   },
   "source": [
    "Al igual que con word2vec, entrenar un modelo de lenguaje requiere de una gran cantidad de datos sumado a un poder de computo interesante (cuando BERT fué publicado en 2018, tomó 4 días entrenar el modelo usando 16 TPUs. Si se hubiera entrenado en 8 GPUs hubiera tomado entre 40–70 días).Por este motivo, utilizaremos un modelo pre-entrenado para un cuerpo de texto en español. Este modelo, BETO, fué entrenado sobre un gran corpora de texto. Pueden encontrar más información sobre el autor de este modelo en: https://github.com/dccuchile/beto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTDeAPaysIuV"
   },
   "source": [
    "### Tokenizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTn2Z7gssIuW"
   },
   "source": [
    "BERT utiliza su propio tokenizer que está basado en WordPiece. Este tokenizer tiene un vocabulario de 30.000 tokens donde cada secuencia comienza con un token especial [CLS]. Exploremos como funciona este tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxWFx0HosIuY"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIscXqOpsIue"
   },
   "source": [
    "*Noten que el tokenizer depende del modelo que estamos utilizando*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afpvu4chsIuf"
   },
   "source": [
    "Exploremos los tokens que genera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9iDb340sIug"
   },
   "outputs": [],
   "source": [
    "text = tweets['TEXTO'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmN6b_tJsIun"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLG_OhQDsIup"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGPYvhKAsIut"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 1008,\n",
       " 985,\n",
       " 14666,\n",
       " 1114,\n",
       " 5232,\n",
       " 30958,\n",
       " 985,\n",
       " 9419,\n",
       " 1211,\n",
       " 1123,\n",
       " 5232,\n",
       " 30958,\n",
       " 985,\n",
       " 1457,\n",
       " 1049,\n",
       " 5232,\n",
       " 30958,\n",
       " 1665,\n",
       " 4615,\n",
       " 1013,\n",
       " 989,\n",
       " 10552,\n",
       " 3379,\n",
       " 2689,\n",
       " 2915,\n",
       " 1316,\n",
       " 17400,\n",
       " 24981,\n",
       " 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wH2F4qrHsIuz"
   },
   "source": [
    "¿Notan algo raro en los tokens generados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuQwdQk5sIu1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]. @ policiadeburgos @ pcivilburgos @ aytoburgos mismo peligro c / rio viejo junto mercadona villimar [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQwFFhsXsIu3"
   },
   "source": [
    "¿Siguen notando algo raro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TruYQtvsIu3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '.',\n",
       " '@',\n",
       " 'policia',\n",
       " '##de',\n",
       " '##burgo',\n",
       " '##s',\n",
       " '@',\n",
       " 'pc',\n",
       " '##iv',\n",
       " '##il',\n",
       " '##burgo',\n",
       " '##s',\n",
       " '@',\n",
       " 'ay',\n",
       " '##to',\n",
       " '##burgo',\n",
       " '##s',\n",
       " 'mismo',\n",
       " 'peligro',\n",
       " 'c',\n",
       " '/',\n",
       " 'rio',\n",
       " 'viejo',\n",
       " 'junto',\n",
       " 'mercado',\n",
       " '##na',\n",
       " 'vill',\n",
       " '##imar',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.convert_ids_to_tokens(idx) for idx in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8W01ColsIu6"
   },
   "source": [
    "### Cargando nuestro modelo de BERT para español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xifxnZc5sIu6"
   },
   "source": [
    "Para cargar nuestro modelo, utilizaremos la libreria transformers nuevamente, utilizando el método from_pretrained. Este método descargará automaticamente el modelo desde el directorio de modelos de HuggingFace. Pueden ver el listado de modelos que están disponibles en este directorio en [https://huggingface.co/models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUKFFSmAsIu7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062e23cdbac04a3cb240635f43bce197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.BertForMaskedLM.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>El parámetro return_dict=True hará que el modelo retorne un diccionario con los resultados en lugar de un tupla. Esto solo hará que sea más sencillo interpretar los resultados.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-gb7c9KsIu-"
   },
   "source": [
    "Veamos como se comporta nuestro modelo en la tarea de predecir una palabra de un texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8Vgq8GesIvB"
   },
   "outputs": [],
   "source": [
    "text = \"[CLS] Cuando [MASK] contaron lo que sucedia nos quedamos helados. [SEP]\"\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MOk8vhpsIvD"
   },
   "source": [
    "Noten que el token [MASK] es la palabra que estamos intentando predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuxmDIWbsIvD"
   },
   "source": [
    "Necesitamos saber cual de todos los tokens que generamos es exactamente el que enmascaramos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vXP0940sIvE"
   },
   "outputs": [],
   "source": [
    "masked_indxs = [idx for idx in range(0, len(tokens)) if tokens[idx] == '[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jW9CXBpssIvG"
   },
   "source": [
    "Corremos nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlBcKmvgsIvG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "predictions = model(tokens_tensor).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>**Nota:** Ejecutar el modelo sobre nuestro texto devolverá diferente cantidad de objetos dependiendo de como se configuró la carga del modelo en el método `from_pretrained`. En nuestro caso, hemos especificado `return_dic=True` y por lo tanto la salida del médoto es un diccionario con los resultados (si no lo hubieramos especificado hubiera retornado una tupla). Dentro de lo que nos interesa a nosotros está:</i>\n",
    " - <i>**logits:** Retorna la secuencia de hidden-states en la última capa del modelo. Esto tiene tamaño (batch_size, sequence_length, hidden_size).</i>\n",
    " - <i>**hidden_states:** Retorna los hidden-states de todas las capas del modelo. 12 en el caso de BERT. Esta información solo se retorna cuando se indica `output_hidden_states=True`</i>\n",
    "\n",
    "<i>Para más información sobre que objetos se retornan de la ejecución del modelo pueden ver la documentación del modelo BERT: https://huggingface.co/transformers/model_doc/bert.html#bertmodel</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaHu-uY5sIvI"
   },
   "source": [
    "Verificamos cuales son las palabras más probables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2q1fyi2sIvI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 5 palabras más probables para la mascara 0 son: ['nos', 'me', 'les', 'le', 'supi']\n"
     ]
    }
   ],
   "source": [
    "for i,midx in enumerate(masked_indxs):\n",
    "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
    "    print('Las 5 palabras más probables para la mascara',i,'son:',predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LKh3A6_sIvL"
   },
   "source": [
    "## Explorando las representaciones de BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección exploraremos las representaciones que genera BERT. Como se mencionó, BERT generá representaciones que son dependientes del contexto, algo que lo diferencia de Word2Vec. Esto implica que por ejemplo la palabra \"banco\" recibirá la misma representación en las oraciones *Los domingos no abre el banco y salimos a caminar. Estabamos tan cansados que nos sentamos en un banco a ver gente pasar*. Veamos si esto es así como mencionamos explorando estas representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer esto, volveremos a cargar el modelo, ahora especificando el parametro `output_hidden_states=True`. Esto hará que el modelo retorne las representaciones de todos los estados intermedios como parte de la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertForMaskedLM.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', \n",
    "                                                     return_dict=True, \n",
    "                                                     output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Queriamos retirar dinero del banco. Sin embargo, los domingos el banco no está abierto. Estabamos tan cansados que nos sentamos en un banco a ver gente pasar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens]) #Al igual que antes, siempre convertimos primero el input en un tensor\n",
    "\n",
    "hidden_states = model(tokens_tensor).hidden_states\n",
    "token_embeddings = torch.stack(hidden_states, dim=0) #hidden_states retorna una lista, transformemos esto en un tensor :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las dimensiones de este objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 33, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que significan?\n",
    " - 13 es la cantidad de capas dentro del modelo. Si bien BERT posee 12 capas, la capa numero 0 corresponde a los inputs (entradas) del modelo y por eso vemos 13 capas finalmente\n",
    " - 1 es la cantidad de muestra en el lote (ie. batch size)\n",
    " - 33 es la cantidad de tokens que se inputaron al modelo\n",
    " - 768 es la cantidad de unidades de la red neuronal (units) en cada capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nuestro lote/batch solo contiene una oración, podemos deshacernos de la segunda dimensión de este tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 33, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para facilitar el entendimiento de la salida, vamos a cambiar el orden de los valores de este tensor. Recordaran que actualmente tenemos un tensor del tamaño (numero_de_capas, numero_de_tokens, features). Para el análisis que queremos realizar, sería mucho más interesante tener algo del tipo (numero_de_tokens, numero_de_capas, features). De esta forma podriamos revisar todas las representaciones de cada token más facilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 13, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiamos las dimensiones 0 y 1\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Entonces cuales son las representaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las representaciones son tensores del tamaño (12,768). Claramente esto es un gran progreso desde las 100 dimensiones en Word2Vec! Veamos si efectivamente las representaciones de banco son diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 queria\n",
      "2 ##mos\n",
      "3 retirar\n",
      "4 dinero\n",
      "5 del\n",
      "6 banco\n",
      "7 .\n",
      "8 sin\n",
      "9 embargo\n",
      "10 ,\n",
      "11 los\n",
      "12 domingos\n",
      "13 el\n",
      "14 banco\n",
      "15 no\n",
      "16 está\n",
      "17 abierto\n",
      "18 .\n",
      "19 estabamos\n",
      "20 tan\n",
      "21 cansados\n",
      "22 que\n",
      "23 nos\n",
      "24 sentamos\n",
      "25 en\n",
      "26 un\n",
      "27 banco\n",
      "28 a\n",
      "29 ver\n",
      "30 gente\n",
      "31 pasar\n",
      "32 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(indexed_tokens):\n",
    "  print (i, tokenizer.convert_ids_to_tokens(token_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo tenemos la palabra **banco** en las posiciones 6, 14 y 27. De estas posiciones, la palabra **banco** en las posiciones 6 y 14 debería de tener un significado similar, mientra que su versión en la posición 27 debería ser distinta. Para medir esto necesitariamos comparar la similaridad de estas representaciones. Para esto podríamos utilizar una métrica como la Similaridad de coseno (cosine similarity), sin embargo necesitamos contar con un vector unidimensional. Si bien no hay una única forma de resolver este problema, una forma podría ser tomar el promedio de los valores a lo largo de las 13 capas para generar un vector unidimensional por cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(token_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computemos ahora las similaridad de las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(...) retirar dinero del **banco**. Sin embargo, los domingos el **banco** no está abierto (...)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8966357111930847"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding[6].detach().numpy(), sentence_embedding[14].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(...) retirar dinero del **banco** (...) nos sentamos en un **banco** a (...)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7544149160385132"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding[6].detach().numpy(), sentence_embedding[27].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la similaridad entre cada una de las representaciones es distinta. Tengan en cuenta que este método no es exacto, pero de alguna forma nos da una idea y una intuición de que tan cercanas pueden ser estas representaciones. BERT es un modelo donde las representaciones dependen del contexto y por lo tanto el concepto de \"similaridad\" aquí es distinto e incluso podría carecer un poco de sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De los autores de BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autores de BERT proponen una solución un poco distinta, en base a diferentes experimentos que realizaron. Una de las formas que generaron las representaciones con los mejores resultados resultó de concatenar los valores de las últimas 4 capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"600\" src='http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo interesante que decanta de este ultimo experimento es que claramente cada uno de las diferentes capas dentro de BERT codifican diferentes aspectos de las palabras y que por lo tanto, la estrategia que mejor se adapta para generar estas representaciones más compactas depende mucho de la tarea que se esté realizando. A continuación intentaremos aplicarlo (noten que en las lineas subsiguientes habrá bastante manipulación de las formas de los vectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con las ultimas capas de cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_ = token_embeddings.narrow(1, 9, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 4, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos los valores de las últimas 4 capas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor = slice_.reshape(33, 4*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 3072])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la similaridad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337031006813049"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(concatenated_tensor[6].detach().numpy(), concatenated_tensor[14].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6618704795837402"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(concatenated_tensor[6].detach().numpy(), concatenated_tensor[27].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
