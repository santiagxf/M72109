{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ME72: Maestría en Métodos Cuantitativos para la Gestión y Análisis de Datos\n",
    "M72109: Analisis de datos no estructurados\n",
    "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
    "Año: 2020\n",
    "Profesor: Facundo Santiago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BERT: Fine-tunning para modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook, intentaremos resolver el problema de clasificación de tweets con el que venimos trabajando pero aplicando un modelo basado en la arquitectura de BERT y entrenado para la tarea especifica que queremos resolver utilizando el concepto de Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarguemos algunos fragmentos de código para simplificar el trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/ClassificationDataset.py --directory-prefix ./Utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --directory-prefix ./Datasets/mascorpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, existen 2 estrategias para utilizar modelos de lenguaje pre-entrenados en una tarea especifica:\n",
    " - Feature-based\n",
    " - Fine-tunning\n",
    " \n",
    "Las técnicas que se conocen como **Feature-based** utiliza arquitecturas especificas para resolver cada una de las tareas de NLP, en donde los pesos de las representaciones vectoriales están \"congeladas\" y no son parámetros que el modelo deba optimizar. En consecuencia, estos modelos son más rápidos de entrenar y permiten aplicar arquitecturas especificas que sean diferenciales en cada una de las tareas.\n",
    " \n",
    "Por el otro lado, las técnicas que emplean **Fine-tunning** tiene la flexibilidad de poder adaptar sus representaciones al permitir que todos los parametros sean optimizados en la tarea en particular. Además, estas arquitecturas permiten resolver multiples problemas de NLP utilizando una mínima cantidad de parametros específicos para la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BETO: BERT en español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con word2vec, entrenar un modelo de lenguaje requiere de una gran cantidad de datos sumado a un poder de computo interesante (cuando BERT fué publicado en 2018, tomó 4 días entrenar el modelo usando 16 TPUs. Si se hubiera entrenado en 8 GPUs hubiera tomado entre 40–70 días).Por este motivo, utilizaremos un modelo pre-entrenado para un cuerpo de texto en español. Este modelo, BETO, fué entrenado sobre un gran corpora de texto. Pueden encontrar más información sobre el autor de este modelo en: https://github.com/dccuchile/beto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT utiliza su propio tokenizer que está basado en WordPiece. Este tokenizer tiene un vocabulario de 30.000 tokens donde cada secuencia comienza con un token especial [CLS]. Recuerden que los tokenizers dependen del modelo con el que estamos trabajando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Noten que el tokenizer depende del modelo que estamos utilizando*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crando un modelo de clasificación basado en BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trataremos de resolver entonces el mismo problema de clasificación con el que veniamos trabajando: clasificar los tweets dependiendo del sector al que pertenecen.Recordemos que tenemos 7 categorias distintas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RETAIL', 'TELCO', 'ALIMENTACION', 'AUTOMOCION', 'BANCA',\n",
       "       'BEBIDAS', 'DEPORTES'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['SECTOR'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos contar con el numero de categorias para nuestro clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(tweets['SECTOR'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de hacer fine-tunning de nuestro modelo, tenemos que instanciar el modelo sobre el cual queremos aplicar esta técnica. Para ello instanciaremos el modelo base el cual no está entrenado en ninguna tarea en particular. De hecho, si habilitan las alertas en este notebook, verán que cuando se carga el modelo, la libreria HuggingFace les advierte sobre esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construiremos nuestro dataset sobre el que queremos entrenar el modelo. Recuerden que ya habíamos separado el set de datos en porciones para entrenar y para testear el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como entrenar modelos con Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería transformers puede entrenar modelos tanto utilizando TensorFlow como PyTorch como backend. En nuestro caso utilizaremos PyTorch simplemente porque generaremos código un poco más compacto, pero pueden utilizar el backend con el que más cómodos se sientan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar el objeto Trainer que provee transformers, necesitamos crear un objetivo de tipo Dataset. PyTorch implementa este objeto el modulo torch.utils.data.Dataset. Para simplificar la tarea, les he creado una clase que hace todo el procesamiento de datos y generación de los sets de datos utilizando dicho modulo. Pueden encontrar esta implementación en Utils.ClassificationDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.ClassificationDataset import ClassificationDataset\n",
    "\n",
    "train_dataset = ClassificationDataset(examples=X_train, labels=y_train, tokenizer=tokenizer)\n",
    "val_dataset = ClassificationDataset(examples=X_test, labels=y_test, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especificamos los parametros con los que entrenaremos nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directorio de trabajo del Trainer\n",
    "    num_train_epochs=3,              # Numero total de epochs sobre el que entrenaremos\n",
    "    per_device_train_batch_size=16,  # Tamaño del batch de datos por cada dispositivo de entrenamiento\n",
    "    per_device_eval_batch_size=64,   # Tamaño del batch de datos que usaremos para evaluación\n",
    "    warmup_steps=500,                # Numero de pasos que se usaran para determinar la politica de Learning Rate\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    logging_dir='./logs',            # Directorio de logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # modelo sobre el que haremos fine tunning\n",
    "    args=training_args,                  # parametros del entrenamiento\n",
    "    train_dataset=train_dataset,         # set de datos de entrenamiento\n",
    "    eval_dataset=val_dataset             # set de datos de evaluación\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cea2ab7dcf24a6e9602aff931757b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee3aa56bec46bba7dea55795efe448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=158.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.0130661964416503, \"learning_rate\": 1.0000000000000002e-06, \"epoch\": 0.06329113924050633, \"step\": 10}\n",
      "{\"loss\": 2.0143117666244508, \"learning_rate\": 2.0000000000000003e-06, \"epoch\": 0.12658227848101267, \"step\": 20}\n",
      "{\"loss\": 1.9196344375610352, \"learning_rate\": 3e-06, \"epoch\": 0.189873417721519, \"step\": 30}\n",
      "{\"loss\": 1.9248562574386596, \"learning_rate\": 4.000000000000001e-06, \"epoch\": 0.25316455696202533, \"step\": 40}\n",
      "{\"loss\": 1.8505674958229066, \"learning_rate\": 5e-06, \"epoch\": 0.31645569620253167, \"step\": 50}\n",
      "{\"loss\": 1.8715176820755004, \"learning_rate\": 6e-06, \"epoch\": 0.379746835443038, \"step\": 60}\n",
      "{\"loss\": 1.7870514273643494, \"learning_rate\": 7.000000000000001e-06, \"epoch\": 0.4430379746835443, \"step\": 70}\n",
      "{\"loss\": 1.78349130153656, \"learning_rate\": 8.000000000000001e-06, \"epoch\": 0.5063291139240507, \"step\": 80}\n",
      "{\"loss\": 1.7086456418037415, \"learning_rate\": 9e-06, \"epoch\": 0.569620253164557, \"step\": 90}\n",
      "{\"loss\": 1.5932150483131409, \"learning_rate\": 1e-05, \"epoch\": 0.6329113924050633, \"step\": 100}\n",
      "{\"loss\": 1.5246841192245484, \"learning_rate\": 1.1000000000000001e-05, \"epoch\": 0.6962025316455697, \"step\": 110}\n",
      "{\"loss\": 1.404003095626831, \"learning_rate\": 1.2e-05, \"epoch\": 0.759493670886076, \"step\": 120}\n",
      "{\"loss\": 1.0961741983890534, \"learning_rate\": 1.3000000000000001e-05, \"epoch\": 0.8227848101265823, \"step\": 130}\n",
      "{\"loss\": 1.0059254884719848, \"learning_rate\": 1.4000000000000001e-05, \"epoch\": 0.8860759493670886, \"step\": 140}\n",
      "{\"loss\": 0.7816226780414581, \"learning_rate\": 1.5e-05, \"epoch\": 0.9493670886075949, \"step\": 150}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82d008277ba42869b6ec688d3907f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=158.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.5704751312732697, \"learning_rate\": 1.6000000000000003e-05, \"epoch\": 1.0126582278481013, \"step\": 160}\n",
      "{\"loss\": 0.5037536889314651, \"learning_rate\": 1.7000000000000003e-05, \"epoch\": 1.0759493670886076, \"step\": 170}\n",
      "{\"loss\": 0.334173247218132, \"learning_rate\": 1.8e-05, \"epoch\": 1.139240506329114, \"step\": 180}\n",
      "{\"loss\": 0.28362494260072707, \"learning_rate\": 1.9e-05, \"epoch\": 1.2025316455696202, \"step\": 190}\n",
      "{\"loss\": 0.2360175520181656, \"learning_rate\": 2e-05, \"epoch\": 1.2658227848101267, \"step\": 200}\n",
      "{\"loss\": 0.15980467796325684, \"learning_rate\": 2.1e-05, \"epoch\": 1.3291139240506329, \"step\": 210}\n",
      "{\"loss\": 0.09582721889019012, \"learning_rate\": 2.2000000000000003e-05, \"epoch\": 1.3924050632911391, \"step\": 220}\n",
      "{\"loss\": 0.19044460952281952, \"learning_rate\": 2.3000000000000003e-05, \"epoch\": 1.4556962025316456, \"step\": 230}\n",
      "{\"loss\": 0.10874612033367156, \"learning_rate\": 2.4e-05, \"epoch\": 1.518987341772152, \"step\": 240}\n",
      "{\"loss\": 0.11367769986391067, \"learning_rate\": 2.5e-05, \"epoch\": 1.5822784810126582, \"step\": 250}\n",
      "{\"loss\": 0.059561118483543396, \"learning_rate\": 2.6000000000000002e-05, \"epoch\": 1.6455696202531644, \"step\": 260}\n",
      "{\"loss\": 0.10623660385608673, \"learning_rate\": 2.7000000000000002e-05, \"epoch\": 1.7088607594936709, \"step\": 270}\n",
      "{\"loss\": 0.04469514265656471, \"learning_rate\": 2.8000000000000003e-05, \"epoch\": 1.7721518987341773, \"step\": 280}\n",
      "{\"loss\": 0.06593715846538543, \"learning_rate\": 2.9e-05, \"epoch\": 1.8354430379746836, \"step\": 290}\n",
      "{\"loss\": 0.06079080849885941, \"learning_rate\": 3e-05, \"epoch\": 1.8987341772151898, \"step\": 300}\n",
      "{\"loss\": 0.12163993120193481, \"learning_rate\": 3.1e-05, \"epoch\": 1.9620253164556962, \"step\": 310}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6cf09bfbdf4523a7be652f11106376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=158.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0333705710247159, \"learning_rate\": 3.2000000000000005e-05, \"epoch\": 2.0253164556962027, \"step\": 320}\n",
      "{\"loss\": 0.019112420082092286, \"learning_rate\": 3.3e-05, \"epoch\": 2.088607594936709, \"step\": 330}\n",
      "{\"loss\": 0.04272092580795288, \"learning_rate\": 3.4000000000000007e-05, \"epoch\": 2.151898734177215, \"step\": 340}\n",
      "{\"loss\": 0.007846251130104065, \"learning_rate\": 3.5e-05, \"epoch\": 2.2151898734177213, \"step\": 350}\n",
      "{\"loss\": 0.01155136078596115, \"learning_rate\": 3.6e-05, \"epoch\": 2.278481012658228, \"step\": 360}\n",
      "{\"loss\": 0.049505618214607236, \"learning_rate\": 3.7e-05, \"epoch\": 2.3417721518987342, \"step\": 370}\n",
      "{\"loss\": 0.043567579984664914, \"learning_rate\": 3.8e-05, \"epoch\": 2.4050632911392404, \"step\": 380}\n",
      "{\"loss\": 0.029395869374275206, \"learning_rate\": 3.9000000000000006e-05, \"epoch\": 2.4683544303797467, \"step\": 390}\n",
      "{\"loss\": 0.049667558073997496, \"learning_rate\": 4e-05, \"epoch\": 2.5316455696202533, \"step\": 400}\n",
      "{\"loss\": 0.01619624048471451, \"learning_rate\": 4.1e-05, \"epoch\": 2.5949367088607596, \"step\": 410}\n",
      "{\"loss\": 0.033603560924530027, \"learning_rate\": 4.2e-05, \"epoch\": 2.6582278481012658, \"step\": 420}\n",
      "{\"loss\": 0.002587515115737915, \"learning_rate\": 4.3e-05, \"epoch\": 2.721518987341772, \"step\": 430}\n",
      "{\"loss\": 0.07240344136953354, \"learning_rate\": 4.4000000000000006e-05, \"epoch\": 2.7848101265822782, \"step\": 440}\n",
      "{\"loss\": 0.04187065213918686, \"learning_rate\": 4.5e-05, \"epoch\": 2.848101265822785, \"step\": 450}\n",
      "{\"loss\": 0.09450587257742882, \"learning_rate\": 4.600000000000001e-05, \"epoch\": 2.911392405063291, \"step\": 460}\n",
      "{\"loss\": 0.08734026849269867, \"learning_rate\": 4.7e-05, \"epoch\": 2.9746835443037973, \"step\": 470}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=474, training_loss=0.5901083445023226)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 21:08:55.512290: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-09-09 21:08:55.512390: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-09-09 21:08:55.512411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "TensorBoard 2.3.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiquemos la performance de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4e05cf2c7a42be8f2831872620aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Prediction', max=20.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=val_dataset).predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el modelo, primero deberemos obtener cual es la categoria que obtuvo la mayor probabilidad en la clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los IDs de las categorias a los labels correctos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = val_dataset.get_labels()\n",
    "predictions_label = [all_labels[idx] for idx in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       1.00      0.95      0.97       110\n",
      "  AUTOMOCION       0.99      1.00      1.00       148\n",
      "       BANCA       0.99      0.99      0.99       198\n",
      "     BEBIDAS       0.99      1.00      0.99       223\n",
      "    DEPORTES       1.00      0.99      1.00       216\n",
      "      RETAIL       0.98      1.00      0.99       268\n",
      "       TELCO       0.99      0.99      0.99        79\n",
      "\n",
      "    accuracy                           0.99      1242\n",
      "   macro avg       0.99      0.99      0.99      1242\n",
      "weighted avg       0.99      0.99      0.99      1242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (nlp-py37)",
   "language": "python",
   "name": "nlp-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
