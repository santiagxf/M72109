{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ME72: Maestría en Métodos Cuantitativos para la Gestión y Análisis de Datos\n",
    "M72109: Analisis de Datos no Estructurados\n",
    "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
    "Año: 2020\n",
    "Profesor: Facundo Santiago, Javier Ignacio Garcia Fronti\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración de un modelo entrenado con Word2Vec para el idioma español\n",
    "\n",
    "En esta sección, exploraremos las cualidades de las representaciones vectoriales generadas por el algoritmo Word2Vec para el idioma español. Este notebook requiere tener instaladas las siguientes librerias:\n",
    "- gensim\n",
    "- numpy\n",
    "- tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, io, pathlib\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos un modelo pre-entrenado en Español\n",
    "\n",
    "Utilizaremos un modelo pre-entrenado de Word2Vec que fué entrenado con un corpus en español (Spanish CoNLL17 corpus). Este modelo se entrenó trabajando con una ventana de 10 palabras, con el algoritmo Continuous bag of words y un negative-sampling de 5 palabras. Este modelo está almacenado en formato binario, por lo que lo cargamos utilizando el objeto KeyedVectors y el metodo load_word2vec_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./Models/Word2Vec\n",
    "!wget -N https://santiagxf.blob.core.windows.net/public/Word2Vec/model.bin --directory-prefix ./Models/Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencia: NLPL Word Vectors Repository (http://vectors.nlpl.eu/repository/), version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el embedding (este embedding es bastante grande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('Models/Word2Vec/model.bin')\n",
    "embeddings = KeyedVectors.load_word2vec_format(datapath(path), binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos objetener algunas propiedades del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vocabulario es: 2656057\n",
      "El tamaño de los vectores es: 100\n"
     ]
    }
   ],
   "source": [
    "print (\"El tamaño del vocabulario es:\", len(embeddings.vocab))\n",
    "print(\"El tamaño de los vectores es:\", embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploramos las representaciones\n",
    "Podemos comenzar a indagar como lucen las representaciones vectoriales que se aprendieron utilizando el método most_similar. Este método nos devuelve las 10 primeras palabras más similares a la palabra que indicamos. La similaridad se computa utilizando cosine-similarity de las representaciones vectoriales de cada una de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emperatriz', 0.7729766964912415),\n",
       " ('princesa', 0.7713199257850647),\n",
       " ('sofía', 0.7699436545372009),\n",
       " ('coronaciónla', 0.7649693489074707),\n",
       " ('sofíala', 0.7604084014892578),\n",
       " ('virreina', 0.7558672428131104),\n",
       " ('categoría:talavera', 0.7512167692184448),\n",
       " ('isabel', 0.7497586011886597),\n",
       " ('sofíael', 0.735364556312561),\n",
       " ('emperatríz', 0.7345834970474243)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(\"reina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos consultar cuales son las palabras más disimilares a una determinada palabra. Esto lo hacemos especificando el parametro negative dentro del método most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86222880', 0.37661516666412354),\n",
       " ('cze-', 0.3465925455093384),\n",
       " ('episodionumero=21', 0.32355785369873047),\n",
       " ('confederaciónuefa', 0.32355785369873047),\n",
       " ('borderisrael', 0.3164479732513428),\n",
       " ('categoría:syntermitinae', 0.3074568510055542),\n",
       " ('categoría:socialdemocracia', 0.2885240316390991),\n",
       " ('03.05.82', 0.2859065532684326),\n",
       " ('categoría:lujanenses', 0.28335657715797424),\n",
       " ('hansa-sur', 0.2776678204536438)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(negative=\"reina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método anterior no es muy util en general, sin embargo nos permite introducir el concepto de aritmética sobre estos mismos vectores. Por ejemplo: Pordiamos consultar cual es la analogia de dos palabras. El siguiente ejemplo se leería así: A lo que Paris es a Francia, ¿cúal es la analogia de Madrid?. Esto se resolveria tomando \"Francia\", quitandole \"Paris\" y agregandole \"Madrid\"\n",
    "\n",
    "<img src=\"Docs/word2vec-math.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'españa'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"francia\", \"madrid\"], negative=[\"paris\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos nos dice que quizás este espacio vectorial no es solamente un espacio donde vectores que están cerca los unos de los otros en el espacio tienen significados similares, sino que en realidad capturan el significado en una forma más produnda. Concretamente, que hay \"direcciones de significados\" en el espacio donde uno de puede mover. Veamos algunos ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la dirección de \"hacedor\"\n",
    " - Conducir es a conductor lo que limpiar es a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trapeador'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"conductor\", \"limpiar\"], negative=[\"conducir\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la dirección \"bebidas\"\n",
    " - El vino es a Francia lo que el whisky es a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bélgica'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"francia\", \"whisky\"], negative=[\"vino\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la dirección \"extremar\"\n",
    " - Bueno es a genial lo que malo es a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'malísimo'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=[\"genial\", \"malo\"], negative=[\"bueno\"])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del espacio contino de word2vec\n",
    "\n",
    "El siguiente código utiliza un plugin de Tensorflow llamado Tensorboard que permite hacer una proyección en un espacio 3D (para luego hacer una proyección en un espacio 2D - el de la pantalla) de las representaciones de cada una de las palabras que están en el vocabulario. Esta visualización requiere tener instalado TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante:** Actualmente hay un problema con Google Colaboratory con la carga de Projector en Tensorboard. Al parecer es debido al modulo tfjs. Si quieren probar esta funcionalidad tendrán que ejecutar este codigo en su equipo local. Para hacer esto deberan descargar este notebook en su equipo con Python instalado y las librerias mensionadas en la parte superior. Adicionalmente:\n",
    "\n",
    "!pip install tensorboard\n",
    "!tensorboard --logdir ./logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = '/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p '/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    weights = tf.Variable(embeddings.vectors, )\n",
    "    checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "    checkpoint.save(os.path.join(logdir, 'embedding.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente codigo genera los \"labels\" asociados al espacio vectorial que cargamos anteriormente. Pueden descargar estos labels ya preprocesados desde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-03 21:17:23--  https://santiagxf.blob.core.windows.net/public/Word2Vec/meta.tsv\n",
      "Resolving santiagxf.blob.core.windows.net (santiagxf.blob.core.windows.net)... 52.239.220.32\n",
      "Connecting to santiagxf.blob.core.windows.net (santiagxf.blob.core.windows.net)|52.239.220.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 930085 (908K) [text/tab-separated-values]\n",
      "Saving to: ‘/tmp/logdir/meta.tsv’\n",
      "\n",
      "meta.tsv            100%[===================>] 908.29K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2020-09-03 21:17:23 (99.0 MB/s) - ‘/tmp/logdir/meta.tsv’ saved [930085/930085]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/meta.tsv --directory-prefix '/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(logdir + '/meta.tsv', 'w', encoding='utf-8') as metadata_file:\n",
    "    with io.open(logdir + '/vecs.tsv', 'w', encoding='utf-8') as vectors_file:\n",
    "        for index in tqdm(range(len(embeddings.index2word))):\n",
    "            word = embeddings.index2word[index]\n",
    "            vec = embeddings.vectors[index]\n",
    "            metadata_file.write(word + \"\\n\")\n",
    "            vectors_file.write('\\t'.join([str(x) for x in vec]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up config\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'meta.tsv'\n",
    "projector.visualize_embeddings(logdir, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la extensión de Google Colab Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M72109 (Python 3.8)",
   "language": "python",
   "name": "m72109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
