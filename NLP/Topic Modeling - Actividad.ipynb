{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "NLP (nlp-py37)",
      "language": "python",
      "name": "nlp-py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Topic Modeling - Actividad.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it108cAwxRJt",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling - Actividad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VYPdzDTxRJw",
        "colab_type": "text"
      },
      "source": [
        "Topic modeling es una técnica de aprendizaje automático no supervisado donde intentados descubrir tópicos que son abstractos al texto pero que pueden describir una colección de documentos. Es importante marcar que estos \"tópicos\" no son necesariamente equivalentes a la interpretación coloquial de tópicos, sino que responden a un patrón que emerge de las palabras que están en los documentos.\n",
        "\n",
        "La suposición básica para Topic Modeling es que cada documento está representado por una mescla de tópicos, y cada tópico consite en una conlección de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VomDdG0PxRJx",
        "colab_type": "text"
      },
      "source": [
        "## Direcciones\n",
        "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
        " - Eliminación de stopwords\n",
        " - Tokenización\n",
        " - Stemming y Lemmatization\n",
        " - Procesamiento especico del tema\n",
        " - Creación de features utilizando algun metodo de reducción de dimensionalidad, SVD, LSI, LDA\n",
        "\n",
        ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estamos viendo tweets, algunos casos interesantes podrían ser:\n",
        " - Predecir el sector al que pertenece el tweet: Alimentación, Bebidas, etc.\n",
        " - Predecir el paso en el Marketing Funel al que pertece\n",
        " \n",
        "En esta actividad les propongo realizar cambios en alguna de las etapas del procesamiento para modificar la performance del modelo resultante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrD82rvfxRJy",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/santiagxf/M72109/blob/master/NLP/Docs/atap_0406.png?raw=1' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrEbIYHxRJz",
        "colab_type": "text"
      },
      "source": [
        "## Modelado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8wE_TWuxRJ0",
        "colab_type": "text"
      },
      "source": [
        "Descargamos el set de datos y lo cargmamos en un DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZfVvtGfxRJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "667fc09a-4a48-4c2e-f84e-d6dab2d1aa48"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --directory-prefix ./Datasets/mascorpus/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-01 13:05:20--  https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512573 (501K) [text/plain]\n",
            "Saving to: ‘./Datasets/mascorpus/tweets_marketing.csv’\n",
            "\n",
            "tweets_marketing.cs 100%[===================>] 500.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-09-01 13:05:20 (16.4 MB/s) - ‘./Datasets/mascorpus/tweets_marketing.csv’ saved [512573/512573]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnFmjMFkxRJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9R7yFZZxRJ7",
        "colab_type": "text"
      },
      "source": [
        "### Instalamos las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Xgn3NqxRJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "38f981ef-a740-4d81-ac3c-fc7a1c8bdf83"
      },
      "source": [
        "!pip install unidecode\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2MB 102.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp36-none-any.whl size=16172934 sha256=b9a8623aeb6d46d08dc215ceb11222b934b6596fba9ec0614d8786da694aa111\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_g_u4muj/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n_LptW1xRJ-",
        "colab_type": "text"
      },
      "source": [
        "### Construccion de los pasos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm4omFKExRJ_",
        "colab_type": "text"
      },
      "source": [
        "**Paso 1:** Instanciamos nuestro preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSJelhRfxRKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "import spacy\n",
        "import es_core_news_sm as spa\n",
        "import re\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
        "    def __init__(self, language='spanish'):\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "        self.parser = spa.load() # Cargamos el parser en español\n",
        "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
        "        self.stemmer = stem.SnowballStemmer(language=language) # Creamos un steammer\n",
        "        self.lemmatizer = lambda word : \" \".join([token.lemma_ for token in self.parser(word)]) # Creamos un lemmatizer\n",
        "        self.stopwords = set(stopwords.words(language)) # Instanciamos las stopwords en español\n",
        "        self.urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
        "    \n",
        "    def process_text(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = [token for token in tokens if not re.match(self.urls_regex, token)]\n",
        "        tokens = [token for token in tokens if len(token) > 4]\n",
        "        tokens = [token for token in tokens if token not in self.stopwords]\n",
        "        tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
        "        tokens = [self.lemmatizer(token) for token in tokens]\n",
        "        return tokens\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        for doc in X:\n",
        "          yield ' '.join(self.process_text(text=doc))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbt-OdJFxRKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bed3f8f9-2e4a-4148-d58f-f6ae08c6fb5a"
      },
      "source": [
        "normalizer = TextNormalizer()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFY6eUe4xRKF",
        "colab_type": "text"
      },
      "source": [
        "**Paso 2:** Instanciamos nuestro vectorizador, en este caso usando el método TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oMARrKNxRKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOW2STAxRKI",
        "colab_type": "text"
      },
      "source": [
        "**Paso 3:** Instanciamos nuestro generador de features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUvY-6AjxRKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "featurizer = LatentDirichletAllocation(n_components=7)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdi6dwy_xRKL",
        "colab_type": "text"
      },
      "source": [
        "**Paso 4:** Instanciamos nuestro clasificador que utilizará las features generadas hasta este momento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wspdz2OZxRKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator = LogisticRegression(max_iter=10000, multi_class='multinomial')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk5oDgUpxRKN",
        "colab_type": "text"
      },
      "source": [
        "Ensamblamos el pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVA3dR60xRKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
        "                           ('vectorizer', vectorizer),\n",
        "                           ('featurizer', featurizer),\n",
        "                           ('estimator', estimator)])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCHelkwGxRKS",
        "colab_type": "text"
      },
      "source": [
        "### Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXQsgKLaxRKT",
        "colab_type": "text"
      },
      "source": [
        "**Evaluación:** Entrenamos el modelo y testeamos su performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ6wpf2wxRKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], test_size=0.33, stratify=tweets['SECTOR'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeSBrVY3xRKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pipeline.fit(X=X_train, y=y_train)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbZSy77rxRKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ94y_xpxRKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "da21b552-4e7a-43a2-dbd1-7bd11bf8de09"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.00      0.00      0.00       110\n",
            "  AUTOMOCION       0.00      0.00      0.00       148\n",
            "       BANCA       0.00      0.00      0.00       198\n",
            "     BEBIDAS       0.28      0.27      0.27       223\n",
            "    DEPORTES       0.30      0.53      0.39       216\n",
            "      RETAIL       0.24      0.58      0.34       268\n",
            "       TELCO       0.00      0.00      0.00        79\n",
            "\n",
            "    accuracy                           0.27      1242\n",
            "   macro avg       0.12      0.20      0.14      1242\n",
            "weighted avg       0.15      0.27      0.19      1242\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgIB7iwJx-uP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}