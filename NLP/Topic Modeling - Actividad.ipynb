{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "it108cAwxRJt"
   },
   "source": [
    "# Topic Modeling - Actividad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VYPdzDTxRJw"
   },
   "source": [
    "Topic modeling es una técnica de aprendizaje automático no supervisado donde intentados descubrir tópicos que son abstractos al texto pero que pueden describir una colección de documentos. Es importante marcar que estos \"tópicos\" no son necesariamente equivalentes a la interpretación coloquial de tópicos, sino que responden a un patrón que emerge de las palabras que están en los documentos.\n",
    "\n",
    "La suposición básica para Topic Modeling es que cada documento está representado por una mescla de tópicos, y cada tópico consite en una conlección de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VomDdG0PxRJx"
   },
   "source": [
    "## Direcciones\n",
    "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
    " - Eliminación de stopwords\n",
    " - Tokenización\n",
    " - Stemming y Lemmatization\n",
    " - Procesamiento especico del tema\n",
    " - Creación de features utilizando algun metodo de reducción de dimensionalidad, SVD, LSI, LDA\n",
    "\n",
    ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estamos viendo tweets, algunos casos interesantes podrían ser:\n",
    " - Predecir el sector al que pertenece el tweet: Alimentación, Bebidas, etc.\n",
    " - Predecir el paso en el Marketing Funel al que pertece\n",
    " \n",
    "En esta actividad les propongo realizar cambios en alguna de las etapas del procesamiento para modificar la performance del modelo resultante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrD82rvfxRJy"
   },
   "source": [
    "<img src='https://github.com/santiagxf/M72109/blob/master/NLP/Docs/atap_0406.png?raw=1' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9R7yFZZxRJ7"
   },
   "source": [
    "### Instalamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "21Xgn3NqxRJ7",
    "outputId": "38f981ef-a740-4d81-ac3c-fc7a1c8bdf83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n",
      "Collecting es_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
      "\u001b[K     |████████████████████████████████| 16.2MB 102.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (49.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: es-core-news-sm\n",
      "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp36-none-any.whl size=16172934 sha256=b9a8623aeb6d46d08dc215ceb11222b934b6596fba9ec0614d8786da694aa111\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_g_u4muj/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
      "Successfully built es-core-news-sm\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPrEbIYHxRJz"
   },
   "source": [
    "### Set de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8wE_TWuxRJ0"
   },
   "source": [
    "Descargamos el set de datos y lo cargmamos en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "HZfVvtGfxRJ0",
    "outputId": "667fc09a-4a48-4c2e-f84e-d6dab2d1aa48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-01 13:05:20--  https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 512573 (501K) [text/plain]\n",
      "Saving to: ‘./Datasets/mascorpus/tweets_marketing.csv’\n",
      "\n",
      "tweets_marketing.cs 100%[===================>] 500.56K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-09-01 13:05:20 (16.4 MB/s) - ‘./Datasets/mascorpus/tweets_marketing.csv’ saved [512573/512573]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --directory-prefix ./Datasets/mascorpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mnFmjMFkxRJ4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ6wpf2wxRKT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1n_LptW1xRJ-"
   },
   "source": [
    "### Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fm4omFKExRJ_"
   },
   "source": [
    "**Paso 1:** Instanciamos nuestro preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSJelhRfxRKA"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import spacy\n",
    "import es_core_news_sm as spa\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "        self.parser = spa.load() # Cargamos el parser en español\n",
    "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
    "        self.stemmer = stem.SnowballStemmer(language='spanish') # Creamos un steammer\n",
    "        self.lemmatizer = lambda word : \" \".join([token.lemma_ for token in self.parser(word)]) # Creamos un lemmatizer\n",
    "        self.stopwords = set(stopwords.words('spanish')) # Instanciamos las stopwords en español\n",
    "        self.urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if not re.match(self.urls_regex, token)]\n",
    "        tokens = [token for token in tokens if len(token) > 4]\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
    "        tokens = [self.lemmatizer(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for doc in X:\n",
    "            yield ' '.join(self.process_text(text=doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rbt-OdJFxRKC",
    "outputId": "bed3f8f9-2e4a-4148-d58f-f6ae08c6fb5a"
   },
   "outputs": [],
   "source": [
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFY6eUe4xRKF"
   },
   "source": [
    "**Paso 2:** Instanciamos nuestro vectorizador, en este caso usando el método TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oMARrKNxRKG"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atOW2STAxRKI"
   },
   "source": [
    "**Paso 3:** Instanciamos nuestro generador de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUvY-6AjxRKJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "featurizer = LatentDirichletAllocation(n_components=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdi6dwy_xRKL"
   },
   "source": [
    "**Paso 4:** Instanciamos nuestro clasificador que utilizará las features generadas hasta este momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wspdz2OZxRKL"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "estimator=GradientBoostingClassifier(learning_rate=0.01, \n",
    "                                     n_estimators=1500,\n",
    "                                     max_depth=4, \n",
    "                                     min_samples_split=40, \n",
    "                                     min_samples_leaf=7, \n",
    "                                     subsample=1, \n",
    "                                     random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uk5oDgUpxRKN"
   },
   "source": [
    "Ensamblamos el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVA3dR60xRKO"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
    "                           ('vectorizer', vectorizer),\n",
    "                           ('featurizer', featurizer),\n",
    "                           ('estimator', estimator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCHelkwGxRKS"
   },
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXQsgKLaxRKT"
   },
   "source": [
    "**Evaluación:** Entrenamos el modelo y testeamos su performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeSBrVY3xRKW"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbZSy77rxRKY"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "uJ94y_xpxRKa",
    "outputId": "da21b552-4e7a-43a2-dbd1-7bd11bf8de09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       0.00      0.00      0.00       110\n",
      "  AUTOMOCION       0.00      0.00      0.00       148\n",
      "       BANCA       0.00      0.00      0.00       198\n",
      "     BEBIDAS       0.28      0.27      0.27       223\n",
      "    DEPORTES       0.30      0.53      0.39       216\n",
      "      RETAIL       0.24      0.58      0.34       268\n",
      "       TELCO       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.27      1242\n",
      "   macro avg       0.12      0.20      0.14      1242\n",
      "weighted avg       0.15      0.27      0.19      1242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos resultados son bastante probres como podemos ver. Intentemos buscar un poco de intuición en cuales son valores interesantes para un modelo de clasificación de este tipo utilizando Hyper-parameter tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgIB7iwJx-uP"
   },
   "source": [
    "## Hyper-parameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma más práctica de ver como nuestro modelo se comporta ante diferentes valores de los parametros es utilizando Hyper-parameter tunning. Este proceso busca configuraciones de parametros de los diferentes de nuestro modelo con el objetivo de mejorar una métrica que debemos especificar a optimizar en el proceso. Existen varias técnicas para optimización de parametros, desde técnicas sencillas como una busqueda exhaustiva donde se prueban todas las combinaciones, hasta búsquedas más avanzadas como Bayesian Optimization donde cada combinación de parametros que se prueba informa al proceso de generación sobre cuales son las direcciones a explorar en el espacio que son mas probables de tener buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro caso, utilizaremos una técnica relativamente sencilla llamada Random Search, o busqueda aleatoria. Consiste básicamente en muestrear diferentes valores de una distribución de parametros para así generar la siguiente configuración de parametros. A continuación definimos los espacios de busqueda para los paramteros:\n",
    " - n_components en LDA\n",
    " - n_estimators en nuestro GBT\n",
    " - learning_rate en nuestro GBT\n",
    " - max_depth en nuestro GBT\n",
    " \n",
    "Noten como se especifican los parametros a optimizar y los rangos a incluir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "param_grid = {\n",
    "    'featurizer__n_components': [10, 20, 30, 50, 100],\n",
    "    'estimator__n_estimators' : [128, 512, 1024, 2048],\n",
    "    'estimator__learning_rate':loguniform(1e-4, 1e-1),\n",
    "    'estimator__max_depth':[2,4,8]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos cual es la métrica que queremos optimizar. En nuestro caso, al ser un problema de clasificación de multiples clases, una métrica interesante es el Average Precision (Weithed) lo cual es un promedio ponderado de las diferentes precisiones de cada clase. Podríamos estár tentados a utilizar accuracy, pero en esta configuración del problema no nos sería muy util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "scorer = sklearn.metrics.make_scorer(sklearn.metrics.precision_score, average = 'weighted')\n",
    "tuning = RandomizedSearchCV(estimator = pipeline,\n",
    "                            param_distributions = param_grid, \n",
    "                            scoring=scorer,\n",
    "                            n_iter=10,\n",
    "                            n_jobs=4,\n",
    "                            cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la configuración anterior generaremos 10 sets de parametros que se generaran a partir de muestras del espacio de búsqueda que indicamos. Para generar las métricas estamos especificando Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota: La siguiente celda puede tardar bastante tiempo en ejecutarse. Colab podría abortar la sesión antes que termine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('normalizer', TextNormalizer()),\n",
       "                                             ('vectorizer',\n",
       "                                              TfidfVectorizer(sublinear_tf=True)),\n",
       "                                             ('featurizer',\n",
       "                                              LatentDirichletAllocation(n_components=7)),\n",
       "                                             ('estimator',\n",
       "                                              GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                         max_depth=4,\n",
       "                                                                         min_samples_leaf=7,\n",
       "                                                                         min_samples_split=40,\n",
       "                                                                         n_estimators=1500,\n",
       "                                                                         random_state=1234,\n",
       "                                                                         subsample=1))]),\n",
       "                   n_jobs=4,\n",
       "                   param_distributions={'estimator__learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4106c8dba8>,\n",
       "                                        'estimator__max_depth': [2, 4, 8],\n",
       "                                        'estimator__n_estimators': [128, 512,\n",
       "                                                                    1024,\n",
       "                                                                    2048],\n",
       "                                        'featurizer__n_components': [10, 20, 30,\n",
       "                                                                     50, 100]},\n",
       "                   scoring=make_scorer(precision_score, average=weighted))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos revisar los resultados de la búsqueda construyendo un DataFrame con los resultados de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=tuning.cv_results_['params'])\n",
    "data['mean_test_score'] = tuning.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamos y ordenamos los resultados para una visualización más sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('featurizer__n_components').apply(lambda x:x.sort_values(by='mean_test_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>estimator__learning_rate</th>\n",
       "      <th>estimator__max_depth</th>\n",
       "      <th>estimator__n_estimators</th>\n",
       "      <th>featurizer__n_components</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>featurizer__n_components</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">10</th>\n",
       "      <th>8</th>\n",
       "      <td>0.000165</td>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>0.172604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003880</td>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>0.333639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000112</td>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>10</td>\n",
       "      <td>0.341656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">30</th>\n",
       "      <th>4</th>\n",
       "      <td>0.000301</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>30</td>\n",
       "      <td>0.358786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010742</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>0.371221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.375723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034349</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>30</td>\n",
       "      <td>0.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000571</td>\n",
       "      <td>8</td>\n",
       "      <td>2048</td>\n",
       "      <td>30</td>\n",
       "      <td>0.385264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003226</td>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>30</td>\n",
       "      <td>0.389510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>2</th>\n",
       "      <td>0.023705</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.548903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            estimator__learning_rate  estimator__max_depth  \\\n",
       "featurizer__n_components                                                     \n",
       "10                       8                  0.000165                     2   \n",
       "                         6                  0.003880                     8   \n",
       "                         5                  0.000112                     8   \n",
       "30                       4                  0.000301                     2   \n",
       "                         7                  0.010742                     4   \n",
       "                         9                  0.003124                     4   \n",
       "                         1                  0.034349                     2   \n",
       "                         0                  0.000571                     8   \n",
       "                         3                  0.003226                     4   \n",
       "100                      2                  0.023705                     4   \n",
       "\n",
       "                            estimator__n_estimators  featurizer__n_components  \\\n",
       "featurizer__n_components                                                        \n",
       "10                       8                      512                        10   \n",
       "                         6                     1024                        10   \n",
       "                         5                     2048                        10   \n",
       "30                       4                     1024                        30   \n",
       "                         7                      128                        30   \n",
       "                         9                      512                        30   \n",
       "                         1                     1024                        30   \n",
       "                         0                     2048                        30   \n",
       "                         3                     2048                        30   \n",
       "100                      2                      128                       100   \n",
       "\n",
       "                            mean_test_score  \n",
       "featurizer__n_components                     \n",
       "10                       8         0.172604  \n",
       "                         6         0.333639  \n",
       "                         5         0.341656  \n",
       "30                       4         0.358786  \n",
       "                         7         0.371221  \n",
       "                         9         0.375723  \n",
       "                         1         0.381000  \n",
       "                         0         0.385264  \n",
       "                         3         0.389510  \n",
       "100                      2         0.548903  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo el mejor clasificador de la búsqueda que hicimos. El mismo corresponde a un GBT con:\n",
    " - learning_rate = 0.023705\n",
    " - max_depth = 4\n",
    " - n_estimators = 128\n",
    "\n",
    "Nuestro generador de topicos es LDA con k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = tuning.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el mismo con datos que nunca fueron vistos por el proceso de parameter tunning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       0.50      0.51      0.51       110\n",
      "  AUTOMOCION       0.60      0.44      0.51       148\n",
      "       BANCA       0.60      0.34      0.43       198\n",
      "     BEBIDAS       0.55      0.74      0.63       223\n",
      "    DEPORTES       0.59      0.53      0.56       216\n",
      "      RETAIL       0.40      0.61      0.48       268\n",
      "       TELCO       0.83      0.13      0.22        79\n",
      "\n",
      "    accuracy                           0.52      1242\n",
      "   macro avg       0.58      0.47      0.48      1242\n",
      "weighted avg       0.55      0.52      0.50      1242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Topic Modeling - Actividad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP (nlp-py37)",
   "language": "python",
   "name": "nlp-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
