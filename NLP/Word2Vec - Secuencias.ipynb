{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ME72: Maestría en Métodos Cuantitativos para la Gestión y Análisis de Datos\n",
    "M72109: Analisis de Datos no Estructurados\n",
    "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
    "Año: 2020\n",
    "Profesor: Facundo Santiago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - Modelos basados en secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos basados en secuencias tienen la fortaleza que toman una secuencia de token (en un determinado orden) y generan una salida dependiendo del tipo de problema que se trate.\n",
    " - Seq2Class: Toman una secuencia de tokens y generan una clase\n",
    " - Seq2Seq: Toman una secuencia de token y generan otra secuencia de tokens. \n",
    " \n",
    "En este notebook exploraremos este tipo de modelos utilizandolos de la mano de word2vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepración del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarguemos algunos fragmentos de código para simplificar el trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/TextNormalizer.py --directory-prefix ./Utils/\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/PadSequenceTransformer.py --directory-prefix ./Utils/\n",
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Utils/Word2VecVectorizer.py --directory-prefix ./Utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --directory-prefix ./Datasets/mascorpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos nuestros vectores de word2vec en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./Models/Word2Vec\n",
    "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/model.bin --directory-prefix ./Models/Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=tweets['SECTOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con Topic Modeling, nuestro primer paso es preprocesar el texto. Para focalizarnos en Word2Vec en este modulo, les preparé un modulo TweetTextNormalizer que hará todo el preprocesamiento por nosotros. Pueden explorar los parametros que recibe el constructor de esta clase para ver que opciones podemos configurar como Stemmer, Lemmatization, etc.\n",
    "\n",
    "En lo particular, estamos creando un TweetTextNormalizer que:\n",
    " - Aplicará un tokenizer especifico para Twitter\n",
    " - Eliminará stop words\n",
    " - Aplicará lemmatization\n",
    " - Eliminará URLs\n",
    " - Eliminará acentos\n",
    " - Eliminará las mayusculas\n",
    " \n",
    "Adicionalmente, el parametro text_to_sequence=True indica que la salida de este proceso no serán oraciones sino que tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.TextNormalizer import TweetTextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "normalizer = TweetTextNormalizer(preserve_case=False,text_to_sequence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar como funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las actividades anteriores utilizamos siempre un TF-IDF vectorizer para generar los vectores. En esta oportunidad utilizaremos Word2Vec utilizando un modelo pre-entrenado para el idioma español. Adicionalmente, vemos que este vectorizer tiene el parametro sequence_to_idx en Verdadero. Esto significa que no queremos que como salida obtengamos los vectores de Word2Vec, sino que queremos \"el indice\" que se corresponde a la palabra en una matriz de indice-palabra/vectores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Word2VecVectorizer import Word2VecVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2VecVectorizer(model_path='Models/Word2Vec/model.bin', sequence_to_idx=True)\n",
    "embedding_weights = w2v.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método get_weights() construye la matríz de indice-palabra/vector que luego será utilizado para encontrar los vectores correspondientes de cada palabra. Esta matriz tiene dimensiones m x n, donde m es la cantidad de palabras del vocabulario y n la dimensión de los vectores de word2vec. En este caso trabajamos con vectores de dimensionalidad 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos basados en sequencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos que cuando aplicamos técnicas de Topic Modeling, intentamos reducir la cantidad de dimensiones de nuestras representaciones de palabras para luego utilizar un clasificador para resolver la tarea en cuestión. Sin embargo, la suponsición básica de ese tipo de modelos es que un texto no es mas que una distribución de palabras (bag of words). Sin embargo, nosotros sabemos que un texto es una secuencia de palabras donde importa el orden. Para capturar este tipo de propiedades podemos utilizar modelos basados en secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construirmos un modelo basado en secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir nuestro modelo, utilizaremos TensorFlow. En particular utilizaremos la API de Keras que nos permite componer modelos de redes neuronales como una secuencia de pasos o capas que se conectan en una dirección. \n",
    "\n",
    "Utilizemos los siguientes tipos de capas:\n",
    " *  **Embedding:** Esta capa transforma vectores que representan indices dentro de una matriz en representaciones vectoriales densas. Básicamente en este caso nos resolverá la busqueda de las representaciones vectoriales para nuestras palabras.\n",
    " * **SpatialDropout1D:** Este tipo de capas ayudan a promover la independencia entre filtros (feature maps). Funciona en forma analoga a Dropout pero en lugar de desconectar elementos individuales, desconecta el filtro completo.\n",
    " * **LSTM:** Long Short-Term Memory layer - Hochreiter 1997\n",
    " * **Dense:** Una típica capa de una red neuronal completamente conectada (fully connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos detalles para notar:\n",
    " * *loss='sparse_categorical_crossentropy'*, este problema de clasificación (crossentropy) de más de una clase (categorical). Sin embargo, nuestro output produce probabilidades de cada una de las clases posibles (7) en forma one-hot encoding. \n",
    " * *metrics=['accuracy']*: Si bien nuestra metrica es accuracy, Keras hará un promedio ponderado del accuracy de cada clase. Este es el comportamiento por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sequence_len, vocab_size, emdedding_size, embedding_weights):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, emdedding_size,\n",
    "                  weights=[embedding_weights],\n",
    "                  trainable=False,\n",
    "                  mask_zero=True),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(emdedding_size),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos inspeccionar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para continuar utilizando la misma metodología que veniamos utilizando antes, embeberemos este clasificador diseñado en TensorFlow dentro de un pipeline en Scikit-Learn. Esto lo podemos lograr utilizando KerasClassifier. Este tipo de elementos se los conoce como Wrappers. Este paso no es necesario siempre, en nuestro caso solo porque estamos trabajando con Scikit-Learn como nuestro orquestador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=build_model, \n",
    "    epochs=50,\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=w2v.vocab_size,\n",
    "    emdedding_size=w2v.emdedding_size,\n",
    "    embedding_weights=embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando la longitud de las secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos basados en secuencias pueden adaptarse a cualquier longitud de secuencia, sin embargo, los parametros de nuestras redes neuronales deberan ser fijos. Para esto definiermos una longitud máxima de la secuencia que vamos analizar. Para esto podemos utilizar un valor especifico o utilizar el valor máximo de tokens que hay en nuestro corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente clase PadSequenceTransformer es un modulo que les preparé para simplificar este procesamiento. El mismo se encarga de ajustar cualquier secuencia para que tenga exactamente max_seq_len. Cuando la lingitud es mejor, se completan con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.PadSequenceTransformer import PadSequenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = PadSequenceTransformer(max_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando nuestro pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos nuestro pipeline como siempre combinando los diferentes pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
    "                           ('vectorizer', w2v),\n",
    "                           ('padder', seq2seq),\n",
    "                           ('estimator', estimator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "79/79 [==============================] - 14s 179ms/step - loss: 1.7382 - accuracy: 0.3392\n",
      "Epoch 2/50\n",
      "79/79 [==============================] - 14s 176ms/step - loss: 1.4215 - accuracy: 0.4891\n",
      "Epoch 3/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 1.2835 - accuracy: 0.5363\n",
      "Epoch 4/50\n",
      "79/79 [==============================] - 14s 175ms/step - loss: 1.2324 - accuracy: 0.5498\n",
      "Epoch 5/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 1.1720 - accuracy: 0.5748\n",
      "Epoch 6/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 1.1465 - accuracy: 0.5791\n",
      "Epoch 7/50\n",
      "79/79 [==============================] - 14s 173ms/step - loss: 1.1214 - accuracy: 0.5867\n",
      "Epoch 8/50\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 1.0836 - accuracy: 0.6002\n",
      "Epoch 9/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 1.0687 - accuracy: 0.5974\n",
      "Epoch 10/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 1.0422 - accuracy: 0.6152\n",
      "Epoch 11/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 1.0049 - accuracy: 0.6347\n",
      "Epoch 12/50\n",
      "79/79 [==============================] - 14s 181ms/step - loss: 0.9949 - accuracy: 0.6307\n",
      "Epoch 13/50\n",
      "79/79 [==============================] - 14s 181ms/step - loss: 0.9527 - accuracy: 0.6493\n",
      "Epoch 14/50\n",
      "79/79 [==============================] - 14s 175ms/step - loss: 0.9382 - accuracy: 0.6470\n",
      "Epoch 15/50\n",
      "79/79 [==============================] - 14s 181ms/step - loss: 0.9101 - accuracy: 0.6605\n",
      "Epoch 16/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 0.8962 - accuracy: 0.6605\n",
      "Epoch 17/50\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 0.8749 - accuracy: 0.6696\n",
      "Epoch 18/50\n",
      "79/79 [==============================] - 14s 176ms/step - loss: 0.8423 - accuracy: 0.6882\n",
      "Epoch 19/50\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 0.8043 - accuracy: 0.7073\n",
      "Epoch 20/50\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 0.7809 - accuracy: 0.7057\n",
      "Epoch 21/50\n",
      "79/79 [==============================] - 14s 171ms/step - loss: 0.7865 - accuracy: 0.7088\n",
      "Epoch 22/50\n",
      "79/79 [==============================] - 14s 176ms/step - loss: 0.7553 - accuracy: 0.7160\n",
      "Epoch 23/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 0.7437 - accuracy: 0.7203\n",
      "Epoch 24/50\n",
      "79/79 [==============================] - 14s 175ms/step - loss: 0.7185 - accuracy: 0.7346\n",
      "Epoch 25/50\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 0.7037 - accuracy: 0.7382\n",
      "Epoch 26/50\n",
      "79/79 [==============================] - 14s 171ms/step - loss: 0.6798 - accuracy: 0.7438\n",
      "Epoch 27/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 0.6591 - accuracy: 0.7620\n",
      "Epoch 28/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 0.6421 - accuracy: 0.7576\n",
      "Epoch 29/50\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 0.6205 - accuracy: 0.7755\n",
      "Epoch 30/50\n",
      "79/79 [==============================] - 14s 176ms/step - loss: 0.6124 - accuracy: 0.7798\n",
      "Epoch 31/50\n",
      "79/79 [==============================] - 14s 181ms/step - loss: 0.5860 - accuracy: 0.7798\n",
      "Epoch 32/50\n",
      "79/79 [==============================] - 14s 179ms/step - loss: 0.5645 - accuracy: 0.7937\n",
      "Epoch 33/50\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 0.5702 - accuracy: 0.7894\n",
      "Epoch 34/50\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 0.5633 - accuracy: 0.7914\n",
      "Epoch 35/50\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 0.5397 - accuracy: 0.7985\n",
      "Epoch 36/50\n",
      "79/79 [==============================] - 14s 175ms/step - loss: 0.5290 - accuracy: 0.8048\n",
      "Epoch 37/50\n",
      "79/79 [==============================] - 14s 175ms/step - loss: 0.5351 - accuracy: 0.7997\n",
      "Epoch 38/50\n",
      "79/79 [==============================] - 14s 176ms/step - loss: 0.5192 - accuracy: 0.8171\n",
      "Epoch 39/50\n",
      "79/79 [==============================] - 14s 173ms/step - loss: 0.4981 - accuracy: 0.8155\n",
      "Epoch 40/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 0.4838 - accuracy: 0.8223\n",
      "Epoch 41/50\n",
      "79/79 [==============================] - 14s 179ms/step - loss: 0.4697 - accuracy: 0.8215\n",
      "Epoch 42/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 0.4790 - accuracy: 0.8155\n",
      "Epoch 43/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 0.4554 - accuracy: 0.8366\n",
      "Epoch 44/50\n",
      "79/79 [==============================] - 14s 178ms/step - loss: 0.4574 - accuracy: 0.8350\n",
      "Epoch 45/50\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 0.4415 - accuracy: 0.8370\n",
      "Epoch 46/50\n",
      "79/79 [==============================] - 13s 171ms/step - loss: 0.4409 - accuracy: 0.8386\n",
      "Epoch 47/50\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 0.4419 - accuracy: 0.8397\n",
      "Epoch 48/50\n",
      "79/79 [==============================] - 14s 177ms/step - loss: 0.4388 - accuracy: 0.8382\n",
      "Epoch 49/50\n",
      "79/79 [==============================] - 14s 176ms/step - loss: 0.4219 - accuracy: 0.8441\n",
      "Epoch 50/50\n",
      "79/79 [==============================] - 14s 175ms/step - loss: 0.4017 - accuracy: 0.8532\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalución de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos su performance utilizando el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/nlp-py37/lib/python3.6/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "ALIMENTACION       0.78      0.64      0.70       110\n",
      "  AUTOMOCION       0.50      0.51      0.50       148\n",
      "       BANCA       0.57      0.51      0.54       198\n",
      "     BEBIDAS       0.58      0.72      0.64       223\n",
      "    DEPORTES       0.49      0.63      0.55       216\n",
      "      RETAIL       0.61      0.52      0.56       268\n",
      "       TELCO       0.62      0.38      0.47        79\n",
      "\n",
      "    accuracy                           0.57      1242\n",
      "   macro avg       0.59      0.56      0.57      1242\n",
      "weighted avg       0.58      0.57      0.57      1242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (nlp-py37)",
   "language": "python",
   "name": "nlp-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
