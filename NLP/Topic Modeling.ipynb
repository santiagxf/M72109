{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "NLP (nlp-py37)",
      "language": "python",
      "name": "nlp-py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Topic Modeling.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Irja1O_J9X",
        "colab_type": "text"
      },
      "source": [
        "# Introducci√≥n a Topic Modeling\n",
        "\n",
        "Topic modeling es una t√©cnica de aprendizaje autom√°tico no supervisado donde intentados descubrir t√≥picos que son abstractos al texto pero que pueden describir una colecci√≥n de documentos. Es importante marcar que estos \"t√≥picos\" no son necesariamente equivalentes a la interpretaci√≥n coloquial de t√≥picos, sino que responden a un patr√≥n que emerge de las palabras que est√°n en los documentos.\n",
        "\n",
        "La suposici√≥n b√°sica para Topic Modeling es que cada documento est√° representado por una mescla de t√≥picos, y cada t√≥pico consite en una conlecci√≥n de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPfF_O0U_J9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RSsdeZTGw3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6801dccc-0397-4665-b13a-69c4bd91106e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --directory-prefix ./Datasets/mascorpus/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-27 22:48:08--  https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512573 (501K) [text/plain]\n",
            "Saving to: ‚Äò./Datasets/mascorpus/tweets_marketing.csv.2‚Äô\n",
            "\n",
            "tweets_marketing.cs 100%[===================>] 500.56K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-08-27 22:48:08 (5.80 MB/s) - ‚Äò./Datasets/mascorpus/tweets_marketing.csv.2‚Äô saved [512573/512573]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc44Q7do_J9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv.1')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd6EocPdG5A0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "71eace2a-d02f-4bdb-f485-28ecc080121f"
      },
      "source": [
        "tweets.head(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXTO</th>\n",
              "      <th>SECTOR</th>\n",
              "      <th>MARCA</th>\n",
              "      <th>CANAL</th>\n",
              "      <th>AWARENESS</th>\n",
              "      <th>EVALUATION</th>\n",
              "      <th>PURCHASE</th>\n",
              "      <th>POSTPURCHASE</th>\n",
              "      <th>NC2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>IKEA</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#tr Me ofrezco para montar muebles de Ikea - H...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>IKEA</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeido...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>ALCAMPO</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#ZonaTecno Destacado: Todo lo que hay que sabe...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>CARREFOUR</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>$Carrefour retira pez #Panga. OCU y grupos x #...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>CARREFOUR</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               TEXTO  SECTOR  ... POSTPURCHASE  NC2\n",
              "0  #tablondeanuncios Funda nordica ikea #madrid h...  RETAIL  ...            0  1.0\n",
              "1  #tr Me ofrezco para montar muebles de Ikea - H...  RETAIL  ...            0  1.0\n",
              "2  #VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeido...  RETAIL  ...            0  1.0\n",
              "3  #ZonaTecno Destacado: Todo lo que hay que sabe...  RETAIL  ...            0  1.0\n",
              "4  $Carrefour retira pez #Panga. OCU y grupos x #...  RETAIL  ...            0  1.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXwJS2Og_J9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "26728132-6ed0-4aba-fcbe-40bf9533ebd9"
      },
      "source": [
        "tweets.groupby('SECTOR').head(1)[['TEXTO', 'SECTOR']]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXTO</th>\n",
              "      <th>SECTOR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
              "      <td>RETAIL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>\"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...</td>\n",
              "      <td>TELCO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964</th>\n",
              "      <td>#CarlosSlim y Bimbo lanzar√°n un veh√≠culo el√©ct...</td>\n",
              "      <td>ALIMENTACION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1298</th>\n",
              "      <td>‚ÄºüèéToyota #Day, 4ruedas ,1/4 milla, 1 #pasi√≥n, ...</td>\n",
              "      <td>AUTOMOCION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>\"- T√∫ qu√©.\\n- Yo na.\"\\nConversaciones banco sa...</td>\n",
              "      <td>BANCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2348</th>\n",
              "      <td>- Cari√±o, te juro que s√≥lo ten√≠an Cruzcampo en...</td>\n",
              "      <td>BEBIDAS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3023</th>\n",
              "      <td>#adidas #hockey Amenabar 2080 CABA https://t.c...</td>\n",
              "      <td>DEPORTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  TEXTO        SECTOR\n",
              "0     #tablondeanuncios Funda nordica ikea #madrid h...        RETAIL\n",
              "725   \"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...         TELCO\n",
              "964   #CarlosSlim y Bimbo lanzar√°n un veh√≠culo el√©ct...  ALIMENTACION\n",
              "1298  ‚ÄºüèéToyota #Day, 4ruedas ,1/4 milla, 1 #pasi√≥n, ...    AUTOMOCION\n",
              "1748  \"- T√∫ qu√©.\\n- Yo na.\"\\nConversaciones banco sa...         BANCA\n",
              "2348  - Cari√±o, te juro que s√≥lo ten√≠an Cruzcampo en...       BEBIDAS\n",
              "3023  #adidas #hockey Amenabar 2080 CABA https://t.c...      DEPORTES"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9FcIehJ_J9q",
        "colab_type": "text"
      },
      "source": [
        "# Preprosesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VlfbDQc_J9q",
        "colab_type": "text"
      },
      "source": [
        "## Stop words\n",
        "\n",
        "Algunas palabras que son extremadamente frecuentes, \"a-priori\" (revisaremos este concepto luego) no son de mucha utilidad para resolver una tarea de clasificaci√≥n de texto espec√≠fica. Estas palabras se las conoce como Stop words y, dado que son de poca utilidad, son eliminadas del texto.\n",
        "\n",
        "**Spoiler Alert:**\n",
        "Mencionamos 'a priori', porque la tendencia general en los ultimos tiempos ha sido ir desde grandes listas de stop words en el order de 200-300 a listas muy peque√±as (10-15 - si es que las hay). Los buscadores, por ejemplo, hoy en d√≠a no eliminan estas palabras. Cuando veamos modelos de lenguaje, en realidad las vamos a necesitar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja75kpIe_J9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKPDo8b6_J9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQlTlxvA_J90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spa_stopwords = stopwords.words('spanish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZIgUnkM_J93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spa_stopwords[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi58DXii_J97",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTU9Ak6_J98",
        "colab_type": "text"
      },
      "source": [
        "Se refiere al proceso de generaci√≥n de tokens basado en un texto. Un token se diferencia de una palabra en el hecho de que una palabra es una instancia de un token. Existen varias t√©cnicas para separar una oraci√≥n o texto en general en palabras discretas.\n",
        "\n",
        "Lectura recomendada: Diferentes tokenizers:\n",
        " - http://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzRujM1t_J98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet = tweets['TEXTO'][5]\n",
        "print(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXXh8by-_J9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_IERd4u_J-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "tokenizer = TweetTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlP2oXmn_J-F",
        "colab_type": "text"
      },
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "Existen palabras cuyo significado no cambia ya que estan atados a una palabra raiz que les da el significado:\n",
        "\n",
        "<i>Organizan, organso, organiza, organizando</i>\n",
        "\n",
        "**Stemming y Lemmatization** son dos t√©cnicas que generan la palabra raiz dada una palabra. La diferencia que hay entre estas t√©cnicas es que **Lemmatization** utiliza reglas del lenguaje para extraer las palabras raiz y por lo tanto, el resultado son palabras que existen en el vocabulario. Por el contrario, **Stemming** utiliza heuristicas que truncan la palabra hasta su raiz invariable. El resultado son \"psudopalabras\" o mejor conocidos como tokens que no forman una palabra del lenguaje propiamente dicho. Esta t√©cnica, como se puede intuir, es m√°s r√°pida computacionalmente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSGONgWJ_J-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import stem\n",
        "\n",
        "import spacy\n",
        "from spacy.lemmatizer import Lemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHsY1CO9_J-I",
        "colab_type": "text"
      },
      "source": [
        "**Sobre la libreria spaCy:** Spacy es una libreria para NLP muy polupar actualmente ya que, al contrario de nltk, ofrece formas muy eficientes de hacer solo algunos tipos de operaciones. NLTK es una herramienta m√°s general. Para instalar spaCy en espa√±ol necesitaran ejecutar:\n",
        "\n",
        "```\n",
        "conda install -c spacy spacy\n",
        "python -m spacy download es_core_news_sm\n",
        "```\n",
        "\n",
        "Si bien NLTK ofrece la opci√≥n de hacer Lemmatization, su soporte mayoritariamente es para ingles. La versi√≥n en espa√±ol no es demasiado buena. Si les interesa probarla puede hacerlo a traves del metodo.\n",
        "\n",
        "```\n",
        "nltk.wordnet.lemas(\"palabra\", lang='spa')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2g96tb8_J-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVeCzmo5_J-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = spacy.load('es_core_news_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--OMLZ2k_J-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)])\n",
        "stemmer = stem.SnowballStemmer(language='spanish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqLoIHHd_J-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['amigos', 'amigo', 'amiga', 'amistad' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_4-B8Me_J-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[stemmer.stem(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zffzVC3_J-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[lemmatizer(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5wt-YMf_J-e",
        "colab_type": "text"
      },
      "source": [
        "Nota: La precisi√≥n de Lemmatization depende de la implementaci√≥n. La de espa√±ol no es demasiado buena. Algunas palabras podrian no encontrarse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VT-fzN_J-f",
        "colab_type": "text"
      },
      "source": [
        "### Otra estrategia\n",
        "\n",
        "El problema de reducir las palabras a sus formatos raiz radica en que en general cada palabra (separada por espacios, puntos, etc) conforma un elemento en nuestro vocabulario y no queremos diferentes elementos de nuestro vocabulario que mapeen al mismo elemento o concepto. Si por el contrario utilizaramos otra estrategia para determinar nuestro vocabulario (o mejor dicho, cada elemento de nuestro vocabulario) entonces este problema quiz√°s no existir√≠a (o se volver√≠a peor).\n",
        "\n",
        "Este tipo de t√©cnicas por lo general intentan representar el vocabulario con \"sub-palabras\" o partes de las palabras como unidad. Un ejemplo de esto es SentencePiece."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZISG0Cs_J-g",
        "colab_type": "text"
      },
      "source": [
        "## Creando una rutina de preprosesamiento de texto\n",
        "\n",
        "Adicionalmente de utilizar Lemmatization y eliminar stop words, necesitamos hacer algunas tareas extras:\n",
        " - Eliminar caracteres especiales: Acentos y caracteres especiales podr√≠an complejizar el la representaci√≥n de palabras, por lo que los eliminaremos.\n",
        " - Eliminaremos URLs y handles que son t√≠picos en tweeter. Esto es especifico en este set de datos ya que una URL no representa informaci√≥n en este contexto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eJxv1LA_J-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "import spacy\n",
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "parser = spacy.load('es_core_news_sm')\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "stemmer = stem.SnowballStemmer(language='spanish')\n",
        "lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)])\n",
        "stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "def process_text(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in stopwords]\n",
        "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
        "    tokens = [lemmatizer(token) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4uXheye_J-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_list = []\n",
        "\n",
        "for doc in tqdm(tweets['TEXTO']):\n",
        "    tokens = process_text(doc)\n",
        "    doc_list.append(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtykSqbA_J-l",
        "colab_type": "text"
      },
      "source": [
        "Revisemos algunos resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQgWOejW_J-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['TEXTO'][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huZ2Cu2Q_J-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_list[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXVzn24_J-p",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1nMf7TY_J-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWk8TAKN_J-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OP1XPee_J-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = vectorizer.fit_transform(doc_list).todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqr6FFqe_J-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBHCllSp_J-2",
        "colab_type": "text"
      },
      "source": [
        "¬øQue representa 7665?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcWd57oJ_J-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.array(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30nO-ORD_J-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab[4040:4050]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tC0kyK_J-6",
        "colab_type": "text"
      },
      "source": [
        "## M√©todos b√°sados en SVD\n",
        "\n",
        "Los modelos basados en factorizaci√≥n de matrices intentan reducir la dimensionalidad de la matriz al aproximarla usando dos matrices m√°s peque√±as con <i>k</i> factores latentes. Este m√©todo es bastante popular no solo en NLP sino que tambi√©n en sistemas de recomendaci√≥n, m√©todo que fu√© ganador del Netflix Prize (Funk SVD).\n",
        "\n",
        "El concepto de decomposici√≥n de matrices es muy similar al de PCA en el sentido de que el n√∫mero de factores latentes determina la cantidad de concepto abstractos que queremos mapear en un espacio dimensional menor. A medida que agregamos factores latentes, aumentaremos la especificaci√≥n de los mismos hasta que llegue un momento donde los factores ser√°n demasiados y el modelo comience a saturarse (over-fitting).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*Z0EUVs7QElEqRqXtqut_FQ.png\" />\n",
        "\n",
        "\n",
        "U y V(trapuesta) son ortogonales. Esto es de esperar porrque si determinadas propiedades determinan un determinado factor latente, entonces esas propiedades ser√°n poco relevantes en los restantes factores (pues sino, no har√≠a sentido que conformen un factor distinto en un primer lugar).\n",
        "\n",
        "SVC es un metodo de decomposici√≥n exacto, lo que singnifica que las matrices U y V son lo suficientemente grandes para mapear exactamente la matriz A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1pbwdPv_J-7",
        "colab_type": "text"
      },
      "source": [
        "## LSI - Latent Semantic Indexing\n",
        "\n",
        "Cuando SVD es utilizado para procesar t√≥picos en texto y en donde los valores de la matriz A corresponden a frecuencias de palabras (ya sea por su frecuencia de aparici√≥n o con el m√©todo TF-IDF), este m√©todo se lo denomina Latent Semantic Analysis (sin embargo, en NLP no se lo suele nombrar como LSI).\n",
        "\n",
        "<img src='https://github.com/fastai/course-nlp/raw/aabfeddf61fea29b18c72f841d057b56a216b7eb/images/svd_fb.png' />\n",
        "\n",
        "Facebook Research: Fast Randomized SVD [https://research.fb.com/fast-randomized-svd/])\n",
        "\n",
        "En esta configuraci√≥n entonces:\n",
        " - A es una matriz de m x n donde m es la cantidad de documentos √∫ observaciones, y n es la cantidad de palabras en el vocabulario.\n",
        " - Los valores de A corresponden a la frecuencia de la cada palabra del vocabulario en cada observaci√≥n √∫ documento.\n",
        " \n",
        "Adicionalmente, dado que SVC es un m√©todo de decomposici√≥n exacto, tiende a producir matrices de poca densidad (sparse). Para evitar este problema, se utiliza una versi√≥n modificada de SVC conocida como Truncated SVD que solamente computa los k componentes mas grandes en la descomposici√≥n. Esto ayuda a que LSI combata efectivamente el problema de matrices sparse que tienden a generarse cuando se tienen cuerpos de texto con sin√≥nimos y palabras que significan varias cosas dependiendo del contexto. Truncated SVD ev√≠ta ser un m√©todo de decomposici√≥n exacto al trabjar con una matriz Q que satisface:\n",
        "\n",
        "$$A \\approx QQ^*A $$\n",
        "\n",
        "M√©todos para generar Q pueden ser encontrados en el paper: Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions [https://arxiv.org/abs/0909.4061]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUldeeN6_J-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')\n",
        "vectors = vectorizer.fit_transform(doc_list).todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSf9hs5Z_J--",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF es una forma de normalizar los vectores de frecuencias al tomar en consideraci√≥n la frecuencia en la que aparece la palabra en el documento, la longitud del documento y que tan comun o raro es la palabra en todo el corpus.\n",
        "\n",
        "$$TF = \\frac {freq(w_i)} {len(doc)} $$\n",
        "\n",
        "\n",
        "$$IDF = log(\\frac {len(corpus)} {freq(w_i, corpus)}) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ2uzF82_J--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svd = TruncatedSVD(n_components=7, algorithm='randomized')\n",
        "USigma = svd.fit_transform(vectors)\n",
        "Sigma = svd.singular_values_\n",
        "VT = svd.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvvt3kpu_J_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VT.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwu_KCg__J_C",
        "colab_type": "text"
      },
      "source": [
        "Internamente, TrucatedSVC es un wrapper de la clase randomized_svd donde la matr√≠z Q que vimos anteriormente se genera a trav√©s de un m√©todo de sampling aleatorio. Las siguientes lineas son equivalentes a lo que vimos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kigl-Xl_J_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(vectors, \n",
        "                              n_components=7,\n",
        "                              n_iter=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9fOGLB_J_F",
        "colab_type": "text"
      },
      "source": [
        "Podemos validar que U es una matriz ortogonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLYEo9Gm_J_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.allclose(U.T @ U, np.eye(U.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSv1wUgK_J_I",
        "colab_type": "text"
      },
      "source": [
        "Si vemos los valores de la matriz Sigma, veremos la importancia relativa de los documentos con respecto a los t√≥picos que encontramos. Si los gr√°ficamos vemos que sus valores comienzan a decrecer relativamente r√°pido, sosteniendo la supoci√≥n de que Truncated SVD genera los K m√°s relevantes t√≥picos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVI80HRw_J_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(Sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR8CpAYe_J_K",
        "colab_type": "text"
      },
      "source": [
        "### Interpretando los t√≥picos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBNIN0xE_J_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_topics(a):\n",
        "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[-8:-1]]\n",
        "    topic_words = ([top_words(t) for t in a])\n",
        "    return [' '.join(t) for t in topic_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDhDFVY9_J_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_topics(VT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKCo8UOd_J_Q",
        "colab_type": "text"
      },
      "source": [
        "Limitaciones en SVD:\n",
        " - SVD sufre de un problema llamado \"Indeterminaci√≥n del signo\", que b√°sicamente significa que el signo en la matr√≠z VT y USigma dependen del algorimo que se utiliz√≥ para generarlos y de las condiciones iniciales (initial random state). En este contexto, que significa que un t√≥pico est√© relacionado con una palabra en un valor negativo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no6V6rtO_J_Q",
        "colab_type": "text"
      },
      "source": [
        "## NMF: Non-negative Matrix Factorization\n",
        "\n",
        "Motivaci√≥n: En lugar de construir nuestros factores imponiendo la restricci√≥n de que sean ortogonales, la idea es de construirlos de tal forma que sean no-negativos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQwyjnN8_J_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf = NMF(n_components=7, random_state = 1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdTfBg2n_J_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W1 = nmf.fit_transform(vectors)\n",
        "H1 = nmf.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S39Zrr06_J_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8qfO_Dx_J_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_topics(H1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbMNkPv7_J_i",
        "colab_type": "text"
      },
      "source": [
        "## LDA: Latent Dirichlet Allocation\n",
        "\n",
        "LDA es un m√©todo Bayesiano basado en la distribuci√≥n de Dirichlet, la cual es una distribuci√≥n sobre probabilidades en K categorias. LDA supone que los documentos que tenemos pertenecen a K categorias distintas cuya distribuci√≥n es desconocida.\n",
        "\n",
        "La distribuci√≥n Dirichlet es una generalizaci√≥n de la distribuci√≥n Beta en un espacio multidimensional. As√≠ como la distribuci√≥n beta es la distribuci√≥n previa de la binomial, la distribuci√≥n de Dirichlet es la distribuci√≥n previa de la multinomial. \n",
        "\n",
        "$$ P(w\\mid d) = P(d)\\sum_c P(k\\mid d)P(w\\mid k) $$\n",
        "\n",
        "David Blei, Andrew Ng, Michael Jordan:  Latent Dirichlet Allocation [https://jmlr.org/papers/volume3/blei03a/blei03a.pdf]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN1vPUB3_J_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1dC1tN6_J_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda.fit(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQdTx5My_J_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx, topic in enumerate(lda.components_):\n",
        "    print (\"Topic \", idx, \" \".join(vocab[i] for i in topic.argsort()[:-10 - 1:-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFoS39i_J_r",
        "colab_type": "text"
      },
      "source": [
        "# Creando un pipeline de preprocesamiento de texto\n",
        "\n",
        "A pesar de que los m√©todos anteriores son no supervisados, son de utilidad para el modelado de de problemas no supervisados como supervisados. Para llevar estos m√©todos a un entorno pr√°ctico normalmente se construyen flujos de procesamiento como el que se muestra m√°s abajo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLSZG2Gy_J_r",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/santiagxf/M72109/blob/master/NLP/Docs/atap_0406.png?raw=1' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdw5EvPI_J_s",
        "colab_type": "text"
      },
      "source": [
        "A modo de ejemplo, el siguiente codigo utiliza la API de Scikit-Learn para generar el paso de normalizaci√≥n de texto. Este \"paso\" lo podemos insertar en un pipeline de Machine Learning que luego utilicemos para resolver una tarea en particular"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WWuoi17_J_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "import spacy\n",
        "import sklearn\n",
        "\n",
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
        "    def __init__(self, language='spanish'):\n",
        "        parser = spacy.load('es_core_news_sm')\n",
        "        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "        stemmer = stem.SnowballStemmer(language=language)\n",
        "        lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)])\n",
        "        stopwords = set(stopwords.words(language))\n",
        "    \n",
        "    def process_text(text):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        tokens = [token for token in tokens if len(token) > 4]\n",
        "        tokens = [token for token in tokens if token not in stopwords]\n",
        "        tokens = [unidecode.unidecode(token) for token in tokens]\n",
        "        tokens = [lemmatizer(token) for token in tokens]\n",
        "        return tokens\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        for doc in X:\n",
        "            yield ' '.join(self.process_text(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhSKt53_J_w",
        "colab_type": "text"
      },
      "source": [
        "Importamos algunas librerias que necesitaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bx5kQr6_J_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSpeGyC_J_0",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3pP6II_J_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = TextNormalizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU4ykD8z_J_3",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro vectorizador, en este caso usando el m√©todo TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVIdgSc9_J_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSnMG5la_J_5",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro generador de features, que en este caso son los t√≥picos que LDA genere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o18xNsj_J_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featurizer = LatentDirichletAllocation(n_components=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FumqlRDO_J__",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro clasificador que utilizar√° las features generadas hasta este momento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mpzmpo__KAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = LogisticRegression(max_iter=10000, tol=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xkn_cWu_KAE",
        "colab_type": "text"
      },
      "source": [
        "Creamos un pipeline que ejecute todos los pasos en secuencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAccQHAG_KAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
        "                           ('vectorizer', vectorizer),\n",
        "                           ('featurizer', featurizer),\n",
        "                           ('estimator', estimator)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp-fZa4E_KAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline.fit(tweets['TEXTO'], tweets['SECTOR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiQLKHrr_KAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = pipeline.predict(tweets['TEXTO'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbDgBh5M_KAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(tweets['SECTOR'], predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Spv_QP_KAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}