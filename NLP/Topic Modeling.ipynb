{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "NLP (nlp-py37)",
      "language": "python",
      "name": "nlp-py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Topic Modeling.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUcDn3buE-_q",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "ME72: Maestr√≠a en M√©todos Cuantitativos para la Gesti√≥n y An√°lisis de Datos\n",
        "M72109: Analisis de datos no estructurados\n",
        "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
        "A√±o: 2020\n",
        "Profesor: Facundo Santiago\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Irja1O_J9X",
        "colab_type": "text"
      },
      "source": [
        "# Introducci√≥n a M√©todos basados en frecuencias y Topic Modeling\n",
        "\n",
        "Topic modeling es una t√©cnica de aprendizaje autom√°tico no supervisado donde intentados descubrir t√≥picos que son abstractos al texto pero que pueden describir una colecci√≥n de documentos. Es importante marcar que estos \"t√≥picos\" no son necesariamente equivalentes a la interpretaci√≥n coloquial de t√≥picos, sino que responden a un patr√≥n que emerge de las palabras que est√°n en los documentos.\n",
        "\n",
        "La suposici√≥n b√°sica para Topic Modeling es que cada documento est√° representado por una mescla de t√≥picos, y cada t√≥pico consiste en una colecci√≥n de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_O7bEjLebd",
        "colab_type": "text"
      },
      "source": [
        "## Sobre el set de datos con el que vamos a trabajar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8lcRTa_Li4e",
        "colab_type": "text"
      },
      "source": [
        "Utilizaremos como ejemplo un set de datos en espa√±ol que contiene tweets que diferentes usuarios han publicado en relaci√≥n a diferentes marcas de productos u empresas en el rubro de alimentaci√≥n, construcci√≥n, automoviles, etc. Estos tweets, a su vez, est√°n asociados a una de las diferentes fases en el proceso de ventas (tambi√©n conocido como Marketing Funel) y por eso est√°n tagueados con las fases de:\n",
        " - Awareness ‚Äì el cliente es conciente de la existencia de un producto o servicio\n",
        " - Interest ‚Äì activamente expresa el interes de un producto o servicio\n",
        " - Evaluation ‚Äì aspira una marca o producto en particular\n",
        " - Purchase ‚Äì toma el siguiente paso necesario para comprar el producto o servicio\n",
        " - Postpurchase - realizaci√≥n del proceso de compra. El cliente compara la diferencia entre lo que deseaba y lo que obtuvo\n",
        "\n",
        "Referencia: Spanish Corpus of Tweets for Marketing [http://ceur-ws.org/Vol-2111/paper1.pdf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPpqVNrwSdhL",
        "colab_type": "text"
      },
      "source": [
        "Primero importaremos algunas librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPfF_O0U_J9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2TJjaoMSh-w",
        "colab_type": "text"
      },
      "source": [
        "Prepar√© una versi√≥n pre-procesada de este dataset que pueden descargar desde:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RSsdeZTGw3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "85790c19-c645-40c1-967e-be7866bd2346"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --directory-prefix ./Datasets/mascorpus/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-28 15:29:51--  https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512573 (501K) [text/plain]\n",
            "Saving to: ‚Äò./Datasets/mascorpus/tweets_marketing.csv.2‚Äô\n",
            "\n",
            "tweets_marketing.cs 100%[===================>] 500.56K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-08-28 15:29:51 (6.06 MB/s) - ‚Äò./Datasets/mascorpus/tweets_marketing.csv.2‚Äô saved [512573/512573]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc44Q7do_J9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INJwReUXSs4K",
        "colab_type": "text"
      },
      "source": [
        "Inspeccionamos el set de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd6EocPdG5A0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6d1df056-08d1-4e7d-89b8-342c80082c2d"
      },
      "source": [
        "tweets.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXTO</th>\n",
              "      <th>SECTOR</th>\n",
              "      <th>MARCA</th>\n",
              "      <th>CANAL</th>\n",
              "      <th>AWARENESS</th>\n",
              "      <th>EVALUATION</th>\n",
              "      <th>PURCHASE</th>\n",
              "      <th>POSTPURCHASE</th>\n",
              "      <th>NC2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>IKEA</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#tr Me ofrezco para montar muebles de Ikea - H...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>IKEA</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeido...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>ALCAMPO</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#ZonaTecno Destacado: Todo lo que hay que sabe...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>CARREFOUR</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>$Carrefour retira pez #Panga. OCU y grupos x #...</td>\n",
              "      <td>RETAIL</td>\n",
              "      <td>CARREFOUR</td>\n",
              "      <td>Microblog</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               TEXTO  SECTOR  ... POSTPURCHASE  NC2\n",
              "0  #tablondeanuncios Funda nordica ikea #madrid h...  RETAIL  ...            0  1.0\n",
              "1  #tr Me ofrezco para montar muebles de Ikea - H...  RETAIL  ...            0  1.0\n",
              "2  #VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeido...  RETAIL  ...            0  1.0\n",
              "3  #ZonaTecno Destacado: Todo lo que hay que sabe...  RETAIL  ...            0  1.0\n",
              "4  $Carrefour retira pez #Panga. OCU y grupos x #...  RETAIL  ...            0  1.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXwJS2Og_J9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "7b354598-bf6f-41a6-dd63-afe7897e3d68"
      },
      "source": [
        "tweets.groupby('SECTOR').head(1)[['TEXTO', 'SECTOR']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXTO</th>\n",
              "      <th>SECTOR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#tablondeanuncios Funda nordica ikea #madrid h...</td>\n",
              "      <td>RETAIL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>\"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...</td>\n",
              "      <td>TELCO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964</th>\n",
              "      <td>#CarlosSlim y Bimbo lanzar√°n un veh√≠culo el√©ct...</td>\n",
              "      <td>ALIMENTACION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1298</th>\n",
              "      <td>‚ÄºüèéToyota #Day, 4ruedas ,1/4 milla, 1 #pasi√≥n, ...</td>\n",
              "      <td>AUTOMOCION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>\"- T√∫ qu√©.\\n- Yo na.\"\\nConversaciones banco sa...</td>\n",
              "      <td>BANCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2348</th>\n",
              "      <td>- Cari√±o, te juro que s√≥lo ten√≠an Cruzcampo en...</td>\n",
              "      <td>BEBIDAS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3023</th>\n",
              "      <td>#adidas #hockey Amenabar 2080 CABA https://t.c...</td>\n",
              "      <td>DEPORTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  TEXTO        SECTOR\n",
              "0     #tablondeanuncios Funda nordica ikea #madrid h...        RETAIL\n",
              "725   \"Ilcinsisti lis MB dispiniblis\" te odeeeeeo Mo...         TELCO\n",
              "964   #CarlosSlim y Bimbo lanzar√°n un veh√≠culo el√©ct...  ALIMENTACION\n",
              "1298  ‚ÄºüèéToyota #Day, 4ruedas ,1/4 milla, 1 #pasi√≥n, ...    AUTOMOCION\n",
              "1748  \"- T√∫ qu√©.\\n- Yo na.\"\\nConversaciones banco sa...         BANCA\n",
              "2348  - Cari√±o, te juro que s√≥lo ten√≠an Cruzcampo en...       BEBIDAS\n",
              "3023  #adidas #hockey Amenabar 2080 CABA https://t.c...      DEPORTES"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9FcIehJ_J9q",
        "colab_type": "text"
      },
      "source": [
        "## Preprosesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfUnlH25HEeM",
        "colab_type": "text"
      },
      "source": [
        "Como en toda tarea de NLP, y m√°s generalmente, en Machine Learning, ocuparemos una porci√≥n de nuestro tiempo en preprocesar los datos para generar representaciones √∫tiles y deshacernos de problemas especificos que podr√≠a exhibir nuestro set de datos. Veamos a continuaci√≥n algunos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VlfbDQc_J9q",
        "colab_type": "text"
      },
      "source": [
        "### Stop words\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcaReFCnS5lf",
        "colab_type": "text"
      },
      "source": [
        "Algunas palabras que son extremadamente frecuentes, \"a-priori\" (revisaremos este concepto luego) no son de mucha utilidad para resolver una tarea de clasificaci√≥n de texto espec√≠fica. Estas palabras se las conoce como Stop words y, dado que son de poca utilidad, son eliminadas del texto.\n",
        "\n",
        "**Spoiler Alert:**\n",
        "Mencionamos 'a priori', porque la tendencia general en los ultimos tiempos ha sido ir desde grandes listas de stop words en el order de 200-300 a listas muy peque√±as (10-15 - si es que las hay). Los buscadores, por ejemplo, hoy en d√≠a no eliminan estas palabras. Cuando veamos modelos de lenguaje, en realidad las vamos a necesitar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja75kpIe_J9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKPDo8b6_J9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7c6ff55a-67f9-4ec4-edc1-ca0cd6d32c8d"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQlTlxvA_J90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spa_stopwords = stopwords.words('spanish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZIgUnkM_J93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e907aec5-8c16-445f-ae3a-224c0ddf0278"
      },
      "source": [
        "spa_stopwords[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi58DXii_J97",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTU9Ak6_J98",
        "colab_type": "text"
      },
      "source": [
        "Se refiere al proceso de generaci√≥n de tokens basado en un texto. A alto nivel, se podr√≠a ver como la tarea de dividir oraciones en palabras. Un token se diferencia de una palabra en el hecho de que una palabra es una instancia de un token. Existen varias t√©cnicas para separar una oraci√≥n o texto en general en tokens:\n",
        "\n",
        "Lectura recomendada: Diferentes tokenizers:\n",
        " - http://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BiCyQ7yTON5",
        "colab_type": "text"
      },
      "source": [
        "Tomemos un tweet de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzRujM1t_J98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aab2077e-8acc-43cc-9412-695a85dccbbf"
      },
      "source": [
        "tweet = tweets['TEXTO'][5]\n",
        "print(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXXh8by-_J9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5glHPgt0Ir6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "58d3dd2b-42c8-42d1-ee37-b26b2196b1d9"
      },
      "source": [
        "tokenizer.tokenize(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " '@',\n",
              " 'PoliciadeBurgos',\n",
              " '@',\n",
              " 'PCivilBurgos',\n",
              " '@',\n",
              " 'Aytoburgos',\n",
              " 'Mismo',\n",
              " 'peligro',\n",
              " 'c/',\n",
              " 'Rio',\n",
              " 'Viejo',\n",
              " 'junto',\n",
              " 'Mercadona',\n",
              " 'Villimar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_IERd4u_J-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "tokenizer = TweetTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6AHmFOgI-DT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b774d5ff-c7a7-48b5-b6ed-ea2be842a49e"
      },
      "source": [
        "tokenizer.tokenize(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " '@PoliciadeBurgos',\n",
              " '@PCivilBurgos',\n",
              " '@Aytoburgos',\n",
              " 'Mismo',\n",
              " 'peligro',\n",
              " 'c',\n",
              " '/',\n",
              " 'Rio',\n",
              " 'Viejo',\n",
              " 'junto',\n",
              " 'Mercadona',\n",
              " 'Villimar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlP2oXmn_J-F",
        "colab_type": "text"
      },
      "source": [
        "### Stemming and Lemmatization\n",
        "\n",
        "Existen palabras cuyo significado no cambia ya que estan atados a una palabra raiz que les da el significado:\n",
        "\n",
        "<i>Organizan, organiza, organizando, organizaron</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uvHJjdPTewA",
        "colab_type": "text"
      },
      "source": [
        "**Stemming y Lemmatization** son dos t√©cnicas que generan la palabra raiz dada una palabra. La diferencia que hay entre estas t√©cnicas es que **Lemmatization** utiliza reglas del lenguaje para extraer las palabras raiz y por lo tanto, el resultado son palabras que existen en el vocabulario. Por el contrario, **Stemming** utiliza heuristicas que truncan la palabra hasta su raiz invariable. El resultado son \"psudopalabras\" o mejor conocidos como tokens que no forman una palabra del lenguaje propiamente dicho. Esta t√©cnica, como se puede intuir, es m√°s r√°pida computacionalmente. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WATarE2OTlRI",
        "colab_type": "text"
      },
      "source": [
        "Utilizaremos la libreria nltk para stemming y spaCy para lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSGONgWJ_J-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import stem\n",
        "\n",
        "import spacy\n",
        "from spacy.lemmatizer import Lemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBRNgDd_JkJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6f09736f-5ab1-4949-9544-3bc227cb30c3"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz#egg=es_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHsY1CO9_J-I",
        "colab_type": "text"
      },
      "source": [
        "**Sobre la libreria spaCy:** Spacy es una libreria para NLP muy polupar actualmente ya que, al contrario de nltk, ofrece formas muy eficientes de hacer solo algunos tipos de operaciones. NLTK es una herramienta m√°s general. Para instalar spaCy en espa√±ol necesitaran ejecutar:\n",
        "\n",
        "```\n",
        "conda install -c spacy spacy\n",
        "python -m spacy download es_core_news_sm\n",
        "```\n",
        "\n",
        "Si bien NLTK ofrece la opci√≥n de hacer Lemmatization, su soporte mayoritariamente es para ingles. La versi√≥n en espa√±ol no es demasiado buena. Si les interesa probarla puede hacerlo a traves del metodo.\n",
        "\n",
        "```\n",
        "nltk.wordnet.lemas(\"palabra\", lang='spa')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWqTzFm0Tydy",
        "colab_type": "text"
      },
      "source": [
        "Cargamos el modelo en espa√±ol e instanciamos el parser:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVeCzmo5_J-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import es_core_news_sm as spa\n",
        "parser = spa.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLWW840SUEcS",
        "colab_type": "text"
      },
      "source": [
        "Creamos 2 funciones que nos ayuden a simplificar el uso de estos m√©todos: una para cada una de las t√©cnicas mencionadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--OMLZ2k_J-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)])\n",
        "stemmer = stem.SnowballStemmer(language='spanish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqLoIHHd_J-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['amigos', 'amigo', 'amiga', 'amistad' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_4-B8Me_J-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "904b68ed-60a1-4142-a351-676fe88fada3"
      },
      "source": [
        "[stemmer.stem(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amig', 'amig', 'amig', 'amist']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zffzVC3_J-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "127c641e-aee0-4c85-aaf0-b2aa6f9408ae"
      },
      "source": [
        "[lemmatizer(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amigo', 'amigar', 'amigo', 'amistar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5wt-YMf_J-e",
        "colab_type": "text"
      },
      "source": [
        "Nota: La precisi√≥n de Lemmatization depende de la implementaci√≥n. La de espa√±ol no es demasiado buena. Noten tambi√©n lo que sucede con la palabra \"amigo\": ¬øEs el verbo amigar o el sustantivo amigo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VT-fzN_J-f",
        "colab_type": "text"
      },
      "source": [
        "### Otra estrategia\n",
        "\n",
        "El problema de reducir las palabras a sus formatos raiz radica en que en general cada palabra (separada por espacios, puntos, etc) conforma un elemento en nuestro vocabulario y no queremos diferentes elementos de nuestro vocabulario que mapeen al mismo elemento o concepto. Si por el contrario utilizaramos otra estrategia para determinar nuestro vocabulario (o mejor dicho, cada elemento de nuestro vocabulario) entonces este problema quiz√°s no existir√≠a (o se volver√≠a peor).\n",
        "\n",
        "Este tipo de t√©cnicas por lo general intentan representar el vocabulario con \"sub-palabras\" o partes de las palabras como unidad. Un ejemplo de esto es SentencePiece."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZISG0Cs_J-g",
        "colab_type": "text"
      },
      "source": [
        "### Creando una rutina de preprosesamiento de texto\n",
        "\n",
        "Adicionalmente de utilizar Lemmatization y eliminar stop words, necesitamos hacer algunas tareas extras:\n",
        " - Eliminar caracteres especiales: Acentos y caracteres especiales podr√≠an complejizar el la representaci√≥n de palabras, por lo que los eliminaremos.\n",
        " - Eliminaremos URLs y handles que son t√≠picos en tweeter. Esto es especifico en este set de datos ya que una URL no representa informaci√≥n en este contexto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_DPo-_PM_Xv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e13fc9cf-e740-4f19-f32e-521c057fdae2"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eJxv1LA_J-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "import spacy\n",
        "import es_core_news_sm as spa\n",
        "import re\n",
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "parser = spa.load() # Cargamos el parser en espa√±ol\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
        "stemmer = stem.SnowballStemmer(language='spanish') # Creamos un steammer\n",
        "lemmatizer = lambda word : \" \".join([token.lemma_ for token in parser(word)]) # Creamos un lemmatizer\n",
        "stopwords = set(stopwords.words('spanish')) # Instanciamos las stopwords en espa√±ol\n",
        "urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
        "\n",
        "def process_text(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token for token in tokens if not re.match(urls_regex, token)]\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in stopwords]\n",
        "    tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
        "    tokens = [lemmatizer(token) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4uXheye_J-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a79ef5e4-2048-41c6-e95b-bd8143567c28"
      },
      "source": [
        "doc_list = []\n",
        "\n",
        "for doc in tqdm(tweets['TEXTO']):\n",
        "    tokens = process_text(doc)\n",
        "    doc_list.append(' '.join(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3763/3763 [02:08<00:00, 29.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtykSqbA_J-l",
        "colab_type": "text"
      },
      "source": [
        "Revisemos algunos resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQgWOejW_J-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9bf3e73-d7fd-45df-83f4-06d74e9d4af2"
      },
      "source": [
        "tweets['TEXTO'][2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#VozP√≥puli Vozp√≥puli @voz_populi - #LoM√°sLeidoHoy Mercadona, DIA o Alcampo guardan silencio ante la ola europea... https://t.co/aJTuA4J9UV'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huZ2Cu2Q_J-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2735e7f4-9625-4ab8-d826-360912b5ca19"
      },
      "source": [
        "doc_list[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# VozPopuli Vozpopuli # LoMasLeidoHoy Mercadona Alcampo guardar silenciar europeo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WSbMjTnRYXT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdd43d26-0161-46eb-eafe-3a3ad9c9211e"
      },
      "source": [
        "len(doc_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXVzn24_J-p",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLwWu__sX4Rq",
        "colab_type": "text"
      },
      "source": [
        "Una vez que nuesto texto fue preprocesado para mantener solo aquellas palabras que nos son relevantes, pasamos al proceso de generar vectores a partir de las palabras que componen nuestro vocabulario. Nuestros modelos no pueden operar sobre palabras, y por lo tanto necesitamos una representaci√≥n n√∫merica de las mismas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjAlqTArYGkD",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KNGM__EYKHX",
        "colab_type": "text"
      },
      "source": [
        "Quiz√°s una de las t√©cnicas m√°s sencillas, one-hot encoding utiliza la misma l√≥gica que cuando se utiliza para codificar variables categoricas. Utilizando un vector de longitud igual al tama√±o del vocabulario, asigna un 1 a la posici√≥n de cada palabra en el diccionario. As√≠ si la palabra \"hola\" es la primera palabra del diccionario, su vector ser√° [1 0 0 0 0 0 ... 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLXvkGy8Y889",
        "colab_type": "text"
      },
      "source": [
        "### Frecuencia de la palabra (TF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBcSI27AZDYJ",
        "colab_type": "text"
      },
      "source": [
        "Una forma m√°s interesante de representar una palabra podr√≠a ser tratando de capturar la frecuencia con la que aparece, haciendo sentido de que si una palabra aparece muy frecuentemente en un texto, entonces tendr√≠a mas relevancia. Esto lo podemos lograr con CountVectorizer el cual calcula:\n",
        "\n",
        "$$TF = \\frac {freq(w_i)} {len(doc)} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1nMf7TY_J-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWk8TAKN_J-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OP1XPee_J-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = vectorizer.fit_transform(doc_list).todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqr6FFqe_J-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a9cdcf2-937f-46bd-deff-0b5b9139756c"
      },
      "source": [
        "vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3763, 6729)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBHCllSp_J-2",
        "colab_type": "text"
      },
      "source": [
        "¬øQue representa 6729?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcWd57oJ_J-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.array(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30nO-ORD_J-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "73d80e42-d826-42c5-bcd3-dc6a8776a3be"
      },
      "source": [
        "vocab[3020:3050]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['gastar', 'gasto', 'gazelle', 'gedauto', 'gemeliers', 'generacion',\n",
              "       'general', 'generar', 'generation', 'generico', 'genesis',\n",
              "       'genetica', 'genial', 'genialidad', 'genil', 'genio', 'gente',\n",
              "       'george', 'geosimbolicamente', 'gerencia', 'gerente', 'gestion',\n",
              "       'gestionar', 'gestor', 'ghost', 'ghvip', 'ghvipdirecto', 'gibaja',\n",
              "       'gibraltar', 'giga'], dtype='<U29')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UanHET-FaAZN",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSf9hs5Z_J--",
        "colab_type": "text"
      },
      "source": [
        "Un problema que hay con las frecuencias de las palabras es que palabras muy comunes en el lenguage, como \"el\", dominarian el proceso de vectorizaci√≥n. TF-IDF es una forma de normalizar los vectores de frecuencias al tomar en consideraci√≥n la frecuencia en la que aparece la palabra en el documento, la longitud del documento y que tan comun o raro es la palabra en todo el corpus.\n",
        "\n",
        "$$TF = \\frac {freq(w_i)} {len(doc)} $$\n",
        "\n",
        "\n",
        "$$IDF = log(\\frac {len(corpus)} {freq(w_i, corpus)}) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmlzH04aXc7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_tnL2Bsanm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = vectorizer.fit_transform(doc_list).todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cAm8AT_ap1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd2f1725-204e-4489-f6b4-60a6aa290957"
      },
      "source": [
        "vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3763, 6729)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiCNkuiFatKU",
        "colab_type": "text"
      },
      "source": [
        "Noten que cambiar la forma de vectorizaci√≥n en estos casos no cambia la longitud de nuestros vectores (que siempre est√° dada por la dimensionalidad del vocabulario, en este caso 6729). Solo cambia los valores numericos que se asignan en los vectores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5YtJxjQXgxH",
        "colab_type": "text"
      },
      "source": [
        "## Reducci√≥n de dimensionalidad: Featurization\n",
        "Una vez que tenemos nuestros palabras representadas como vectores, nos aparece el problema de que ¬°a√∫n son demasiado grandes! En el ejemplo anterior, estamos trabajando con vectores en un espacio de 6K+. Necesitamos reducir esta dimensionalidad. Para esto, utilizaremos m√©todos de reducci√≥n de dimensionalidad con el objetivo de generar features que nos sean m√°s utiles. Estas features las generaremos de forma \"no supervisada\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4HLD1tkTBT",
        "colab_type": "text"
      },
      "source": [
        "### M√©todos b√°sados en SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tC0kyK_J-6",
        "colab_type": "text"
      },
      "source": [
        "Los modelos basados en factorizaci√≥n de matrices intentan reducir la dimensionalidad de la matriz al aproximarla usando dos matrices que representan embeddings de palabras y embeddings de documentos (m√°s una matriz singular que los vincula los unos con los otros). Este m√©todo es bastante popular no solo en NLP sino que tambi√©n en sistemas de recomendaci√≥n, m√©todo que fu√© ganador del Netflix Prize (Funk SVD).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*Z0EUVs7QElEqRqXtqut_FQ.png\" />\n",
        "\n",
        "\n",
        "U y V(trapuesta) son ortogonales. Esto es de esperar porque si determinadas propiedades determinan un determinado factor latente, entonces esas propiedades ser√°n poco relevantes en los restantes factores (pues sino, no har√≠a sentido que conformen un factor distinto en un primer lugar).\n",
        "\n",
        "SVC es un metodo de decomposici√≥n exacto, lo que singnifica que las matrices U y V son lo suficientemente grandes para mapear exactamente la matriz A. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh1u5cZzkKbO",
        "colab_type": "text"
      },
      "source": [
        "### LSI - Latent Semantic Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1pbwdPv_J-7",
        "colab_type": "text"
      },
      "source": [
        "LSI es un caso particular de factorizaci√≥n de matrices. Cuando SVD es utilizado para procesar t√≥picos en texto y en donde los valores de la matriz A corresponden a frecuencias de palabras, este m√©todo se lo denomina Latent Semantic Analysis (sin embargo, en NLP no se lo suele nombrar como LSI).\n",
        "\n",
        "Dado que SVC es un m√©todo de decomposici√≥n exacto, tiende a producir matrices de poca densidad (sparse). Para evitar este problema, se utiliza una versi√≥n modificada de SVC conocida como Truncated SVD que solamente computa los k componentes mas grandes en la descomposici√≥n. Esto ayuda a que LSI combata efectivamente el problema de matrices sparse que tienden a generarse cuando se tienen cuerpos de texto con sin√≥nimos y palabras que significan varias cosas dependiendo del contexto. Truncated SVD ev√≠ta ser un m√©todo de decomposici√≥n exacto al aproximar la matriz A utilizando los k t√≥picos m√°s relevantes.\n",
        "\n",
        "<img src='https://github.com/fastai/course-nlp/raw/aabfeddf61fea29b18c72f841d057b56a216b7eb/images/svd_fb.png' />\n",
        "\n",
        "Facebook Research: Fast Randomized SVD [https://research.fb.com/fast-randomized-svd/])\n",
        "\n",
        "En esta configuraci√≥n entonces:\n",
        " - Un documento es nada mas que la distribuci√≥n de palabras que ocurren en el (Bag of words)\n",
        " - A es una matriz de m x n donde m es la cantidad de documentos √∫ observaciones, y n es la cantidad de palabras en el vocabulario.\n",
        " - Los valores de A corresponden a la frecuencia de la cada palabra del vocabulario en cada observaci√≥n √∫ documento.\n",
        " - A es una matriz sujeta a ruido con distribuci√≥n Gausiana.\n",
        "\n",
        "\n",
        "Referencia: Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions [https://arxiv.org/abs/0909.4061]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUldeeN6_J-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')\n",
        "vectors = vectorizer.fit_transform(doc_list).todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1iGWgQZca0C",
        "colab_type": "text"
      },
      "source": [
        "El principal parametro en LSI es el numero de factores que queremos generar (el parametro K). No existe una regla para especificar este parametro ya que depende del escenario. Valores muy peque√±os pueden forzar a los documentos a ser colisionar en los t√≥picos que son asignados, mientras que valores muy grandes pueden hacer que palabras poco frecuentes y raras terminen determinando su propio \"topico\". Este valor lo especificaremos en n_components. El parametro algorithm hace referencia al m√©todo que utilizaremos para generar la descomposici√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ2uzF82_J--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=7, algorithm='randomized')\n",
        "USigma = svd.fit_transform(vectors)\n",
        "Sigma = svd.singular_values_\n",
        "VT = svd.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bFhXZc6eHQL",
        "colab_type": "text"
      },
      "source": [
        "Si bien en el codigo anterior estamos viendo las 3 matrices, solo nos interesa la matriz VT. ¬øPorque? Recuerden que nuestro \"input\" es un conjunto de palabras que luego vectorizamos utilizando TF-IDF. Cada documento est√° representado por este conjunto de palabras. Nuestro objetivo es disponer una forma donde podamos convertir este set de palabras a \"t√≥picos\" que sean m√°s informativos que las palabras propiamente dichas. **En consecuencia, lo √∫nico que nos interesa aqui es la matriz VT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvvt3kpu_J_A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6005751-c4c8-476f-9df0-1c0697732a3a"
      },
      "source": [
        "VT.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 6729)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwu_KCg__J_C",
        "colab_type": "text"
      },
      "source": [
        "Internamente, TrucatedSVC es un wrapper de la clase randomized_svd donde la matr√≠z Q que vimos anteriormente se genera a trav√©s de un m√©todo de sampling aleatorio. Las siguientes lineas son equivalentes a lo que vimos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kigl-Xl_J_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(vectors, \n",
        "                              n_components=7,\n",
        "                              n_iter=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9fOGLB_J_F",
        "colab_type": "text"
      },
      "source": [
        "Podemos validar que U es una matriz ortogonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLYEo9Gm_J_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d538b528-1067-4e18-a3f7-4c7b362e67cc"
      },
      "source": [
        "np.allclose(U.T @ U, np.eye(U.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSv1wUgK_J_I",
        "colab_type": "text"
      },
      "source": [
        "Lo siguiente es solo a titulo informativo, pero si vemos los valores de la matriz Sigma, veremos la importancia relativa de los documentos con respecto a los t√≥picos que encontramos. Si los gr√°ficamos vemos que sus valores comienzan a decrecer relativamente r√°pido, sosteniendo la supoci√≥n de que Truncated SVD genera los K m√°s relevantes t√≥picos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVI80HRw_J_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "95681231-913f-4db9-b04c-f5936cae8e15"
      },
      "source": [
        "plt.plot(Sigma)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f349d3837b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR8CpAYe_J_K",
        "colab_type": "text"
      },
      "source": [
        "#### Interpretando los t√≥picos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5f0S7FfKjG",
        "colab_type": "text"
      },
      "source": [
        "La siguiente funci√≥n solo toma la matriz de VT y obtiene las 8 palabras m√°s importantes en este topico. Si quieren pueden variar este parametro para ver m√°s palabras e inspeccionar los t√≥picos. Esto es importante porque LSI es un m√©todo no supervisado, por lo tanto no sabemos a priori cuando un t√≥pico es bueno o malo. El sentido debemos darselo nosotros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBNIN0xE_J_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_topics(a):\n",
        "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[-8:-1]]\n",
        "    topic_words = ([top_words(t) for t in a])\n",
        "    return [' '.join(t) for t in topic_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDhDFVY9_J_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9069708a-658f-4509-86f4-9116ecf25e4f"
      },
      "source": [
        "show_topics(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['medalla querer ganar gustar comprar cerveza cruzcampo',\n",
              " 'olimpia nikes siempre nuevo zapatilla camiseta adidas',\n",
              " 'comprar nuevo conocer gustar aldub81stweeksary superstar cruzcampo',\n",
              " 'bueno querer invitar beber arruinaunacitacon4palabras cerveza gustar',\n",
              " '10 terminar carrera milka ritmar correr acabo',\n",
              " 'alfajor querer ganar carrefour comprar mercadona chocolate',\n",
              " 'panga vender movistar mejor comprar bimbo carrefour']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKCo8UOd_J_Q",
        "colab_type": "text"
      },
      "source": [
        "Limitaciones en LSI:\n",
        " - LSI sufre de un problema llamado \"Indeterminaci√≥n del signo\", que b√°sicamente significa que el signo en la matr√≠z VT y USigma dependen del algorimo que se utiliz√≥ para generarlos y de las condiciones iniciales (initial random state). En este contexto, que significa que un t√≥pico est√© relacionado con una palabra en un valor negativo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no6V6rtO_J_Q",
        "colab_type": "text"
      },
      "source": [
        "### NMF: Non-negative Matrix Factorization\n",
        "\n",
        "Motivaci√≥n: En lugar de construir nuestros factores imponiendo la restricci√≥n de que sean ortogonales, la idea es de construirlos de tal forma que sean no-negativos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQwyjnN8_J_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf = NMF(n_components=7, random_state = 1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdTfBg2n_J_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W1 = nmf.fit_transform(vectors)\n",
        "H1 = nmf.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIYwacxIfxqQ",
        "colab_type": "text"
      },
      "source": [
        "En este caso, la matriz que nos interesa es H1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S39Zrr06_J_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10dddfe5-2fab-4c6b-e0b1-f73beee22622"
      },
      "source": [
        "H1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 6729)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkEBkOc-fqtT",
        "colab_type": "text"
      },
      "source": [
        "#### Interpretando los t√≥picos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8qfO_Dx_J_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9394872e-1c2a-4f5f-8ab9-7ef8c7cb7a77"
      },
      "source": [
        "show_topics(H1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['novio comprar querer corona medalla cerveza ganar',\n",
              " 'olimpia color nikes siempre nuevo zapatilla camiseta',\n",
              " 'tenis napaacc cliente nuevo conocer aldub81stweeksary superstar',\n",
              " 'mahou invitar querer beber arruinaunacitacon4palabras cerveza gustar',\n",
              " '50 10 terminar carrera ritmar correr acabo',\n",
              " 'mcflurry necesito quiero querer alfajor ganar chocolate',\n",
              " 'panga vender movistar mejor comprar bimbo carrefour']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbMNkPv7_J_i",
        "colab_type": "text"
      },
      "source": [
        "### LDA: Latent Dirichlet Allocation\n",
        "\n",
        "LDA es un m√©todo Bayesiano basado en la distribuci√≥n de Dirichlet, la cual es una distribuci√≥n sobre probabilidades en K categorias. LDA supone que los documentos que tenemos pertenecen a K categorias distintas cuya distribuci√≥n es desconocida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bct-Ck5f6fs",
        "colab_type": "text"
      },
      "source": [
        "La distribuci√≥n Dirichlet es una generalizaci√≥n de la distribuci√≥n Beta en un espacio multidimensional. As√≠ como la distribuci√≥n beta es la distribuci√≥n previa de la binomial, la distribuci√≥n de Dirichlet es la distribuci√≥n previa de la multinomial. \n",
        "\n",
        "$$ P(w\\mid d) = P(d)\\sum_c P(k\\mid d)P(w\\mid k) $$\n",
        "\n",
        "*¬øNotan alguna similitud con SVD?*\n",
        "\n",
        "David Blei, Andrew Ng, Michael Jordan:  Latent Dirichlet Allocation [https://jmlr.org/papers/volume3/blei03a/blei03a.pdf]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN1vPUB3_J_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1dC1tN6_J_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "77bbfe70-3ae7-4c2e-f829-d93282057b45"
      },
      "source": [
        "lda.fit(vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=7, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=None,\n",
              "                          topic_word_prior=None, total_samples=1000000.0,\n",
              "                          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF-MpBfDgk2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "62f4a800-b397-4893-b7ca-f101ece7334f"
      },
      "source": [
        "for idx, topic in enumerate(lda.components_):\n",
        "    print (\"Topic \", idx, \" \".join(vocab[i] for i in topic.argsort()[:-10 - 1:-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic  0 cruzcampo milka heineken toyota gustar bimbo carrefour cerveza llevar arruinaunacitacon4palabras\n",
            "Topic  1 adir adidas mercadona bimbo comprar heineken alcampo milka barato supermercado\n",
            "Topic  2 querer heineken adidas mercadona ir movistar banco poder hacendar vez\n",
            "Topic  3 galicia carrefour estrella movistar banco mercadona heineken soler sabadell bueno\n",
            "Topic  4 movistar toyota heineken mercadona adidas favor suzuki hacer vender adir\n",
            "Topic  5 heineken nikeplus acabo correr ritmar santander mejor bimbo mercadona cerveza\n",
            "Topic  6 heineken mueble bankia adir montar encontrar vestir quien medalla carrefour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFoS39i_J_r",
        "colab_type": "text"
      },
      "source": [
        "## Creando un pipeline de preprocesamiento de texto\n",
        "\n",
        "A pesar de que los m√©todos anteriores son no supervisados, son de utilidad para el modelado de de problemas no supervisados como supervisados. Para llevar estos m√©todos a un entorno pr√°ctico normalmente se construyen flujos de procesamiento como el que se muestra m√°s abajo. Estos flujos se los llama Pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLSZG2Gy_J_r",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/santiagxf/M72109/blob/master/NLP/Docs/atap_0406.png?raw=1' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdw5EvPI_J_s",
        "colab_type": "text"
      },
      "source": [
        "A modo de ejemplo, vamos a utilizar la API de Scikit-Learn para generar cada uno de estos pasos y as√≠ construir un modelo que resuelva un problema de negocio de punta a punta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONzBUk9yhH9k",
        "colab_type": "text"
      },
      "source": [
        "**¬øQue es lo que vamos a hacer?**\n",
        "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
        " - Eliminaci√≥n de stopwords\n",
        " - Tokenizaci√≥n\n",
        " - Stemming y Lemmatization\n",
        " - Procesamiento especico del tema\n",
        " - Creaci√≥n de features utilizando algun metodo de reducci√≥n de dimensionalidad, SVD, LSI, LDA\n",
        "\n",
        ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estamos viendo tweets, algunos casos interesantes podr√≠an ser:\n",
        " - Predecir el sector al que pertenece el tweet: Alimentaci√≥n, Bebidas, etc.\n",
        " - Predecir el paso en el Marketing Funel al que pertece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-TetBhbiGMz",
        "colab_type": "text"
      },
      "source": [
        "### Creando un paso de Pipeline para procesamiento de texto\n",
        "\n",
        "El paso m√°s complejo que tenemos para crear es quizas el preprocesamiento del texto. Esto lo podemos encapsular en un modulo de Scikit-Learn. Esta libreria tiene 2 tipos de modulos:\n",
        " - Transformers\n",
        " - Estimators\n",
        "\n",
        "Los transformers toman un set de features y devuelven otro set de features, por eso es que reciben el nombre de \"trasformers\", porque basicamente transforman vectores. Los estimators, por el contrario, reciben un set de features y producen un podelo que aproxima, o estima, una variable target. Por este motivo, estos modulos reciben el nombre de \"estimators\".\n",
        "\n",
        "¬øQu√© modulo les parece que va a implementar nuestro proceso de \"preprocesamiento\" de texto?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WWuoi17_J_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "import spacy\n",
        "import es_core_news_sm as spa\n",
        "import re\n",
        "import sklearn\n",
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "class TextNormalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
        "    def __init__(self, language='spanish'):\n",
        "        self.parser = spa.load() # Cargamos el parser en espa√±ol\n",
        "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # Creamos un tokenizer\n",
        "        self.stemmer = stem.SnowballStemmer(language=language) # Creamos un steammer\n",
        "        self.lemmatizer = lambda word : \" \".join([token.lemma_ for token in self.parser(word)]) # Creamos un lemmatizer\n",
        "        self.stopwords = set(stopwords.words(language)) # Instanciamos las stopwords en espa√±ol\n",
        "        self.urls_regex = re.compile('http\\S+') # Usamos una expresion regular para encontrar las URLs\n",
        "    \n",
        "    def process_text(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = [token for token in tokens if not re.match(urls_regex, token)]\n",
        "        tokens = [token for token in tokens if len(token) > 4]\n",
        "        tokens = [token for token in tokens if token not in self.stopwords]\n",
        "        tokens = [unidecode.unidecode(token) for token in tokens] # Quitamos acentos\n",
        "        tokens = [self.lemmatizer(token) for token in tokens]\n",
        "        return tokens\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        for doc in X:\n",
        "          yield ' '.join(self.process_text(text=doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSpeGyC_J_0",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3pP6II_J_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = TextNormalizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8YY0A5wkPa0",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver como funciona nuestro modulo de preprocesamiento de texto al llamarlo con la funci√≥n transform:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N89kgK3tkYUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f71196b3-f049-4449-da0f-9224062e7fa3"
      },
      "source": [
        "tweet = tweets['TEXTO'][5]\n",
        "print(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". @PoliciadeBurgos @PCivilBurgos @Aytoburgos Mismo peligro c/ Rio Viejo junto Mercadona Villimar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIZm5SWMkgvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef2ff58c-0934-4de9-9cdd-6e771a7a5d43"
      },
      "source": [
        "list(normalizer.transform([tweet]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mismo peligrar Viejo juntar Mercadona Villimar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKl5iLwpl1Jm",
        "colab_type": "text"
      },
      "source": [
        "*Nota: Usamso list() solamente porque transform es una operaci√≥n de tipo lazy. Esto significa que no se ejecuta hasta que enumeramos los resultados*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDuoM0BRmESe",
        "colab_type": "text"
      },
      "source": [
        "### Construyendo nuestro pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhSKt53_J_w",
        "colab_type": "text"
      },
      "source": [
        "Importamos algunas librerias que necesitaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bx5kQr6_J_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU4ykD8z_J_3",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro vectorizador, en este caso usando el m√©todo TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVIdgSc9_J_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSnMG5la_J_5",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro generador de features, que en este caso son los t√≥picos que LDA genere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o18xNsj_J_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featurizer = LatentDirichletAllocation(n_components=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FumqlRDO_J__",
        "colab_type": "text"
      },
      "source": [
        "Instanciamos nuestro clasificador que utilizar√° las features generadas hasta este momento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mpzmpo__KAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = LogisticRegression(max_iter=10000, multi_class='multinomial')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xkn_cWu_KAE",
        "colab_type": "text"
      },
      "source": [
        "Creamos un pipeline que ejecute todos los pasos en secuencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAccQHAG_KAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(steps=[('normalizer', normalizer), \n",
        "                           ('vectorizer', vectorizer),\n",
        "                           ('featurizer', featurizer),\n",
        "                           ('estimator', estimator)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pY5hsJmRzH",
        "colab_type": "text"
      },
      "source": [
        "### Entrenando nuestro pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wHgLnnYmZH0",
        "colab_type": "text"
      },
      "source": [
        "En este caso intentaremos predecir el sector al que pertenece un tweet en particular. Para ello, como en todo proceso de machine learning separaremos nuestros datos en training y testing, para poder evaluar los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yAc9s1WmjlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'], test_size=0.33, stratify=tweets['SECTOR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWhdGYQn4Ey",
        "colab_type": "text"
      },
      "source": [
        "El m√©todo fit intrenar√° nuestro modelo de punta a punta. Tomar√° unos minutos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp-fZa4E_KAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pipeline.fit(X=X_train, y=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOLZErx-oGIB",
        "colab_type": "text"
      },
      "source": [
        "Es hora de ver que tan bien le fu√© a nuestro modelo en esta tarea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiQLKHrr_KAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbDgBh5M_KAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fc4359a0-c815-4fd7-9247-f8f6c369751d"
      },
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.00      0.00      0.00       110\n",
            "  AUTOMOCION       0.00      0.00      0.00       148\n",
            "       BANCA       0.28      0.11      0.15       198\n",
            "     BEBIDAS       0.25      0.28      0.26       223\n",
            "    DEPORTES       0.27      0.30      0.28       216\n",
            "      RETAIL       0.21      0.52      0.29       268\n",
            "       TELCO       0.00      0.00      0.00        79\n",
            "\n",
            "    accuracy                           0.23      1242\n",
            "   macro avg       0.14      0.17      0.14      1242\n",
            "weighted avg       0.18      0.23      0.18      1242\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "089VY2OupEVK",
        "colab_type": "text"
      },
      "source": [
        "¬øLes parece que est√°s m√©tricas son buenas? ¬øSe les ocurre como mejorarlo? Algunas ideas:\n",
        " - ¬øQuien funcionar√° mejor? ¬øStemmer o Lemmatization?\n",
        " - ¬øQu√© ser√° mejor hacer con los hashtags? ¬øQuitarlos?\n",
        " - ¬øQue cantidad de factores latentes funcionar√° mejor? ¬ø7, 10, 200, 300?\n",
        " - ¬øEs Logistic Regression el mejor clasificador que podemos probar? ¬øSi subimos la cantidad de t√≥picos que me ser√≠a mejor utilizar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XfLkgKApJpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}