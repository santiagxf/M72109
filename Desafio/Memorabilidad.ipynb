{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPWRnEx2oTvM"
   },
   "source": [
    "```\n",
    "ME72: Maestría en Métodos Cuantitativos para la Gestión y Análisis de Datos\n",
    "M72109: Analisis de datos no estructurados\n",
    "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
    "Año: 2020\n",
    "Profesor: Facundo Santiago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWBLdBSioTvO"
   },
   "source": [
    "# Desafio: ¿Qué tan memorable es un video?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbe7972dpDfx"
   },
   "source": [
    "Como trabajo final para la materia, les proponemos resolver un desafio de memorabilidad de un video, el cual requerirá de utilizar todos los conceptos que revisamos en la materia **al mismo tiempo**: imagenes, audio y texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT_t9OxYwKIc"
   },
   "source": [
    "## ¿De que se trata el desafío?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1_-eKx3oTvT"
   },
   "source": [
    "Esta tarea se centra en el problema de predecir qué tan memorable es un video para un espectador. Requiere que los alumnos predigan automáticamente las puntuaciones de memorabilidad de los videos que reflejan la probabilidad de que se recuerde un video. Recibiran un extenso conjunto de datos de videos que van acompañados de anotaciones memorables, así como predictores (features) extraidos previamente para facilitarles el procesamiento. Las etiquetas (labels) se ha recopilado a través de pruebas de reconocimiento y, por lo tanto, es el resultado de una medición objetiva del rendimiento de la memoria.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Docs/memorability.PNG' width=600 />\n",
    "\n",
    "*Creditos del desafio original:*\n",
    "\n",
    "http://www.multimediaeval.org/mediaeval2019/memorability/\n",
    "\n",
    "Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania\n",
    "Bogdan Ionescu, University Politehnica of Bucharest, Romania\n",
    "Claire-Hélène Demarty, Technicolor, France\n",
    "Quang-Khanh-Ngoc Duong, Technicolor, France\n",
    "Xavier Alameda-Pineda, INRIA, France\n",
    "Mats Sjöberg, CSC, Finland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K6p37ncoTvU"
   },
   "source": [
    "## Direcciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr_jJ5wAoTvV"
   },
   "source": [
    "Deberán entrenar modelos computacionales capaces de inferir la memorabilidad de video a partir del contenido visual. Para hacerlo, podran optar por utilizar los videos directamente o utilizar caracteristicas (features) que fueron extraidas desde los mismos utilizando algunas de las técnicas que vimos en este curso. Utilizando estos datos, deberan evaluar la performance del modelo utilizando un set de datos de evaluación. Los modelos se evaluarán a través de métricas de evaluación estándar utilizadas en las tareas de clasificación y regressión (dependiendo del tipo de desafío que elijan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdX8oMEkoTvp"
   },
   "source": [
    "## Anotaciones para el problema de memorabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBDymaXtoTvp"
   },
   "source": [
    "Cuentan con la anotaciones para el problema de memorabilidad. Dependiendo de su elección pueden optar por resolver un problema de clasificación o un problema de regresión. Esto lo veran refrejado en:\n",
    " - **memorability_score:** Representa el puntaje de memorabilidad de la secuencia en particular. Valores altos son mejores.\n",
    " - **memorable:** Variable categórica que representa si un video es memorable o no. Un video con Score superior a 0.5 es marcado como memorable (1), sino es marcado como no memorable (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Data/ground_truth.csv --directory-prefix ./Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69lG6CJyoTvq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv('Data/ground_truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXS_1KRBoTvt",
    "outputId": "570584c2-f3a5-464e-dd8d-7392a5981d46"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>start(sec)</th>\n",
       "      <th>end(sec)</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>Neutral (1)_Typical (0)</th>\n",
       "      <th>nb_annotations</th>\n",
       "      <th>memorability_score</th>\n",
       "      <th>memorabable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127 hours</td>\n",
       "      <td>2000</td>\n",
       "      <td>2010</td>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127 hours</td>\n",
       "      <td>2182</td>\n",
       "      <td>2192</td>\n",
       "      <td>127_hours_2182_2192_5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_name  start(sec)  end(sec)          sequence_name  \\\n",
       "0  127 hours        2000      2010  127_hours_2000_2010_1   \n",
       "1  127 hours        2182      2192  127_hours_2182_2192_5   \n",
       "\n",
       "   Neutral (1)_Typical (0)  nb_annotations  memorability_score  memorabable  \n",
       "0                        0               5                 1.0            1  \n",
       "1                        1               8                 0.0            0  "
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ih544DoTvw"
   },
   "source": [
    "## Elije tu propia aventura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlw8T_TJoTvx"
   },
   "source": [
    "En este desafío, les propongo elegir la dificultad con la que quieren trabajar para solucionar el problema en cuestión. Dependiendo de la elección, son las herramientas que tendrán a su disosición para resolver el problema. Pueden utilizar algunas de las sugerencias, todas las sugerencias, o probar alguna técnica que conozcan y que les parezca interesante de probar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhIQ8dJ3oTvx"
   },
   "source": [
    "### Resolver el desafío utilizando los datos de origen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_vGLIxToTv0"
   },
   "source": [
    "Esta configuración del desafío es la más similar a la vida real, y solo contaran con un conjunto de videos en formato MP4 como datos de entrada junto con sus anotaciones de memorabilidad en formato CSV. Cualquier procesamiento de datos deberá realizarse utilizando esta información únicamente. Dado que algunos procesamientos de datos sobre videos pueden ser complejos, adicionalmente contarán además con:\n",
    " - Imágenes extraidas cada 2 segundos por cada secuencia a analizar\n",
    " - Tracks de audio disponibles en formato WAV listos para utilizar\n",
    "\n",
    "A pesar de que el video a sido separado en imagenes y audio, resolver el desafío en utilizando estos datos será una tarea compleja. Sin embargo, en el contexto de sus organizaciones, normalmente se encontrarán con esta dificultad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69zwNbFnoTv1"
   },
   "source": [
    "#### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_meGx1_xoTv2"
   },
   "source": [
    "En el directorio */Data/Raw/sources* encontraran todas las secuencias utilizadas en este desafío. El archivo *'Data/ground_truth.csv'* contine los nombres de las peliculas a las que corresponde cada fragmento en caso que quieran revisarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6A9oK-uoTv2"
   },
   "source": [
    "#### Audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LACmI1EUoTv3"
   },
   "source": [
    "En el directorio */Data/Raw/audios* encontraran todos los audios correspondientes a cada una de las secuencias de los videos. Los audios están en formato wav, que si bien ocupan mayor espacio, son más sencillos de procesar. \n",
    "\n",
    "Algunas ideas:\n",
    " - Aplicar técnicas de speech-to-text.\n",
    " - Aplicar técnicas de embeddings de audio.\n",
    " - Clasificar primero el audio (en por ejemplo 'instrumento musical', 'personas charlando', 'exterior', 'aire libre') y luego utilizarlos para resolver el problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slqhMQfooTv3"
   },
   "source": [
    "#### Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB67LMgioTv4"
   },
   "source": [
    "En el directorio */Data/Raw/frames* encontraran todos los frames extraidos de cada una de las secuencias. Los frames están disponibles cada 2 segundos por lo cual disponen de 5 frames por cada secuencia.\n",
    "\n",
    "Algunas ideas:\n",
    " - Aplicar modelos de convolución directamente sobre las imagenes.\n",
    " - Aplicar técnicas de descripción de imagenes utilizando Servicios Cognitivos\n",
    " - Reconocimiento de lugares claves.\n",
    " - Los fragmentos tienen personas famosas, ya que son actores de cine y televisión. ¿Será importante?\n",
    " - Colores de la escena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtSQSwfloTv4"
   },
   "source": [
    "### Resolver el desafío utilizando predictores preprocesados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU7gx-HOoTv5"
   },
   "source": [
    "En esta configuración del desafio, ya hay bastante trabajo realizado. Los videos han sido preprocesados para generar un conjunto de predictores que pueden ayudar a resolver el problema. Estos predictores se entregan como un conjunto de archivos en formato CSV. \n",
    "\n",
    "A pesar de que esta opción pueda parecer sencilla, ofrece sus propios desafíos ya que tienen a disposición muchos predictores pero en dominios distintos: audio, texto, embeddings, etc. Deberán desarrollar una técnica que pueda hacer sentido de esto en su conjunto.\n",
    "\n",
    "A continuación detallamos los predictores con los que contarán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Data/Features/audio_vectors.csv --directory-prefix ./Data/Features/\n",
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Data/Features/c3d_vectors.csv --directory-prefix ./Data/Features/\n",
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Data/Features/caption_features.csv --directory-prefix ./Data/Features/\n",
    "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Data/Features/colors_features.csv --directory-prefix ./Data/Features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqqS8evHoTv5"
   },
   "source": [
    "#### Descripciónes de las imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU6rU8EkoTv6"
   },
   "source": [
    "Cuentan con las descripciones de cuadros (frames) de cada una de las secuencias. Estos frames están extraidos cada 1 segundo y las descripciones se generaron utilizando la API de Computer Vision de Azure Cognitive Services. Cada una de las descripciones forma una oración, y por lo tanto, por cada secuencia tendrán disponible un parrafo con 10 oraciones describiendo cada una de las escenas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_8cKMFxoTv7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "captions = pd.read_csv('Data/Features/caption_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfXroshYoTv-",
    "outputId": "34e39914-6417-47f7-c17b-a3fec8f19131"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>a man in a suit and tie standing in front of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127_hours_2182_2192_5</td>\n",
       "      <td>a man holding a toothbrush in his mouth. a wom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sequence_name                                                 cc\n",
       "0  127_hours_2000_2010_1  a man in a suit and tie standing in front of a...\n",
       "1  127_hours_2182_2192_5  a man holding a toothbrush in his mouth. a wom..."
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colores de las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los colores en una imágen pueden ser representativos de lo que uno está viendo. Por ejemplo, en una secuencia de terror, las imagenes con fondos oscuros quizás prevalezcan que con colores claros. Para capturar esto, cuentan con anotaciones para cada secuencia, para cada cuadro (frame de la secuencia) con los valores correspondientes al color más dominante en el fondo y el color más dominante en primer plano. Noten que en este caso, disponen de esta información por cada uno de los cuadros (es decir, 5 anotaciones por cada secuencia y por lo tanto deberán elejir una estrategia para quedarse con 1 solo valor por secuencia. Por ejemplo: el color más frecuente, o el color más raro. También pueden optar por utilizar variables categoricas en el sentido \"contine_fondo_oscuro\" significando una secuencia en que el fondo en algún cuadro es oscuro/negro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colors = pd.read_csv('Data/Features/colors_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>dominant_background</th>\n",
       "      <th>dominant_foreground</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sequence_name dominant_background dominant_foreground\n",
       "0  127_hours_2000_2010_1               Black               Black\n",
       "1  127_hours_2000_2010_1                Grey               Black\n",
       "2  127_hours_2000_2010_1               Black               Black\n",
       "3  127_hours_2000_2010_1               Black               Black\n",
       "4  127_hours_2000_2010_1               Black               Black"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPylGpWaoTwA"
   },
   "source": [
    "#### Vectores de audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GBcZOtjoTwB"
   },
   "source": [
    "Se corresponden a las representaciones (embeddings) utilizados por la red YAMNet que vimos en el curso. YAMNet es un modelo que utiliza la misma ténica que las CNN pero al construir imágenes a partir de audio (llamados parches o patchs). Cada uno de estos patchs es el resultado de calcular espectrogramas de audio, creando así parches de imágenes 2D para utilizar en nuestro modelo.\n",
    "\n",
    "Los vectores generados por YAMNet son vectores en un espacio de 1024-D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDAKlAJ1oTwB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "audio_embeddings = pd.read_csv('Data/Features/audio_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVYDOID8oTwE",
    "outputId": "f4011813-957d-49c3-c6c3-406d042f52b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796399</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          sequence_name    1         2         3         4  \\\n",
       "0           0  127_hours_2000_2010_1  0.0  0.796399  0.009879  0.000000   \n",
       "1           1  127_hours_2000_2010_1  0.0  0.000000  0.000000  0.003163   \n",
       "\n",
       "          5    6    7    8  ...  1015  1016  1017      1018  1019  1020  1021  \\\n",
       "0  0.000000  0.0  0.0  0.0  ...   0.0   0.0   0.0  0.492673   0.0   0.0   0.0   \n",
       "1  0.010787  0.0  0.0  0.0  ...   0.0   0.0   0.0  0.173500   0.0   0.0   0.0   \n",
       "\n",
       "       1022  1023  1024  \n",
       "0  0.391332   0.0   0.0  \n",
       "1  0.177809   0.0   0.0  \n",
       "\n",
       "[2 rows x 1026 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bzVJNVwoTwJ"
   },
   "source": [
    "#### Vectores de la red C3D (convoluciones en 3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN1eZwsfoTwK"
   },
   "source": [
    "Se corresponden a las representaciones aprendidas al aplicar un modelo de convolución 3D sobre el input del video. Estas convoluciones no solo tienen en cuenta localidad a nivel de pixeles en las imagenes sino que también localidad a nivel espacio-temporal en el transcurso del video.\n",
    "\n",
    "<img src='https://github.com/santiagxf/M72109/blob/master/Desafio/Docs/3D-timedistributed.PNG?raw=1' width=500 />\n",
    "\n",
    "Referencias sobre este modelo: https://arxiv.org/pdf/1711.11248.pdf<br />\n",
    "Implementación por el equipo de Facebook AI: https://github.com/facebookresearch/VMZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pD1_dfOZLGW"
   },
   "source": [
    "En en caso de las representaciones provistas aquí, corresponden al caso B) que se muestra en la imagen superior, donde se aplicaron convoluciones 3D (alto, ancho y tiempo) en las primeras capas para capturar relaciones temporales a alto nivel y luego se continuó con convoluciones 2D de forma similar a lo que vimos en este curso.\n",
    "\n",
    "Los embeddings generados son vectores en un espacio dimensional 4096-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OvTxu-K_oTwK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "video_embeddings = pd.read_csv('Data/Features/c3d_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5TgQww92oTwN",
    "outputId": "fd82520b-86eb-47ec-b339-8182bd53ff06"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>127_hours_2000_2010_1</td>\n",
       "      <td>0.82588</td>\n",
       "      <td>6.3702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34248</td>\n",
       "      <td>0.75896</td>\n",
       "      <td>1.7976</td>\n",
       "      <td>0.1524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6224</td>\n",
       "      <td>1.7332</td>\n",
       "      <td>2.4912</td>\n",
       "      <td>3.27780</td>\n",
       "      <td>1.7001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.9559</td>\n",
       "      <td>6.1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>127_hours_2182_2192_5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.2294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.53370</td>\n",
       "      <td>1.2082</td>\n",
       "      <td>2.6512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8342</td>\n",
       "      <td>1.7536</td>\n",
       "      <td>1.8910</td>\n",
       "      <td>0.91983</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0523</td>\n",
       "      <td>1.0713</td>\n",
       "      <td>1.4447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          sequence_name        0       1    2        3        4  \\\n",
       "0           0  127_hours_2000_2010_1  0.82588  6.3702  0.0  0.34248  0.75896   \n",
       "1           1  127_hours_2182_2192_5  0.00000  2.2294  0.0  0.00000  1.53370   \n",
       "\n",
       "        5       6    7   ...      4086  4087    4088    4089    4090     4091  \\\n",
       "0  1.7976  0.1524  0.0   ...    3.9249   0.0  5.6224  1.7332  2.4912  3.27780   \n",
       "1  1.2082  2.6512  0.0   ...    0.3997   0.0  3.8342  1.7536  1.8910  0.91983   \n",
       "\n",
       "     4092    4093    4094    4095  \n",
       "0  1.7001  0.0000  1.9559  6.1765  \n",
       "1  0.0000  2.0523  1.0713  1.4447  \n",
       "\n",
       "[2 rows x 4098 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_embeddings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Memorabilidad.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
