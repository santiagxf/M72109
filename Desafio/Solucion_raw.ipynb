{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPWRnEx2oTvM"
      },
      "source": [
        "```\n",
        "ME72: Maestría en Métodos Cuantitativos para la Gestión y Análisis de Datos\n",
        "M72109: Analisis de datos no estructurados\n",
        "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
        "Año: 2024\n",
        "Profesor: Facundo Santiago, Javier Ignacio Garcia Fronti\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWBLdBSioTvO"
      },
      "source": [
        "# ¿Qué tan memorable es un video?: Solución base utilizando videos, images y audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbe7972dpDfx"
      },
      "source": [
        "Este notebook les permitirá tener a disposición todos los recursos que se vieron en durante la materia. Todos los fragmentos de código se descargaran utilizando la sección preparación del ambiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXdA6Yg761jz"
      },
      "source": [
        "## Preparación del ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZrG1xI-61j0"
      },
      "source": [
        "### Sets de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xb7yWkB61j1"
      },
      "outputs": [],
      "source": [
        "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/Data/ground_truth.csv --directory-prefix ./Data/ --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ9KM2hm61j7"
      },
      "source": [
        "### Librerías de codigo que se utilizan durante el curso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDiK_X1LBU6s"
      },
      "outputs": [],
      "source": [
        "!wget -N https://raw.githubusercontent.com/santiagxf/M72109/master/Desafio/solucion_base.txt --quiet\n",
        "!pip install -r solucion_base.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAPdJg4j61j8"
      },
      "source": [
        "Descargamos todo el código con utilidades que se vieron en el curso desde el repositorio de la materia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgEPMkE861j9"
      },
      "outputs": [],
      "source": [
        "!git clone -n --depth=1  https://github.com/santiagxf/M72109\n",
        "!cd M72109 && git sparse-checkout set --no-cone m72109 && git checkout\n",
        "!mv M72109/m72109 . && rm -rf M72109"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mIgYB7261kS"
      },
      "source": [
        "Descargamos la etiquetas de YamNET en el caso de utilizarlas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNeu_Nue61kT"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/M72109/master/Audio/Models/yamnet/yamnet_class_map.csv \\\n",
        "    --directory-prefix ./Models/yamnet/ --quiet --no-clobber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7n2pmiw61kc"
      },
      "source": [
        "Descargamos modelos de Word2Vec en ingles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7COK8kZ61kd"
      },
      "outputs": [],
      "source": [
        "!wget https://santiagxf.blob.core.windows.net/public/Word2Vec/model-en.bin --directory-prefix ./Models/Word2Vec --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utQ0fxnu61ko"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLjNwmAJ61kp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, SpatialDropout1D\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from m72109.audio.plotting import plot_audio_embeddings\n",
        "from m72109.nlp.normalization import TextNormalizer, TweetTextNormalizer\n",
        "from m72109.nlp.transformation import Word2VecVectorizer, PadSequenceTransformer, split_text_with_context\n",
        "\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucs_gunq61ku"
      },
      "source": [
        "## Solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBDymaXtoTvp"
      },
      "source": [
        "Cuentan con 2 tipos de anotaciones para cada uno de los fragmentos de video disponibles:\n",
        " - **memorability_score:** Representa el puntaje de memorabilidad de la secuencia en particular, desde 0 a 1. Valores más grandes son mejores.\n",
        " - **memorable:** Variable categórica que representa si un video es memorable o no. Un video con `memorability_score` superior a `0.5` es marcado como memorable (`1`), sino es marcado como no memorable (`0`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69lG6CJyoTvq"
      },
      "outputs": [],
      "source": [
        "labels = pd.read_csv('Data/ground_truth.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXS_1KRBoTvt"
      },
      "outputs": [],
      "source": [
        "labels.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[labels['movie_name'] == \"127 hours\"]['movie_name'][0]"
      ],
      "metadata": {
        "id": "pD3hroydbULt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Xi8axtPx7b"
      },
      "source": [
        "> Note que aquí nuestras muestras son \"secuencias\" de determinadas películas. En total dispone de 609 secuencias con el nombre `sequence_name`. El mismo nombre se generó automáticamente concatenando el nombre de la pelicula a la que pertenece la secuencia (movie_name), seguido del segundo en el que comienza la secuencia, seguido del segundo en el que termina, seguido de un numero que indica el número de secuencia. Por ejemplo, la secuencia `127_hours_2000_2010_1` es un fragmento de la pelicula \"127 hours\", que va desde el segundo 2000 (00:33:20 hrs.) al segundo 2010 (00:33:30 hrs.) y es el fragmento número 1. Esta información es totalmente irrelevante para el problema de memorabilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-phMVhQA61mQ"
      },
      "source": [
        "### Datos no estructurados\n",
        "\n",
        "Dado que los conjuntos de datos pueden ser de gran tamaño, descargue solo los datos que necesita:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt2Xp4oe61mR"
      },
      "source": [
        "#### Para utilizar los cuadros (frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn1Cn72C61mS"
      },
      "outputs": [],
      "source": [
        "!wget -N https://santiagxf.blob.core.windows.net/public/Memorability/frames.tar.gz --directory-prefix ./Data/Raw/\n",
        "!tar zxvf ./Data/Raw/frames.tar.gz --directory ./Data/Raw/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQCDTj5PT5U"
      },
      "source": [
        "> En el directorio `Data/Raw/frames` encontraran todos los cuadros extraidos de cada una de las secuencias. Los cuadros están disponibles cada 2 segundos por lo cual disponen de 5 cuadros por cada secuencia. Todos los cuadros pertenecientes a una secuencia están almacenados en una carpeta con el mismo nombre de la secuencia a la que pertenece, por lo que la carpeta `127_hours_2000_2010_1` contiene todos los cuadros pertenecientes a la secuencia `127_hours_2000_2010_1` en el conjunto de datos de anotaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tip: Leyendo el dataset\n",
        "\n",
        "En este ejemplo, las etiquetas no están en el conjunto de datos, sino que se encuentran en un archivo estructurado. Necesitaremos utilizar el directorio donde se encuentra la imagen para ubicar el label correspondiente.\n",
        "\n",
        "Para ello, primero leeremos todas las imagenes disponibles y ubicaremos cual es su etiqueta correspondiente (memorable o no memorable). Todas las imágenes del mismo directorio tendran la misma etiqueta. El siguiente código realiza lo mencionado:"
      ],
      "metadata": {
        "id": "K2lOavBUt-_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memorable_labels = []\n",
        "for root, _, filepaths in os.walk('Data/Raw/frames'):\n",
        "  for filepath in filepaths:\n",
        "    sequence_name = os.path.basename(root)\n",
        "    if sequence_name in np.unique(labels['sequence_name'].values):\n",
        "      memorable_labels.append(labels[labels['sequence_name'] == sequence_name]['memorabable'].iloc[0])\n",
        "    else:\n",
        "       memorable_labels.append(0)\n"
      ],
      "metadata": {
        "id": "m1p-s7G2pTxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder crear un conjunto de datos combinando las imágenes con sus etiquetas deberemos definir una función para leer las imagenes dado un directorio:"
      ],
      "metadata": {
        "id": "aJOrD6yduxsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.data.experimental import AUTOTUNE\n",
        "\n",
        "def parse_image(filename, label, channels:int=3, img_size:int=224):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=channels)\n",
        "    image_resized = tf.image.resize(image_decoded, [img_size, img_size])\n",
        "\n",
        "    return image_resized, label"
      ],
      "metadata": {
        "id": "ZKcGmtIVk-Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, construiremos una función que genera el conjunto de datos que necesitamos:\n",
        "\n",
        "Note que:\n",
        "\n",
        "1. `tf.data.Dataset.list_files` lista todos los archivos de un directorio dado. Estos seran los predictores.\n",
        "1. `tf.data.Dataset.from_tensor_slices` construye un dataset de TensorFlow a partir de un arreglo de `Numpy` o `Pandas`. Estos serán las etiquetas.\n",
        "1. `tf.data.Dataset.zip` construye un dataset combinando 2 datasets previamente creados. En este caso lo utilizaremos para unir los predictores (creados en el paso 1) y las etiquetas (creadas en el paso 2)."
      ],
      "metadata": {
        "id": "jHzoZ06uu-Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(filenames, labels, batch_size=64, is_training=True):\n",
        "    X_train = tf.data.Dataset.list_files(filenames, shuffle=False)\n",
        "    y_train = tf.data.Dataset.from_tensor_slices(np.array(labels))\n",
        "    dataset = tf.data.Dataset.zip((X_train, y_train))\n",
        "    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=500)\n",
        "\n",
        "    # dataset = dataset.batch(batch_size) # Si necesita implementar batching\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "lUUofQrUgoTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos el dataset:"
      ],
      "metadata": {
        "id": "eiFXGdodvpzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = create_dataset(filenames = 'Data/Raw/frames/*/*.jpg', labels = memorable_labels, is_training=True)"
      ],
      "metadata": {
        "id": "w6ZVjYowm1Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifique los datos que obtendra:"
      ],
      "metadata": {
        "id": "VDuYmWkSwVh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feat, label in train_ds.take(1):\n",
        "  print('Features: shape', feat.shape)\n",
        "  print('Label: sample value', label.numpy())"
      ],
      "metadata": {
        "id": "KGSiErhwwXxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkwfP4sC61mw"
      },
      "outputs": [],
      "source": [
        "# completar solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1Q0cmy61nK"
      },
      "source": [
        "#### Para utilizar los audios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttFN_wja61nO"
      },
      "outputs": [],
      "source": [
        "!wget -N https://santiagxf.blob.core.windows.net/public/Memorability/audios.tar.gz --directory-prefix ./Data/Raw/\n",
        "!tar zxvf ./Data/Raw/audios.tar.gz --directory ./Data/Raw/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_pWzWaxOuoC"
      },
      "source": [
        "> En el directorio `Data/Raw/audios` encontraran todos los audios correspondientes a cada una de las secuencias de los videos. Cada audio tiene el mismo nombre de la secuencia a la que pertenece, por lo que el archivo `127_hours_2000_2010_1.wav` pertenece a la secuencia `127_hours_2000_2010_1` en el conjunto de datos de anotaciones. Los audios están en formato `wav`, que si bien ocupan mayor espacio, son más sencillos de procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U69bgtv861nd"
      },
      "outputs": [],
      "source": [
        "# completar solución"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Solucion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}